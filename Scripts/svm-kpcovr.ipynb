{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from regression import PCovR, KPCovR, SparseKPCovR\n",
    "from regression import LR, KRR\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "from kernels import center_kernel, center_kernel_fast\n",
    "from kernels import center_kernel_oos, center_kernel_oos_fast\n",
    "from soap import compute_soap_density, reshape_soaps\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/kernel-tutorials')\n",
    "# sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/KernelPCovR/analysis/scripts')\n",
    "# from utilities.sklearn_covr.kpcovr import KernelPCovR as KPCovR2\n",
    "# from utilities.sklearn_covr.pcovr import PCovR as PCovR2\n",
    "# from helpers import l_regr, l_kpcovr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_class(df, df_type, n_classes, use_df_sums=True):\n",
    "    \"\"\"\n",
    "        Make class predictions based on a decision function.\n",
    "        Based on the sci-kit learn SVC prediction, see\n",
    "        `sklearn.multiclass._ovr_decision_function` \n",
    "        (sci-kit learn licensed under BSD 3-Clause license)\n",
    "        TODO: could we also just use this function?\n",
    "        \n",
    "        ---Arguments---\n",
    "        df: decision function on which to make class predictions\n",
    "        df_type: decision function type, 'ovo' or 'ovr'\n",
    "        n_classes: number of integer classes\n",
    "        use_df_sums: augment the 'ovo' vote counts with\n",
    "            decision function values (useful for tie breaks)\n",
    "            \n",
    "        ---Returns---\n",
    "        predicted_class: predicted integer class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Approximation to the number of classes, should be valid up to at least 1M\n",
    "    #n_classes = int(np.sqrt(2*df.shape[-1])) + 1\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        if df_type == 'ovo':\n",
    "            vote_matrix = np.zeros((df.shape[0], n_classes))\n",
    "            df_sum = np.zeros((df.shape[0], n_classes))\n",
    "\n",
    "            # Predicted class determined by majority vote\n",
    "            col_idx = 0\n",
    "            for i in range(0, n_classes):\n",
    "                for j in range(i + 1, n_classes):\n",
    "                    col_train = df[:, col_idx]\n",
    "                    vote_matrix[col_train > 0, i] += 1\n",
    "                    vote_matrix[col_train <= 0, j] += 1\n",
    "\n",
    "                    # Add value of decision function\n",
    "                    if use_df_sums:\n",
    "                        df_sum[:, i] += df[:, col_idx]\n",
    "                        df_sum[:, j] -= df[:, col_idx]\n",
    "\n",
    "                    col_idx += 1\n",
    "\n",
    "            # sci-kit learn transformation from 'ovo' to 'ovr'\n",
    "            if use_df_sums:\n",
    "                transformed_df_sum = df_sum / (3 * (np.abs(df_sum) + 1))\n",
    "                vote_matrix += transformed_df_sum\n",
    "\n",
    "            predicted_class = np.argmax(vote_matrix, axis=1) + 1\n",
    "\n",
    "        elif df_type == 'ovr':\n",
    "\n",
    "            # Predicted class determined by largest value of the decision function\n",
    "            predicted_class = np.argmax(df, axis=1) + 1\n",
    "\n",
    "        else:\n",
    "            print(\"Error: invalid decision function. Use 'ovo' or 'ovr'\")\n",
    "    else:\n",
    "        predicted_class = np.zeros(df.shape[0], dtype=int)\n",
    "        \n",
    "        # This appears to be the convention, which is \"opposite\" of that above\n",
    "        # Default exactly zero decision function value to the \"positive\" class\n",
    "        predicted_class[df >= 0] = 2\n",
    "        predicted_class[df < 0] = 1\n",
    "        \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set indices for Deem\n",
    "idxs_deem_train = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "idxs_deem_test = np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_deem_train = idxs_deem_train.size\n",
    "n_deem_test = idxs_deem_test.size\n",
    "n_deem = n_deem_train + n_deem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "cantons_iza = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)\n",
    "RWY = np.nonzero(cantons_iza == 4)[0][0]\n",
    "cantons_iza = np.delete(cantons_iza, RWY)\n",
    "n_iza = len(cantons_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_iza = np.ones(n_iza, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IZA sample\n",
    "# (will be overwritten if we load a kernel\n",
    "n_iza_train = n_iza // 2\n",
    "n_iza_test = n_iza - n_iza_train\n",
    "idxs_iza = np.arange(0, n_iza)\n",
    "np.random.shuffle(idxs_iza)\n",
    "\n",
    "idxs_iza_train = idxs_iza[0:n_iza_train]\n",
    "idxs_iza_test = idxs_iza[n_iza_train:n_iza_train+n_iza_test]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SOAPs and build kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to recompute existing kernels\n",
    "remove_kernels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = {}\n",
    "K_test = {}\n",
    "K_test_test = {}\n",
    "kernel_type = {}\n",
    "gamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    kernel_file = f'../Processed_Data/Models/{cutoff}/structure_kernels.hdf5'\n",
    "    \n",
    "    # Start fresh\n",
    "    if remove_kernels and os.path.exists(kernel_file):\n",
    "        os.remove(kernel_file)\n",
    "    \n",
    "    # Load the kernels if they exist\n",
    "    try:\n",
    "        f = h5py.File(kernel_file, 'r')\n",
    "        \n",
    "        K_train[cutoff] = f['K_train'][:]\n",
    "        K_test[cutoff] = f['K_test'][:]\n",
    "        K_test_test[cutoff] = f['K_test_test'][:]\n",
    "        kernel_type[cutoff] = f.attrs['kernel_type']\n",
    "        gamma[cutoff] = f.attrs['gamma']\n",
    "        \n",
    "        # Don't need to store indices in a dictonary\n",
    "        # since they are the same for all cutoffs\n",
    "        idxs_iza_train = f.attrs['idxs_iza_train']\n",
    "        idxs_iza_test = f.attrs['idxs_iza_test']\n",
    "        idxs_deem_train = f.attrs['idxs_deem_train']\n",
    "        idxs_deem_test = f.attrs['idxs_deem_test']\n",
    " \n",
    "        f.close()\n",
    "    \n",
    "    # Compute the kernels if they don't exist\n",
    "    except OSError:\n",
    "    \n",
    "        # Load kernel parameters\n",
    "        model_file = f'../Processed_Data/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "\n",
    "        with open(model_file, 'r') as f:\n",
    "            model_dict = json.load(f)\n",
    "\n",
    "        kernel_type[cutoff] = model_dict['kernel_type']\n",
    "        gamma[cutoff] = model_dict['gamma']\n",
    "\n",
    "        # Load SOAPs\n",
    "        deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        deem_soaps = load_structures_from_hdf5(deem_file, datasets=None, concatenate=False)\n",
    "\n",
    "        iza_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        iza_soaps = load_structures_from_hdf5(iza_file, datasets=None, concatenate=False)\n",
    "        iza_soaps.pop(RWY)\n",
    "\n",
    "        # Build the collection of soap vectors\n",
    "        # for the \"master\" kernel\n",
    "        deem_train = [deem_soaps[i] for i in idxs_deem_train]\n",
    "        deem_test = [deem_soaps[i] for i in idxs_deem_test]\n",
    "        iza_train = [iza_soaps[i] for i in idxs_iza_train]\n",
    "        iza_test = [iza_soaps[i] for i in idxs_iza_test]\n",
    "\n",
    "        # Build \"master\" kernel between all DEEM and all IZA\n",
    "        K_train[cutoff] = build_kernel(iza_train+deem_train, iza_train+deem_train, \n",
    "                                       kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        K_test[cutoff] = build_kernel(iza_test+deem_test, iza_train+deem_train, \n",
    "                                      kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        K_test_test[cutoff] = build_kernel(iza_test+deem_test, iza_test+deem_test, \n",
    "                                           kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        \n",
    "        # Save kernels for later\n",
    "        g = h5py.File(kernel_file, 'w')\n",
    "        \n",
    "        g.create_dataset('K_train', data=K_train[cutoff])\n",
    "        g.create_dataset('K_test', data=K_test[cutoff])\n",
    "        g.create_dataset('K_test_test', data=K_test_test[cutoff])\n",
    "        \n",
    "        g.attrs['idxs_iza_train'] = idxs_iza_train\n",
    "        g.attrs['idxs_iza_test'] = idxs_iza_test\n",
    "        g.attrs['idxs_deem_train'] = idxs_deem_train\n",
    "        g.attrs['idxs_deem_test'] = idxs_deem_test\n",
    "        g.attrs['kernel_type'] = kernel_type[cutoff]\n",
    "        g.attrs['gamma'] = gamma[cutoff]\n",
    "        \n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite kernels with linear\n",
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Load SOAPs\n",
    "    deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    deem_soaps = load_structures_from_hdf5(deem_file, datasets=None, concatenate=False)\n",
    "\n",
    "    iza_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    iza_soaps = load_structures_from_hdf5(iza_file, datasets=None, concatenate=False)\n",
    "    iza_soaps.pop(RWY)\n",
    "\n",
    "    # Build the collection of soap vectors\n",
    "    # for the \"master\" kernel\n",
    "    deem_train = np.vstack([np.mean(deem_soaps[i], axis=0) for i in idxs_deem_train])\n",
    "    deem_test = np.vstack([np.mean(deem_soaps[i], axis=0) for i in idxs_deem_test])\n",
    "    iza_train = np.vstack([np.mean(iza_soaps[i], axis=0) for i in idxs_iza_train])\n",
    "    iza_test = np.vstack([np.mean(iza_soaps[i], axis=0) for i in idxs_iza_test])\n",
    "    \n",
    "    # Build \"master\" kernel between all DEEM and all IZA\n",
    "    K_train[cutoff] = linear_kernel(np.vstack((iza_train, deem_train)), \n",
    "                                    np.vstack((iza_train, deem_train)),\n",
    "                                    zeta=1)\n",
    "    K_test[cutoff] = linear_kernel(np.vstack((iza_test, deem_test)),\n",
    "                                   np.vstack((iza_train, deem_train)),\n",
    "                                   zeta=1)\n",
    "    K_test_test[cutoff] = linear_kernel(np.vstack((iza_test, deem_test)),\n",
    "                                        np.vstack((iza_test, deem_test)),\n",
    "                                        zeta=1)\n",
    "    \n",
    "    # We can also do this, but it is slow\n",
    "    #K_train[cutoff] = build_kernel(iza_train+deem_train, iza_train+deem_train, \n",
    "    #                               kernel='linear', zeta=1)\n",
    "    #K_test[cutoff] = build_kernel(iza_test+deem_test, iza_train+deem_train, \n",
    "    #                              kernel='linear', zeta=1)\n",
    "    #K_test_test[cutoff] = build_kernel(iza_test+deem_test, iza_test+deem_test, \n",
    "    #                                   kernel='linear', zeta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IZA indices for later\n",
    "# (we do this after the kernel loading to make sure that if an existing\n",
    "# kernel is loaded, the associated indices don't get overwritten)\n",
    "#np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/train.idxs', idxs_iza_train, fmt='%d')\n",
    "#np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/test.idxs', idxs_iza_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons_train = np.concatenate((cantons_iza[idxs_iza_train], cantons_deem[idxs_deem_train]))\n",
    "cantons_test = np.concatenate((cantons_iza[idxs_iza_test], cantons_deem[idxs_deem_test]))\n",
    "n_classes = np.amax(cantons_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and scale kernels\n",
    "for cutoff in cutoffs:\n",
    "    K_test[cutoff] = center_kernel_fast(K_test[cutoff], K_ref=K_train[cutoff])\n",
    "    K_train[cutoff] = center_kernel_fast(K_train[cutoff])\n",
    "\n",
    "    K_scale = np.trace(K_train[cutoff]) / K_train[cutoff].shape[0]\n",
    "    K_test[cutoff] /= K_scale\n",
    "    K_train[cutoff] /= K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on full test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different SVM regularization than the optimal\n",
    "C_override = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    if C_override is not None:\n",
    "        C = C_override\n",
    "    else:\n",
    "        C = model_dict['C']\n",
    "            \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "\n",
    "    # SVC\n",
    "    svc = SVC(kernel='precomputed', decision_function_shape=model_dict['df_type'], \n",
    "              class_weight=model_dict['class_weight'], C=C)\n",
    "    svc.fit(k_train, cantons_train)\n",
    "    \n",
    "    df_train = svc.decision_function(k_train)\n",
    "    df_test = svc.decision_function(k_test)\n",
    "    \n",
    "    predicted_cantons_train = svc.predict(k_train)\n",
    "    predicted_cantons_test = svc.predict(k_test)\n",
    "    \n",
    "    # Save decision functions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        if model_dict['df_type'] == 'ovo':\n",
    "            n_df = n_classes * (n_classes - 1) // 2\n",
    "        else:\n",
    "            n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "    \n",
    "    df_deem[idxs_deem_train] = df_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = df_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = df_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = df_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save KSVC class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress full (averaged) SOAP on Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should probably just do this \"legitimately\" with a linear SVM on the full average SOAPs\n",
    "# instead of regressing on the linear kernel SVM\n",
    "# Could also convert from the dual to the primal weights, \n",
    "# but based on the sk-learn implementation this seems a bit messy,\n",
    "# and it is still probably best just to do the linear SVM anyway.\n",
    "# For a quick first pass though, we'll regress on the decision function of a linear kernel SVM\n",
    "# built on the FPS'ed SOAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    \n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "\n",
    "    # Load SOAPs\n",
    "    soaps_deem = load_structures_from_hdf5(f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps_full_avg.hdf5',\n",
    "                                           datasets=None, concatenate=True)\n",
    "    soaps_iza = load_structures_from_hdf5(f'../Processed_Data/IZA_226/Data/{cutoff}/soaps_full_avg.hdf5',\n",
    "                                          datasets=None, concatenate=True)\n",
    "    \n",
    "    soaps_deem_train = soaps_deem[idxs_deem_train]\n",
    "    soaps_deem_test = soaps_deem[idxs_deem_test]\n",
    "    \n",
    "    soaps_iza_train = soaps_iza[idxs_iza_train]\n",
    "    soaps_iza_test = soaps_iza[idxs_iza_test]\n",
    "    \n",
    "    soaps_train = np.concatenate((soaps_iza_train, soaps_deem_train))\n",
    "    soaps_test = np.concatenate((soaps_iza_test, soaps_deem_test))\n",
    "    \n",
    "    soaps_center = np.mean(soaps_train, axis=0)\n",
    "    soaps_train -= soaps_center\n",
    "    soaps_test -= soaps_center\n",
    "    \n",
    "    soaps_scale = np.linalg.norm(soaps_train, axis=0) / np.sqrt(soaps_train.shape[0] / soaps_train.shape[1])\n",
    "    soaps_train /= soaps_scale\n",
    "    soaps_test /= soaps_scale\n",
    "\n",
    "    # Linear regression on decision functions\n",
    "    lr = LR(regularization=1.0E-12)\n",
    "    lr.fit(soaps_train, df_train)\n",
    "    \n",
    "    # Test that the prediction is good\n",
    "    dfp_train = lr.transform(soaps_train)\n",
    "    dfp_test = lr.transform(soaps_test)\n",
    "    \n",
    "    print(np.mean(np.abs(dfp_train - df_train), axis=0))\n",
    "    print(np.mean(np.abs(dfp_test - df_test), axis=0))\n",
    "\n",
    "    # Extract weights\n",
    "    w = lr.W.T\n",
    "    print(w.shape)\n",
    "\n",
    "    # Compute LR weight density\n",
    "    # TODO: set n_pairs, n_max, l_max in a robust way, but for now hard-code the hyperparameters\n",
    "    w = reshape_soaps(w, 3, 12, 9)\n",
    "    density = compute_soap_density(12, 9, cutoff, w,\n",
    "                                   np.linspace(0, cutoff, 50),\n",
    "                                   np.linspace(-1, 1, 50),\n",
    "                                   chunk_size_r=10, chunk_size_p=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import plotly.graph_objects as go\n",
    "rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(-1, 1, 50))\n",
    "fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                               y=ry_grid.flatten(),\n",
    "                               z=tz_grid.flatten(),\n",
    "                               value=density[0][0].flatten(),\n",
    "                               isomin=1000,\n",
    "                               isomax=None,\n",
    "                               opacity=0.2,\n",
    "                               surface_count=20))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that decision functions can be predicted with KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "    \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    \n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "\n",
    "    # Test KRR on decision functions\n",
    "    # NOTE: KRR can't predict the test set\n",
    "    # decision function very well -- why?\n",
    "    \n",
    "#     krr = KernelRidge(alpha=1.0E-12, kernel='precomputed')\n",
    "#     krr.fit(k_train, df_krr_train)\n",
    "#     dfp_krr_train = krr.predict(k_train)\n",
    "#     dfp_krr_test = krr.predict(k_test)\n",
    "\n",
    "    krr = KRR(regularization=1.0E-12)\n",
    "    krr.fit(k_train, df_train)\n",
    "    dfp_train = krr.transform(k_train)\n",
    "    dfp_test = krr.transform(k_test)\n",
    "    \n",
    "    print(np.mean(np.abs(dfp_train - df_train), axis=0))\n",
    "    print(np.mean(np.abs(dfp_test - df_test), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPCovR on full test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different number of components than that used for the optimization\n",
    "n_components_override = 6\n",
    "\n",
    "# Use an alpha other than the optimal\n",
    "alpha_override = 0.0\n",
    "\n",
    "# Use a regularization other than the optimal\n",
    "regularization_override = 1.0E-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/kpcovr_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        df_type = json.load(f)['df_type']\n",
    "        \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "        \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "    \n",
    "    # Set KPCovR parameters\n",
    "    if n_components_override is not None:\n",
    "        n_components = n_components_override\n",
    "    else:\n",
    "        n_components = model_dict['n_components']\n",
    "        \n",
    "    if alpha_override is not None:\n",
    "        alpha = alpha_override\n",
    "    else:\n",
    "        alpha = model_dict['alpha']\n",
    "        \n",
    "    if regularization_override is not None:\n",
    "        regularization = regularization_override\n",
    "    else:\n",
    "        regularization = model_dict['regularization']\n",
    "\n",
    "#     kpcovr = KPCovR2(n_components=n_components, kernel='precomputed',\n",
    "#                      mixing=alpha,\n",
    "#                      krr_params=dict(alpha=regularization))\n",
    "#     kpcovr.fit(k_train, y_train)\n",
    "\n",
    "#     T_train[cutoff] = kpcovr.transform(k_train)\n",
    "#     yp_train[cutoff] = kpcovr.predict(k_train) \n",
    "#     T_test[cutoff] = kpcovr.transform(k_test) \n",
    "#     yp_test[cutoff] = kpcovr.predict(k_test)\n",
    "\n",
    "    kpcovr = KPCovR(n_components=n_components, \n",
    "                    alpha=alpha, \n",
    "                    regularization=regularization)\n",
    "    kpcovr.fit(k_train, df_train)\n",
    "    \n",
    "    T_train = kpcovr.transform_K(k_train)\n",
    "    dfp_train = kpcovr.transform_Y(k_train)\n",
    "    T_test = kpcovr.transform_K(k_test)\n",
    "    dfp_test = kpcovr.transform_Y(k_test)\n",
    "    \n",
    "    dfp_train = np.squeeze(dfp_train) # TODO: move the squeezing to the KPCovR function\n",
    "    dfp_test = np.squeeze(dfp_test)\n",
    "        \n",
    "    # Save KPCovR projections\n",
    "    n_digits_deem = len(str(n_deem - 1))\n",
    "    T_deem = np.zeros((n_deem, n_components)) # TODO: change this so just 1 df for 2-class\n",
    "    T_deem[idxs_deem_train] = T_train[n_iza_train:]\n",
    "    T_deem[idxs_deem_test] = T_test[n_iza_test:]\n",
    "    \n",
    "    g = h5py.File(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structures.hdf5', 'w')\n",
    "    for tdx, t in enumerate(T_deem):\n",
    "        g.create_dataset(str(tdx).zfill(n_digits_deem), data=t)\n",
    "        \n",
    "    g.attrs['n_components'] = n_components\n",
    "    g.attrs['alpha'] = alpha\n",
    "    g.attrs['regularization'] = regularization\n",
    "    \n",
    "    g.close()\n",
    "    \n",
    "    n_digits_iza = len(str(n_iza - 1))\n",
    "    T_iza = np.zeros((n_iza, n_components))\n",
    "    T_iza[idxs_iza_train] = T_train[0:n_iza_train]\n",
    "    T_iza[idxs_iza_test] = T_test[0:n_iza_test]\n",
    "    \n",
    "    g = h5py.File(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structures.hdf5', 'w')\n",
    "    for tdx, t in enumerate(T_iza):\n",
    "        g.create_dataset(str(tdx).zfill(n_digits_iza), data=t)\n",
    "        \n",
    "    g.attrs['n_components'] = n_components\n",
    "    g.attrs['alpha'] = alpha\n",
    "    g.attrs['regularization'] = regularization\n",
    "        \n",
    "    g.close()\n",
    "                    \n",
    "    # Pickle the models\n",
    "    # Copy the dict so we can make the numpy arrays lists\n",
    "    kpcovr_dict = kpcovr.__dict__.copy()\n",
    "\n",
    "    # Convert arrays to lists\n",
    "    for k, v in kpcovr_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            kpcovr_dict[k] = v.tolist()\n",
    "\n",
    "    # Save\n",
    "    with open(f'{model_dir}/kpcovr.json', 'w') as f:\n",
    "        json.dump(kpcovr_dict, f)\n",
    "    \n",
    "    # Rescale to raw decision function\n",
    "    dfp_train = dfp_train * df_scale + df_center\n",
    "    dfp_test = dfp_test * df_scale + df_center\n",
    "\n",
    "    # Predict classes based on KPCovRized decision functions\n",
    "    predicted_cantons_train = df_to_class(dfp_train, df_type, n_classes, use_df_sums=True)\n",
    "    predicted_cantons_test = df_to_class(dfp_test, df_type, n_classes, use_df_sums=True)\n",
    "    \n",
    "    # Save KPCovR decision function predictions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        if df_type == 'ovo':\n",
    "            n_df = n_classes * (n_classes - 1) // 2\n",
    "        else:\n",
    "            n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "\n",
    "    df_deem[idxs_deem_train] = dfp_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = dfp_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = dfp_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = dfp_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save KPCovR class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
