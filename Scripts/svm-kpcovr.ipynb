{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/helfrech/.config/matplotlib/stylelib/cosmo.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In /home/helfrech/.config/matplotlib/stylelib/cosmoLarge.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "/home/helfrech/ENVIRONMENTS/ZEOLITES/lib/python3.6/_collections_abc.py:841: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ML\n",
    "from regression import PCovR, KPCovR, SparseKPCovR\n",
    "from regression import LR, KRR\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "from kernels import center_kernel, center_kernel_fast\n",
    "from kernels import center_kernel_oos, center_kernel_oos_fast\n",
    "from soap import compute_soap_density, reshape_soaps\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read\n",
    "from ase.neighborlist import neighbor_list\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/kernel-tutorials')\n",
    "# sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/KernelPCovR/analysis/scripts')\n",
    "# from utilities.sklearn_covr.kpcovr import KernelPCovR as KPCovR2\n",
    "# from utilities.sklearn_covr.pcovr import PCovR as PCovR2\n",
    "# from helpers import l_regr, l_kpcovr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_class(df, df_type, n_classes, use_df_sums=True):\n",
    "    \"\"\"\n",
    "        Make class predictions based on a decision function.\n",
    "        Based on the sci-kit learn SVC prediction, see\n",
    "        `sklearn.multiclass._ovr_decision_function` \n",
    "        (sci-kit learn licensed under BSD 3-Clause license)\n",
    "        TODO: could we also just use this function?\n",
    "        \n",
    "        ---Arguments---\n",
    "        df: decision function on which to make class predictions\n",
    "        df_type: decision function type, 'ovo' or 'ovr'\n",
    "        n_classes: number of integer classes\n",
    "        use_df_sums: augment the 'ovo' vote counts with\n",
    "            decision function values (useful for tie breaks)\n",
    "            \n",
    "        ---Returns---\n",
    "        predicted_class: predicted integer class\n",
    "    \"\"\"\n",
    "    \n",
    "    # Approximation to the number of classes, should be valid up to at least 1M\n",
    "    #n_classes = int(np.sqrt(2*df.shape[-1])) + 1\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        if df_type == 'ovo':\n",
    "            vote_matrix = np.zeros((df.shape[0], n_classes))\n",
    "            df_sum = np.zeros((df.shape[0], n_classes))\n",
    "\n",
    "            # Predicted class determined by majority vote\n",
    "            col_idx = 0\n",
    "            for i in range(0, n_classes):\n",
    "                for j in range(i + 1, n_classes):\n",
    "                    col_train = df[:, col_idx]\n",
    "                    vote_matrix[col_train > 0, i] += 1\n",
    "                    vote_matrix[col_train <= 0, j] += 1\n",
    "\n",
    "                    # Add value of decision function\n",
    "                    if use_df_sums:\n",
    "                        df_sum[:, i] += df[:, col_idx]\n",
    "                        df_sum[:, j] -= df[:, col_idx]\n",
    "\n",
    "                    col_idx += 1\n",
    "\n",
    "            # sci-kit learn transformation from 'ovo' to 'ovr'\n",
    "            if use_df_sums:\n",
    "                transformed_df_sum = df_sum / (3 * (np.abs(df_sum) + 1))\n",
    "                vote_matrix += transformed_df_sum\n",
    "\n",
    "            predicted_class = np.argmax(vote_matrix, axis=1) + 1\n",
    "\n",
    "        elif df_type == 'ovr':\n",
    "\n",
    "            # Predicted class determined by largest value of the decision function\n",
    "            predicted_class = np.argmax(df, axis=1) + 1\n",
    "\n",
    "        else:\n",
    "            print(\"Error: invalid decision function. Use 'ovo' or 'ovr'\")\n",
    "    else:\n",
    "        predicted_class = np.zeros(df.shape[0], dtype=int)\n",
    "        \n",
    "        # This appears to be the convention, which is \"opposite\" of that above\n",
    "        # Default exactly zero decision function value to the \"positive\" class\n",
    "        predicted_class[df >= 0] = 2\n",
    "        predicted_class[df < 0] = 1\n",
    "        \n",
    "    return predicted_class\n",
    "\n",
    "def rrw_neighbors(frame, center_species, env_species, cutoff, self_interaction=False):\n",
    "    \"\"\"\n",
    "        Compute the neighbor list for every atom of the central atom species\n",
    "        and generate the r, r', w for each pair of neighbors \n",
    "        \n",
    "        ---Arguments---\n",
    "        frame: atomic structure\n",
    "        center_species: species of atoms to use as centers\n",
    "        env_species: species of atoms to include in the environment\n",
    "        cutoff: atomic environment cutoff\n",
    "        self_interaction: include the central atom as its own neighbor\n",
    "        \n",
    "        ---Returns---\n",
    "        rrw: list of a list of numpy 3D numpy arrays. \n",
    "            Each numpy array is of shape (3, n_neighbors_a, n_neighbors_b),\n",
    "            where the axes are organized as follows:\n",
    "            axis=0: distances to neighbor A from the central atom\n",
    "            axis=1: distances to neighbor B from the central atom\n",
    "            axis=2: angle between the distance vectors to neighbors A and B from the central atom\n",
    "        idxs: same structure as rrw, but holds the indices of the atoms involved in the tuple, i.e.,\n",
    "            axis=0: index of central atom\n",
    "            axis=1: index of neighbor A\n",
    "            axis=2: index of neighbor B\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract indices of central atoms and environment atoms\n",
    "    center_species_idxs = [np.nonzero(frame.numbers == i)[0] for i in center_species]\n",
    "    env_species_idxs = [np.nonzero(frame.numbers == i)[0] for i in env_species]\n",
    "    \n",
    "    # Build neighbor list for all atoms\n",
    "    nl = {}\n",
    "    nl['i'], nl['j'], nl['d'], nl['D'] = neighbor_list('ijdD', frame, cutoff, \n",
    "                                                       self_interaction=self_interaction)\n",
    "    \n",
    "    rrw = []\n",
    "    idxs = []\n",
    "    \n",
    "    # Loop over centers grouped by species\n",
    "    for center_idxs in center_species_idxs:\n",
    "        for center in center_idxs:\n",
    "            \n",
    "            # Build subset of neighbor list that just has the neighbors of\n",
    "            # the center\n",
    "            center_nl_idxs = np.nonzero(nl['i'] == center)[0]\n",
    "            nl_center = {}\n",
    "            for k, v in nl.items():\n",
    "                nl_center[k] = v[center_nl_idxs]\n",
    "                \n",
    "            rrw_species = []\n",
    "            idxs_species = []\n",
    "                \n",
    "            # Loop over combinations of environment species\n",
    "            for env_species_a, env_species_b in itertools.combinations_with_replacement(env_species_idxs, 2):\n",
    "                a = np.nonzero(np.isin(nl_center['j'], env_species_a))[0]\n",
    "                b = np.nonzero(np.isin(nl_center['j'], env_species_b))[0]\n",
    "\n",
    "                # Extract distances to neighbors from the central atom (r, r')\n",
    "                da = nl_center['d'][a]\n",
    "                db = nl_center['d'][b]\n",
    "                Da = nl_center['D'][a]\n",
    "                Db = nl_center['D'][b]\n",
    "                r_n, r_m = np.meshgrid(da, db, indexing='ij')                \n",
    "                \n",
    "                # Compute angles between neighbors and central atom (w)\n",
    "                D = np.matmul(Da, Db.T)\n",
    "                d = np.outer(da, db)\n",
    "                d[d <= 0.0] = 1.0\n",
    "                w = D / d\n",
    "\n",
    "                # Extract indices of the atoms in the rr'w triplet\n",
    "                ia = nl_center['j'][a]\n",
    "                ib = nl_center['j'][b]\n",
    "                j_n, j_m = np.meshgrid(ia, ib, indexing='ij')\n",
    "                j_center = np.full(j_n.shape, center, dtype=int)\n",
    "                \n",
    "                # Build 3D matrix of rr'w triplets\n",
    "                rrw_species.append(np.stack((r_n, r_m, w)))\n",
    "                idxs_species.append(np.stack((j_center, j_n, j_m)))\n",
    "            \n",
    "            rrw.append(rrw_species)\n",
    "            idxs.append(idxs_species)\n",
    "    \n",
    "    return rrw, idxs\n",
    "\n",
    "def make_tuples(data):\n",
    "    \"\"\"\n",
    "        Take a list of lists of rr'w formatted 3D arrays (see rrw_neighbors)\n",
    "        and reshape into a list of lists of 2D arrays of shape (n_neighbor_pairs, 3),\n",
    "        where each row is a rr'w triplet and the columns are in the order r, r', w\n",
    "        \n",
    "        ---Arguments---\n",
    "        data: list of lists of arrays to \"reshape\"\n",
    "        \n",
    "        ---Returns---\n",
    "        center_tuple: \"reshaped\" data list\n",
    "    \"\"\"\n",
    "    n_centers = len(data)\n",
    "    center_tuple = []\n",
    "    \n",
    "    # Loop over centers\n",
    "    for nctr in range(0, n_centers):\n",
    "        n_pairs = len(data[nctr])\n",
    "        pair_tuple = []\n",
    "        \n",
    "        # Loop over species pairs\n",
    "        for npr in range(0, n_pairs):\n",
    "            data_shape = np.shape(data[nctr][npr])\n",
    "            \n",
    "            # Reshape the 3D array to a 2D array\n",
    "            tuple_array = np.reshape(np.moveaxis(data[nctr][npr], 0, -1), \n",
    "                                     (np.prod(data_shape[1:]), data_shape[0]))\n",
    "            \n",
    "            pair_tuple.append(tuple_array)\n",
    "        \n",
    "        center_tuple.append(pair_tuple)\n",
    "    \n",
    "    return center_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set indices for Deem\n",
    "idxs_deem_train = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "idxs_deem_test = np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_deem_train = idxs_deem_train.size\n",
    "n_deem_test = idxs_deem_test.size\n",
    "n_deem = n_deem_train + n_deem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "cantons_iza = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)\n",
    "RWY = np.nonzero(cantons_iza == 4)[0][0]\n",
    "cantons_iza = np.delete(cantons_iza, RWY)\n",
    "n_iza = len(cantons_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_iza = np.ones(n_iza, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IZA sample\n",
    "# (will be overwritten if we load a kernel\n",
    "n_iza_train = n_iza // 2\n",
    "n_iza_test = n_iza - n_iza_train\n",
    "idxs_iza = np.arange(0, n_iza)\n",
    "np.random.shuffle(idxs_iza)\n",
    "\n",
    "idxs_iza_train = idxs_iza[0:n_iza_train]\n",
    "idxs_iza_test = idxs_iza[n_iza_train:n_iza_train+n_iza_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SOAPs and build kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to recompute existing kernels\n",
    "remove_kernels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = {}\n",
    "K_test = {}\n",
    "K_test_test = {}\n",
    "kernel_type = {}\n",
    "gamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    kernel_file = f'../Processed_Data/Models/{cutoff}/structure_kernels.hdf5'\n",
    "    \n",
    "    # Start fresh\n",
    "    if remove_kernels and os.path.exists(kernel_file):\n",
    "        os.remove(kernel_file)\n",
    "    \n",
    "    # Load the kernels if they exist\n",
    "    try:\n",
    "        f = h5py.File(kernel_file, 'r')\n",
    "        \n",
    "        K_train[cutoff] = f['K_train'][:]\n",
    "        K_test[cutoff] = f['K_test'][:]\n",
    "        K_test_test[cutoff] = f['K_test_test'][:]\n",
    "        kernel_type[cutoff] = f.attrs['kernel_type']\n",
    "        gamma[cutoff] = f.attrs['gamma']\n",
    "        \n",
    "        # Don't need to store indices in a dictonary\n",
    "        # since they are the same for all cutoffs\n",
    "        idxs_iza_train = f.attrs['idxs_iza_train']\n",
    "        idxs_iza_test = f.attrs['idxs_iza_test']\n",
    "        idxs_deem_train = f.attrs['idxs_deem_train']\n",
    "        idxs_deem_test = f.attrs['idxs_deem_test']\n",
    " \n",
    "        f.close()\n",
    "    \n",
    "    # Compute the kernels if they don't exist\n",
    "    except OSError:\n",
    "    \n",
    "        # Load kernel parameters\n",
    "        model_file = f'../Processed_Data/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "\n",
    "        with open(model_file, 'r') as f:\n",
    "            model_dict = json.load(f)\n",
    "\n",
    "        kernel_type[cutoff] = model_dict['kernel_type']\n",
    "        gamma[cutoff] = model_dict['gamma']\n",
    "\n",
    "        # Load SOAPs\n",
    "        deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        deem_soaps = load_structures_from_hdf5(deem_file, datasets=None, concatenate=False)\n",
    "\n",
    "        iza_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        iza_soaps = load_structures_from_hdf5(iza_file, datasets=None, concatenate=False)\n",
    "        iza_soaps.pop(RWY)\n",
    "\n",
    "        # Build the collection of soap vectors\n",
    "        # for the \"master\" kernel\n",
    "        deem_train = [deem_soaps[i] for i in idxs_deem_train]\n",
    "        deem_test = [deem_soaps[i] for i in idxs_deem_test]\n",
    "        iza_train = [iza_soaps[i] for i in idxs_iza_train]\n",
    "        iza_test = [iza_soaps[i] for i in idxs_iza_test]\n",
    "\n",
    "        # Build \"master\" kernel between all DEEM and all IZA\n",
    "        K_train[cutoff] = build_kernel(iza_train+deem_train, iza_train+deem_train, \n",
    "                                       kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        K_test[cutoff] = build_kernel(iza_test+deem_test, iza_train+deem_train, \n",
    "                                      kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        K_test_test[cutoff] = build_kernel(iza_test+deem_test, iza_test+deem_test, \n",
    "                                           kernel=kernel_type[cutoff], gamma=gamma[cutoff])\n",
    "        \n",
    "        # Save kernels for later\n",
    "        g = h5py.File(kernel_file, 'w')\n",
    "        \n",
    "        g.create_dataset('K_train', data=K_train[cutoff])\n",
    "        g.create_dataset('K_test', data=K_test[cutoff])\n",
    "        g.create_dataset('K_test_test', data=K_test_test[cutoff])\n",
    "        \n",
    "        g.attrs['idxs_iza_train'] = idxs_iza_train\n",
    "        g.attrs['idxs_iza_test'] = idxs_iza_test\n",
    "        g.attrs['idxs_deem_train'] = idxs_deem_train\n",
    "        g.attrs['idxs_deem_test'] = idxs_deem_test\n",
    "        g.attrs['kernel_type'] = kernel_type[cutoff]\n",
    "        g.attrs['gamma'] = gamma[cutoff]\n",
    "        \n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Overwrite kernels with linear\n",
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Load SOAPs\n",
    "    deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    deem_soaps = load_structures_from_hdf5(deem_file, datasets=None, concatenate=False)\n",
    "\n",
    "    iza_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    iza_soaps = load_structures_from_hdf5(iza_file, datasets=None, concatenate=False)\n",
    "    iza_soaps.pop(RWY)\n",
    "\n",
    "    # Build the collection of soap vectors\n",
    "    # for the \"master\" kernel\n",
    "    deem_train = np.vstack([np.mean(deem_soaps[i], axis=0) for i in idxs_deem_train])\n",
    "    deem_test = np.vstack([np.mean(deem_soaps[i], axis=0) for i in idxs_deem_test])\n",
    "    iza_train = np.vstack([np.mean(iza_soaps[i], axis=0) for i in idxs_iza_train])\n",
    "    iza_test = np.vstack([np.mean(iza_soaps[i], axis=0) for i in idxs_iza_test])\n",
    "    \n",
    "    # Build \"master\" kernel between all DEEM and all IZA\n",
    "    K_train[cutoff] = linear_kernel(np.vstack((iza_train, deem_train)), \n",
    "                                    np.vstack((iza_train, deem_train)),\n",
    "                                    zeta=1)\n",
    "    K_test[cutoff] = linear_kernel(np.vstack((iza_test, deem_test)),\n",
    "                                   np.vstack((iza_train, deem_train)),\n",
    "                                   zeta=1)\n",
    "    K_test_test[cutoff] = linear_kernel(np.vstack((iza_test, deem_test)),\n",
    "                                        np.vstack((iza_test, deem_test)),\n",
    "                                        zeta=1)\n",
    "    \n",
    "    # We can also do this, but it is slow\n",
    "    #K_train[cutoff] = build_kernel(iza_train+deem_train, iza_train+deem_train, \n",
    "    #                               kernel='linear', zeta=1)\n",
    "    #K_test[cutoff] = build_kernel(iza_test+deem_test, iza_train+deem_train, \n",
    "    #                              kernel='linear', zeta=1)\n",
    "    #K_test_test[cutoff] = build_kernel(iza_test+deem_test, iza_test+deem_test, \n",
    "    #                                   kernel='linear', zeta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite with full SOAPs\n",
    "soaps_train = {}\n",
    "soaps_test = {}\n",
    "soaps_center = {}\n",
    "soaps_scale = {}\n",
    "for cutoff in cutoffs:\n",
    "    #deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps_full_avg.hdf5'\n",
    "    #deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps_full_avg_nonorm.hdf5'\n",
    "    deem_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps_radial_full_avg_nonorm.hdf5'\n",
    "    soaps_deem = load_structures_from_hdf5(deem_file, datasets=None, concatenate=True)\n",
    "    \n",
    "    #iza_file = f'../Processed_Data/IZA_226/Data/{cutoff}/soaps_full_avg.hdf5'\n",
    "    #iza_file = f'../Processed_Data/IZA_226/Data/{cutoff}/soaps_full_avg_nonorm.hdf5'\n",
    "    iza_file = f'../Processed_Data/IZA_226/Data/{cutoff}/soaps_radial_full_avg_nonorm.hdf5'\n",
    "    soaps_iza = load_structures_from_hdf5(iza_file, datasets=None, concatenate=True)\n",
    "    soaps_iza = np.delete(soaps_iza, RWY, axis=0)\n",
    "    \n",
    "    soaps_deem_train = soaps_deem[idxs_deem_train]\n",
    "    soaps_deem_test = soaps_deem[idxs_deem_test]\n",
    "    \n",
    "    soaps_iza_train = soaps_iza[idxs_iza_train]\n",
    "    soaps_iza_test = soaps_iza[idxs_iza_test]\n",
    "    \n",
    "    soaps_train[cutoff] = np.concatenate((soaps_iza_train, soaps_deem_train))\n",
    "    soaps_test[cutoff] = np.concatenate((soaps_iza_test, soaps_deem_test))\n",
    "    \n",
    "    soaps_center[cutoff] = np.zeros(soaps_train[cutoff].shape[1])\n",
    "    #soaps_scale[cutoff] = 1.0\n",
    "    soaps_scale[cutoff] = np.std(soaps_train[cutoff])\n",
    "    \n",
    "    #soaps_center[cutoff] = np.mean(soaps_train[cutoff], axis=0)\n",
    "    #soaps_train[cutoff] -= soaps_center[cutoff]\n",
    "    #soaps_test[cutoff] -= soaps_center[cutoff]\n",
    "    \n",
    "    #soaps_scale[cutoff] = np.linalg.norm(soaps_train[cutoff], axis=0) / np.sqrt(soaps_train[cutoff].shape[0] / soaps_train[cutoff].shape[1])\n",
    "    #soaps_scale[cutoff] = np.linalg.norm(soaps_train[cutoff]) / np.sqrt(soaps_train[cutoff].shape[0])\n",
    "    soaps_train[cutoff] /= soaps_scale[cutoff]\n",
    "    soaps_test[cutoff] /= soaps_scale[cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save IZA indices for later\n",
    "# (we do this after the kernel loading to make sure that if an existing\n",
    "# kernel is loaded, the associated indices don't get overwritten)\n",
    "#np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/train.idxs', idxs_iza_train, fmt='%d')\n",
    "#np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/test.idxs', idxs_iza_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons_train = np.concatenate((cantons_iza[idxs_iza_train], cantons_deem[idxs_deem_train]))\n",
    "cantons_test = np.concatenate((cantons_iza[idxs_iza_test], cantons_deem[idxs_deem_test]))\n",
    "n_classes = np.amax(cantons_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and scale kernels\n",
    "for cutoff in cutoffs:\n",
    "    K_test[cutoff] = center_kernel_fast(K_test[cutoff], K_ref=K_train[cutoff])\n",
    "    K_train[cutoff] = center_kernel_fast(K_train[cutoff])\n",
    "\n",
    "    K_scale = np.trace(K_train[cutoff]) / K_train[cutoff].shape[0]\n",
    "    K_test[cutoff] /= K_scale\n",
    "    K_train[cutoff] /= K_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on full test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different SVM regularization than the optimal\n",
    "C_override = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857542610022895\n",
      "0.9521794329242488\n",
      "[[-0.03557808  0.04348423 -0.04869769  0.81227903  1.60380599 -0.08174649\n",
      "  -1.09349027  0.86009147  0.01907264 -0.59745718  0.92694671  1.18224905\n",
      "  -0.19608589 -0.13700014 -0.0433833  -0.01767375  0.00207597  0.06435977\n",
      "  -0.2565276   0.31742197  0.76654069  0.08545561 -0.05903185 -0.04736582]]\n",
      "[-0.05248742]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00       113\n",
      "           2       0.95      1.00      0.98      2250\n",
      "\n",
      "    accuracy                           0.95      2363\n",
      "   macro avg       0.48      0.50      0.49      2363\n",
      "weighted avg       0.91      0.95      0.93      2363\n",
      "\n",
      "[[   0  113]\n",
      " [   0 2250]]\n",
      "0.9894428898499109\n",
      "0.9610664409648751\n",
      "[[ 1.10374281e-01  9.56425961e-02  1.19155951e+00 -1.01636374e+00\n",
      "  -5.54478461e-01  2.88024395e+00  8.00689031e-01  3.97893145e-01\n",
      "  -5.00260772e-01  2.59057339e-02  1.50644444e+00  1.19099079e+00\n",
      "  -5.47239597e-01 -1.45174214e-03 -8.20166276e-03  1.00447626e-02\n",
      "   2.22748143e-01  2.01122382e-01 -5.01179874e-01  5.84157010e-03\n",
      "   4.96239665e-02 -1.89086716e-01  8.54695944e-02  3.55107041e-02]]\n",
      "[-0.14468167]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.23      0.36       113\n",
      "           2       0.96      1.00      0.98      2250\n",
      "\n",
      "    accuracy                           0.96      2363\n",
      "   macro avg       0.90      0.61      0.67      2363\n",
      "weighted avg       0.96      0.96      0.95      2363\n",
      "\n",
      "[[  26   87]\n",
      " [   5 2245]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/helfrech/ENVIRONMENTS/ZEOLITES/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    if C_override is not None:\n",
    "        C = C_override\n",
    "    else:\n",
    "        C = model_dict['C']\n",
    "            \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "\n",
    "    # SVC\n",
    "#     svc = SVC(kernel='precomputed', decision_function_shape=model_dict['df_type'], \n",
    "#              class_weight=model_dict['class_weight'], C=C)\n",
    "#     svc.fit(k_train, cantons_train)\n",
    "    \n",
    "#     df_train = svc.decision_function(k_train)\n",
    "#     df_test = svc.decision_function(k_test)\n",
    "    \n",
    "#     predicted_cantons_train = svc.predict(k_train)\n",
    "#     predicted_cantons_test = svc.predict(k_test)\n",
    "    \n",
    "#     print(svc.score(k_train, cantons_train))\n",
    "#     print(svc.score(k_test, cantons_test))\n",
    "    \n",
    "    # TODO: if using LinearSVC, need to optimize with LinearSVC, but re-use C for now\n",
    "    # NOTE: no ovo option for LinearSVC\n",
    "    # NOTE: already centered data\n",
    "    # NOTE: l1 penalty doesn't seem to work so well here, but with logistic regression it is a little better\n",
    "    model_dict['df_type'] = 'ovr'\n",
    "    svc = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, C=1.0, \n",
    "                    multi_class=model_dict['df_type'], fit_intercept=True, \n",
    "                    tol=1.0E-3, max_iter=1000)\n",
    "    \n",
    "    svc.fit(soaps_train[cutoff], cantons_train)\n",
    "    \n",
    "    df_train = svc.decision_function(soaps_train[cutoff])\n",
    "    df_test = svc.decision_function(soaps_test[cutoff])\n",
    "    \n",
    "    predicted_cantons_train = svc.predict(soaps_train[cutoff])\n",
    "    predicted_cantons_test = svc.predict(soaps_test[cutoff])\n",
    "\n",
    "    print(svc.score(soaps_train[cutoff], cantons_train))\n",
    "    print(svc.score(soaps_test[cutoff], cantons_test))\n",
    "    print(svc.coef_)\n",
    "    print(svc.intercept_)\n",
    "    print(classification_report(cantons_test, predicted_cantons_test))\n",
    "    print(confusion_matrix(cantons_test, predicted_cantons_test))\n",
    "\n",
    "#     w = reshape_soaps(svc.coef_, 3, 12, 9)\n",
    "#     density = compute_soap_density(12, 9, cutoff, w,\n",
    "#                                    np.linspace(0, cutoff, 50),\n",
    "#                                    np.linspace(-1, 1, 50),\n",
    "#                                    chunk_size_r=10, chunk_size_p=10)\n",
    "\n",
    "    # Save decision functions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        if model_dict['df_type'] == 'ovo':\n",
    "            n_df = n_classes * (n_classes - 1) // 2\n",
    "        else:\n",
    "            n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "    \n",
    "    df_deem[idxs_deem_train] = df_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = df_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = df_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = df_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save SVC class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(cantons_test, predicted_cantons_test))\n",
    "print(confusion_matrix(cantons_test, predicted_cantons_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nonzero(predicted_cantons_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(soaps_train[6.0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(soaps_train[6.0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot\n",
    "import plotly.graph_objects as go\n",
    "rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(-1, 1, 50))\n",
    "fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                               y=ry_grid.flatten(),\n",
    "                               z=tz_grid.flatten(),\n",
    "                               value=density[0][0].flatten(),\n",
    "                               isomin=1,\n",
    "                               isomax=None,\n",
    "                               opacity=0.2,\n",
    "                               surface_count=20))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress full (averaged) SOAP on Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should probably just do this \"legitimately\" with a linear SVM on the full average SOAPs\n",
    "# instead of regressing on the linear kernel SVM\n",
    "# Could also convert from the dual to the primal weights, \n",
    "# but based on the sk-learn implementation this seems a bit messy,\n",
    "# and it is still probably best just to do the linear SVM anyway.\n",
    "# For a quick first pass though, we'll regress on the decision function of a linear kernel SVM\n",
    "# built on the FPS'ed SOAPs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    \n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "\n",
    "    # Load SOAPs\n",
    "    soaps_deem = load_structures_from_hdf5(f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps_full_avg.hdf5',\n",
    "                                           datasets=None, concatenate=True)\n",
    "    soaps_iza = load_structures_from_hdf5(f'../Processed_Data/IZA_226/Data/{cutoff}/soaps_full_avg.hdf5',\n",
    "                                          datasets=None, concatenate=True)\n",
    "    \n",
    "    soaps_deem_train = soaps_deem[idxs_deem_train]\n",
    "    soaps_deem_test = soaps_deem[idxs_deem_test]\n",
    "    \n",
    "    soaps_iza_train = soaps_iza[idxs_iza_train]\n",
    "    soaps_iza_test = soaps_iza[idxs_iza_test]\n",
    "    \n",
    "    soaps_train = np.concatenate((soaps_iza_train, soaps_deem_train))\n",
    "    soaps_test = np.concatenate((soaps_iza_test, soaps_deem_test))\n",
    "    \n",
    "    soaps_center = np.mean(soaps_train, axis=0)\n",
    "    soaps_train -= soaps_center\n",
    "    soaps_test -= soaps_center\n",
    "    \n",
    "    soaps_scale = np.linalg.norm(soaps_train, axis=0) / np.sqrt(soaps_train.shape[0] / soaps_train.shape[1])\n",
    "    soaps_train /= soaps_scale\n",
    "    soaps_test /= soaps_scale\n",
    "\n",
    "    # Linear regression on decision functions\n",
    "    lr = LR(regularization=1.0E-12)\n",
    "    lr.fit(soaps_train, df_train)\n",
    "    \n",
    "    # Test that the prediction is good\n",
    "    dfp_train = lr.transform(soaps_train)\n",
    "    dfp_test = lr.transform(soaps_test)\n",
    "    \n",
    "    print(np.mean(np.abs(dfp_train - df_train), axis=0))\n",
    "    print(np.mean(np.abs(dfp_test - df_test), axis=0))\n",
    "\n",
    "    # Extract weights\n",
    "    w = lr.W.T\n",
    "    print(w)\n",
    "\n",
    "    # Compute LR weight density\n",
    "    # TODO: set n_pairs, n_max, l_max in a robust way, but for now hard-code the hyperparameters\n",
    "    w = reshape_soaps(w, 3, 12, 9)\n",
    "    density = compute_soap_density(12, 9, cutoff, w,\n",
    "                                   np.linspace(0, cutoff, 50),\n",
    "                                   np.linspace(-1, 1, 50),\n",
    "                                   chunk_size_r=10, chunk_size_p=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Plot\n",
    "import plotly.graph_objects as go\n",
    "rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(0, 6.0, 50), \n",
    "                                        np.linspace(-1, 1, 50))\n",
    "fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                               y=ry_grid.flatten(),\n",
    "                               z=tz_grid.flatten(),\n",
    "                               value=density[0][0].flatten(),\n",
    "                               isomin=2000,\n",
    "                               isomax=None,\n",
    "                               opacity=0.2,\n",
    "                               surface_count=20))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    if C_override is not None:\n",
    "        C = C_override\n",
    "    else:\n",
    "        C = model_dict['C']\n",
    "    \n",
    "    # TODO: if using LogisticRegression, need to optimize with LogisticRegression, but re-use C for now\n",
    "    # NOTE: no ovo option for LogisticRegression\n",
    "    # NOTE: logistic regression seems to want to predict everything as DEEM, moreso than SVM\n",
    "    logr = LogisticRegression(penalty='l2', dual=False, C=C, \n",
    "                              fit_intercept=True, solver='saga',\n",
    "                              multi_class='auto', max_iter=1000, tol=1.0E-3)\n",
    "    \n",
    "    logr.fit(soaps_train[cutoff], cantons_train)\n",
    "    \n",
    "    df_train = logr.decision_function(soaps_train[cutoff])\n",
    "    df_test = logr.decision_function(soaps_test[cutoff])\n",
    "    \n",
    "    predicted_cantons_train = logr.predict(soaps_train[cutoff])\n",
    "    predicted_cantons_test = logr.predict(soaps_test[cutoff])\n",
    "    \n",
    "    probabilities_train = logr.predict_proba(soaps_train[cutoff])\n",
    "    probabilities_test = logr.predict_proba(soaps_test[cutoff])\n",
    "    \n",
    "    # Save decision functions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "    \n",
    "    df_deem[idxs_deem_train] = df_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = df_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = df_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = df_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save LogR class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "    # Save LogR probabilities\n",
    "    probabilities_deem = np.zeros((n_deem, n_classes))\n",
    "    probabilities_deem[idxs_deem_train] = probabilities_train[n_iza_train:]\n",
    "    probabilities_deem[idxs_deem_test] = probabilities_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    probabilities_iza = np.zeros((n_iza, n_classes))\n",
    "    probabilities_iza[idxs_iza_train] = probabilities_train[0:n_iza_train]\n",
    "    probabilities_iza[idxs_iza_test] = probabilities_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "#########\n",
    "    print(logr.score(soaps_train[cutoff], cantons_train))\n",
    "    print(logr.score(soaps_test[cutoff], cantons_test))\n",
    "    print(logr.coef_)\n",
    "    w = reshape_soaps(logr.coef_, 3, 12, 9)\n",
    "    density = compute_soap_density(12, 9, cutoff, w,\n",
    "                                   np.linspace(0, cutoff, 50),\n",
    "                                   np.linspace(-1, 1, 50),\n",
    "                                   chunk_size_r=10, chunk_size_p=10)\n",
    "#########"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logr.n_iter_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(probabilities_deem)\n",
    "print(probabilities_iza)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem, axis=1))\n",
    "print(np.amax(probabilities_iza, axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem))\n",
    "print(np.amax(probabilities_iza))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(probabilities_deem[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.hist(probabilities_iza[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 6.0\n",
    "coef = svc.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419 111\n"
     ]
    }
   ],
   "source": [
    "df_max = np.argmax(np.matmul(soaps_test[cutoff], coef.T))\n",
    "df_min = np.argmin(np.matmul(soaps_test[cutoff], coef.T))\n",
    "print(df_max, df_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.40345322])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(coef, soaps_test[cutoff][df_min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.25753674])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(coef, soaps_test[cutoff][df_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16.25753674])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_soap = soaps_test[cutoff][df_max]*soaps_scale[cutoff]+soaps_center[cutoff]\n",
    "w = coef * (1.0 - soaps_center[cutoff] / unscaled_soap) / soaps_scale[cutoff]\n",
    "np.dot(w, unscaled_soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "soaps_scale[cutoff] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOAP density\n",
    "soap_density = compute_soap_density(12, 9, cutoff,\n",
    "                                    reshape_soaps(soaps_test[cutoff][[df_max, df_min]]*soaps_scale[cutoff]+soaps_center[cutoff],\n",
    "                                                  3, 12, 9),\n",
    "                                    np.linspace(0, cutoff, 50), \n",
    "                                    np.linspace(-1, 1, 50),\n",
    "                                    chunk_size_r=10, chunk_size_p=10)\n",
    "\n",
    "mean_soap_density = compute_soap_density(12, 9, cutoff,\n",
    "                                         reshape_soaps(np.mean(soaps_train[cutoff]*soaps_scale[cutoff]+soaps_center[cutoff], axis=0),\n",
    "                                         3, 12, 9),\n",
    "                                         np.linspace(0.0, cutoff, 50),\n",
    "                                         np.linspace(-1, 1, 50),\n",
    "                                         chunk_size_r=10, chunk_size_p=10)\n",
    "\n",
    "density_overlap = density*soap_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_space_w = coef * (1.0 - soaps_center[cutoff] / soaps_test[cutoff][[df_max, df_min]]) / soaps_scale[cutoff]\n",
    "real_space_w = coef\n",
    "real_space_density = compute_soap_density(12, 9, cutoff,\n",
    "                                    reshape_soaps(real_space_w, 3, 12, 9),\n",
    "                                    np.linspace(0, cutoff, 50), \n",
    "                                    np.linspace(-1, 1, 50),\n",
    "                                    chunk_size_r=10, chunk_size_p=10)\n",
    "\n",
    "density_overlap = real_space_density*soap_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 50, 50, 50)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_overlap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.94566244, -4.53828658])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr = np.diff(np.linspace(0, cutoff, 50))[0]\n",
    "dp = np.diff(np.linspace(-1, 1, 50))[0]\n",
    "np.sum(density_overlap, axis=(1, 2, 3, 4))*dr*dr*dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.039897703813097, 48.6212760239801)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(soap_density), np.amax(soap_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.774295566411343, 2.930572774027066)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(real_space_density), np.amax(real_space_density)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.all(real_space_density[0] == real_space_density[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./real_space_soap_mean.npy', mean_soap_density)\n",
    "np.save('./real_space_soap.npy', soap_density)\n",
    "np.save('./real_space_weights.npy', density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./nnl_soap_scaling.npy', soaps_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_10k = read('../Raw_Data/DEEM_10k/DEEM_10000.xyz', index=':')\n",
    "iza = read('../Raw_Data/GULP/IZA_226/IZA_OPT.xyz', index=':')\n",
    "deem_10k_test = [deem_10k[i] for i in idxs_deem_test]\n",
    "iza_test = [iza[i] for i in idxs_iza_test]\n",
    "frames_test = iza_test + deem_10k_test\n",
    "soap_neighbors, idxs_neighbors = zip(*[rrw_neighbors(frames_test[i], [14], [8, 14], cutoff, \n",
    "                                                     self_interaction=True) for i in [df_max, df_min]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: O-O\n",
    "# 1: Si-O\n",
    "# 2: Si-Si\n",
    "species_pair_idx = 0\n",
    "soap_idx = 0\n",
    "\n",
    "def convert_species_idx(species_pair_idx):\n",
    "    if species_pair_idx == 0:\n",
    "        species_pair_label = 'OO'\n",
    "    elif species_pair_idx == 1:\n",
    "        species_pair_label = 'OSi'\n",
    "    elif species_pair_idx == 2:\n",
    "        species_pair_label = 'SiSi'\n",
    "    else:\n",
    "        species_pair_label = 'XX'\n",
    "    return species_pair_label\n",
    " \n",
    "def convert_soap_idx(soap_idx):\n",
    "    if soap_idx == 0:\n",
    "        soap_label = 'DEEM'\n",
    "    elif soap_idx == 1:\n",
    "        soap_label = 'IZA'\n",
    "    else:\n",
    "        soap_label = 'XXX'\n",
    "    return soap_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "for species_pair_idx in range(0, 3):\n",
    "    species_pair_label = convert_species_idx(species_pair_idx)\n",
    "    \n",
    "    rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, cutoff, 50), \n",
    "                                            np.linspace(0, cutoff, 50), \n",
    "                                            np.linspace(-1, 1, 50), indexing='ij')\n",
    "\n",
    "    fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                                   y=ry_grid.flatten(),\n",
    "                                   z=tz_grid.flatten(),\n",
    "                                   value=real_space_density[0][species_pair_idx].flatten(),\n",
    "                                   #value=real_space_density[soap_idx][species_pair_idx].flatten(),\n",
    "                                   coloraxis='coloraxis',\n",
    "                                   isomin=-1.75E+0,\n",
    "                                   isomax=1.75E+0,\n",
    "                                   opacity=0.6,\n",
    "                                   surface_count=4,\n",
    "                                   caps=dict(x_show=False, y_show=False, z_show=False)))\n",
    "\n",
    "    atom_stencil_x_deem = np.concatenate([soap_neighbors[0][center][species_pair_idx][0].flatten() \n",
    "                                     for center in range(0, len(soap_neighbors[0]))])\n",
    "    atom_stencil_y_deem = np.concatenate([soap_neighbors[0][center][species_pair_idx][1].flatten()\n",
    "                                     for center in range(0, len(soap_neighbors[0]))])\n",
    "    atom_stencil_z_deem = np.concatenate([soap_neighbors[0][center][species_pair_idx][2].flatten()\n",
    "                                     for center in range(0, len(soap_neighbors[0]))])\n",
    "\n",
    "    atom_stencil_x_iza = np.concatenate([soap_neighbors[1][center][species_pair_idx][0].flatten() \n",
    "                                     for center in range(0, len(soap_neighbors[1]))])\n",
    "    atom_stencil_y_iza = np.concatenate([soap_neighbors[1][center][species_pair_idx][1].flatten()\n",
    "                                     for center in range(0, len(soap_neighbors[1]))])\n",
    "    atom_stencil_z_iza = np.concatenate([soap_neighbors[1][center][species_pair_idx][2].flatten()\n",
    "                                     for center in range(0, len(soap_neighbors[1]))])\n",
    "\n",
    "    idx_x_deem = np.concatenate([idxs_neighbors[0][center][species_pair_idx][0].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[0]))])\n",
    "    idx_y_deem = np.concatenate([idxs_neighbors[0][center][species_pair_idx][1].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[0]))])\n",
    "    idx_z_deem = np.concatenate([idxs_neighbors[0][center][species_pair_idx][2].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[0]))])\n",
    "\n",
    "    idx_x_iza = np.concatenate([idxs_neighbors[1][center][species_pair_idx][0].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[1]))])\n",
    "    idx_y_iza = np.concatenate([idxs_neighbors[1][center][species_pair_idx][1].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[1]))])\n",
    "    idx_z_iza = np.concatenate([idxs_neighbors[1][center][species_pair_idx][2].flatten() \n",
    "                            for center in range(0, len(soap_neighbors[1]))])\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(x=atom_stencil_x_deem,\n",
    "                               y=atom_stencil_y_deem,\n",
    "                               z=atom_stencil_z_deem,\n",
    "                               name=f'DEEM, {species_pair_label}',\n",
    "                               mode='markers',\n",
    "                               marker=dict(size=1,\n",
    "                                           color='green'),\n",
    "                               hovertemplate='x: %{x}<br>y: %{y}<br>z: %{z}<br>(i, j): %{text}',\n",
    "                               text=['{}'.format(i) for i in zip(idx_x_deem, idx_y_deem, idx_z_deem)],\n",
    "                               showlegend=True))\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(x=atom_stencil_x_iza,\n",
    "                               y=atom_stencil_y_iza,\n",
    "                               z=atom_stencil_z_iza,\n",
    "                               name=f'IZA, {species_pair_label}',\n",
    "                               mode='markers',\n",
    "                               marker=dict(size=2,\n",
    "                                           color='purple'),\n",
    "                               hovertemplate='x: %{x}<br>y: %{y}<br>z: %{z}<br>(i, j): %{text}',\n",
    "                               text=['{}'.format(i) for i in zip(idx_x_iza, idx_y_iza, idx_z_iza)],\n",
    "                               showlegend=True))\n",
    "\n",
    "    fig.update_layout(template='plotly_white',\n",
    "                      scene=dict(xaxis_title='r',\n",
    "                                 yaxis_title='r\\'',\n",
    "                                 zaxis_title='w'),\n",
    "                      legend=dict(x=0.0, y=1.0,\n",
    "                                  xanchor='left', yanchor='top',\n",
    "                                  itemsizing='constant'),\n",
    "                      coloraxis=dict(colorscale='RdBu',\n",
    "                                     colorbar=dict(title='Weights')),\n",
    "                      autosize=True)\n",
    "\n",
    "    #fig.show()\n",
    "    #fig.write_html(f'../Results/{cutoff}/real_space_weights-{soap_label}-{species_pair_label}.html')\n",
    "    fig.write_html(f'../Results/{cutoff}/real_space_weights-IZA-DEEM-{species_pair_label}.html')\n",
    "    fig.write_image(f'../Results/{cutoff}/real_space_weights-IZA-DEEM-{species_pair_label}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "for species_pair_idx in range(0, 3):\n",
    "    species_pair_label = convert_species_idx(species_pair_idx)\n",
    "    for soap_idx in range(0, 2):\n",
    "        soap_label = convert_soap_idx(soap_idx)\n",
    "        \n",
    "        rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, cutoff, 50), \n",
    "                                                np.linspace(0, cutoff, 50), \n",
    "                                                np.linspace(-1, 1, 50), indexing='ij')\n",
    "\n",
    "        fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                                       y=ry_grid.flatten(),\n",
    "                                       z=tz_grid.flatten(),\n",
    "                                       value=soap_density[soap_idx][species_pair_idx].flatten(),\n",
    "                                       coloraxis='coloraxis',\n",
    "                                       isomin=1.0E+0,\n",
    "                                       isomax=None,\n",
    "                                       opacity=0.2,\n",
    "                                       surface_count=20))\n",
    "\n",
    "        atom_stencil_x = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][0].flatten() \n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        atom_stencil_y = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][1].flatten()\n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        atom_stencil_z = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][2].flatten()\n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "\n",
    "        idx_x = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][0].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        idx_y = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][1].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        idx_z = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][2].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(x=atom_stencil_x,\n",
    "                                   y=atom_stencil_y,\n",
    "                                   z=atom_stencil_z,\n",
    "                                   name=f'{soap_label}, {species_pair_label}',\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(size=1,\n",
    "                                               color='cyan'),\n",
    "                                   hovertemplate='x: %{x}<br>y: %{y}<br>z: %{z}<br>(i, j): %{text}',\n",
    "                                   text=['{}'.format(i) for i in zip(idx_x, idx_y, idx_z)],\n",
    "                                   showlegend=True))\n",
    "\n",
    "        fig.update_layout(template='plotly_white',\n",
    "                          scene=dict(xaxis_title='r',\n",
    "                                     yaxis_title='r\\'',\n",
    "                                     zaxis_title='w'),\n",
    "                          legend=dict(x=0.0, y=1.0,\n",
    "                                      xanchor='left', yanchor='top',\n",
    "                                      itemsizing='constant'),\n",
    "                          coloraxis=dict(colorscale='Plasma',\n",
    "                                         colorbar=dict(title='Density')),\n",
    "                          autosize=True)\n",
    "\n",
    "        #fig.show()\n",
    "        fig.write_html(f'../Results/{cutoff}/real_space_soap-{soap_label}-{species_pair_label}.html')\n",
    "        fig.write_image(f'../Results/{cutoff}/real_space_soap-{soap_label}-{species_pair_label}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "for species_pair_idx in range(0, 3):\n",
    "    species_pair_label = convert_species_idx(species_pair_idx)\n",
    "    for soap_idx in range(0, 2):\n",
    "        soap_label = convert_soap_idx(soap_idx)\n",
    "        rx_grid, ry_grid, tz_grid = np.meshgrid(np.linspace(0, cutoff, 50), \n",
    "                                                np.linspace(0, cutoff, 50), \n",
    "                                                np.linspace(-1, 1, 50), indexing='ij')\n",
    "\n",
    "        fig = go.Figure(data=go.Volume(x=rx_grid.flatten(),\n",
    "                                       y=ry_grid.flatten(),\n",
    "                                       z=tz_grid.flatten(),\n",
    "                                       value=density_overlap[soap_idx][species_pair_idx].flatten()/10,\n",
    "                                       coloraxis='coloraxis',\n",
    "                                       isomin=-1.75E+0,\n",
    "                                       isomax=1.75E+0,\n",
    "                                       opacity=0.6,\n",
    "                                       surface_count=4,\n",
    "                                       caps=dict(x_show=False, y_show=False, z_show=False)))\n",
    "\n",
    "        atom_stencil_x = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][0].flatten() \n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        atom_stencil_y = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][1].flatten()\n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        atom_stencil_z = np.concatenate([soap_neighbors[soap_idx][center][species_pair_idx][2].flatten()\n",
    "                                         for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "\n",
    "        idx_x = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][0].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        idx_y = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][1].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "        idx_z = np.concatenate([idxs_neighbors[soap_idx][center][species_pair_idx][2].flatten() \n",
    "                                for center in range(0, len(soap_neighbors[soap_idx]))])\n",
    "\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(x=atom_stencil_x,\n",
    "                                   y=atom_stencil_y,\n",
    "                                   z=atom_stencil_z,\n",
    "                                   name=f'{soap_label}, {species_pair_label}',\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(size=1,\n",
    "                                               color='green'),\n",
    "                                   hovertemplate='x: %{x}<br>y: %{y}<br>z: %{z}<br>(i, j): %{text}',\n",
    "                                   text=['{}'.format(i) for i in zip(idx_x, idx_y, idx_z)],\n",
    "                                   showlegend=True))\n",
    "\n",
    "        fig.update_layout(template='plotly_white',\n",
    "                          scene=dict(xaxis_title='r',\n",
    "                                     yaxis_title='r\\'',\n",
    "                                     zaxis_title='w'),\n",
    "                          legend=dict(x=0.0, y=1.0,\n",
    "                                      xanchor='left', yanchor='top',\n",
    "                                      itemsizing='constant'),\n",
    "                          coloraxis=dict(colorscale='RdBu',\n",
    "                                         colorbar=dict(title='Density*Weights')),\n",
    "                          autosize=True)\n",
    "\n",
    "        #fig.show()\n",
    "        fig.write_html(f'../Results/{cutoff}/real_space_overlap-{soap_label}-{species_pair_label}.html')\n",
    "        fig.write_image(f'../Results/{cutoff}/real_space_overlap-{soap_label}-{species_pair_label}.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nglview\n",
    "view = nglview.show_ase(deem_10k_test[df_min])\n",
    "view.add_representation('ball+stick', selection=[47], color='black', radius=0.3)\n",
    "view.add_representation('ball+stick', selection=[67, 47], color='blue', radius=0.3)\n",
    "view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that decision functions can be predicted with KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "    \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    \n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "\n",
    "    # Test KRR on decision functions\n",
    "    # NOTE: KRR can't predict the test set\n",
    "    # decision function very well -- why? <-- TODO: is this only for LinearSVC or also SVC?\n",
    "    \n",
    "#     krr = KernelRidge(alpha=1.0E-12, kernel='precomputed')\n",
    "#     krr.fit(k_train, df_krr_train)\n",
    "#     dfp_krr_train = krr.predict(k_train)\n",
    "#     dfp_krr_test = krr.predict(k_test)\n",
    "\n",
    "    krr = KRR(regularization=1.0E-12)\n",
    "    krr.fit(k_train, df_train)\n",
    "    dfp_train = krr.transform(k_train)\n",
    "    dfp_test = krr.transform(k_test)\n",
    "    \n",
    "    print(np.mean(np.abs(dfp_train - df_train), axis=0))\n",
    "    print(np.mean(np.abs(dfp_test - df_test), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPCovR on full test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a different number of components than that used for the optimization\n",
    "n_components_override = 6\n",
    "\n",
    "# Use an alpha other than the optimal\n",
    "alpha_override = 0.0\n",
    "\n",
    "# Use a regularization other than the optimal\n",
    "regularization_override = 1.0E-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/kpcovr_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        df_type = json.load(f)['df_type']\n",
    "        \n",
    "    # Assemble kernels\n",
    "    k_train = K_train[cutoff]\n",
    "    k_test = K_test[cutoff]\n",
    "        \n",
    "    # Load decision functions\n",
    "    df_deem = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    df_iza = np.loadtxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/ksvc_structure_dfs.dat')\n",
    "    \n",
    "    df_deem_train = df_deem[idxs_deem_train]\n",
    "    df_deem_test = df_deem[idxs_deem_test]\n",
    "    \n",
    "    df_iza_train = df_iza[idxs_iza_train]\n",
    "    df_iza_test = df_iza[idxs_iza_test]\n",
    "    \n",
    "    df_train = np.concatenate((df_iza_train, df_deem_train))\n",
    "    df_test = np.concatenate((df_iza_test, df_deem_test))\n",
    "    \n",
    "    # Center and scale decision functions\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "\n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "    \n",
    "    # Set KPCovR parameters\n",
    "    if n_components_override is not None:\n",
    "        n_components = n_components_override\n",
    "    else:\n",
    "        n_components = model_dict['n_components']\n",
    "        \n",
    "    if alpha_override is not None:\n",
    "        alpha = alpha_override\n",
    "    else:\n",
    "        alpha = model_dict['alpha']\n",
    "        \n",
    "    if regularization_override is not None:\n",
    "        regularization = regularization_override\n",
    "    else:\n",
    "        regularization = model_dict['regularization']\n",
    "\n",
    "#     kpcovr = KPCovR2(n_components=n_components, kernel='precomputed',\n",
    "#                      mixing=alpha,\n",
    "#                      krr_params=dict(alpha=regularization))\n",
    "#     kpcovr.fit(k_train, y_train)\n",
    "\n",
    "#     T_train[cutoff] = kpcovr.transform(k_train)\n",
    "#     yp_train[cutoff] = kpcovr.predict(k_train) \n",
    "#     T_test[cutoff] = kpcovr.transform(k_test) \n",
    "#     yp_test[cutoff] = kpcovr.predict(k_test)\n",
    "\n",
    "    kpcovr = KPCovR(n_components=n_components, \n",
    "                    alpha=alpha, \n",
    "                    regularization=regularization)\n",
    "    kpcovr.fit(k_train, df_train)\n",
    "    \n",
    "    T_train = kpcovr.transform_K(k_train)\n",
    "    dfp_train = kpcovr.transform_Y(k_train)\n",
    "    T_test = kpcovr.transform_K(k_test)\n",
    "    dfp_test = kpcovr.transform_Y(k_test)\n",
    "    \n",
    "    dfp_train = np.squeeze(dfp_train) # TODO: move the squeezing to the KPCovR function\n",
    "    dfp_test = np.squeeze(dfp_test)\n",
    "        \n",
    "    # Save KPCovR projections\n",
    "    n_digits_deem = len(str(n_deem - 1))\n",
    "    T_deem = np.zeros((n_deem, n_components)) # TODO: change this so just 1 df for 2-class\n",
    "    T_deem[idxs_deem_train] = T_train[n_iza_train:]\n",
    "    T_deem[idxs_deem_test] = T_test[n_iza_test:]\n",
    "    \n",
    "    g = h5py.File(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structures.hdf5', 'w')\n",
    "    for tdx, t in enumerate(T_deem):\n",
    "        g.create_dataset(str(tdx).zfill(n_digits_deem), data=t)\n",
    "        \n",
    "    g.attrs['n_components'] = n_components\n",
    "    g.attrs['alpha'] = alpha\n",
    "    g.attrs['regularization'] = regularization\n",
    "    \n",
    "    g.close()\n",
    "    \n",
    "    n_digits_iza = len(str(n_iza - 1))\n",
    "    T_iza = np.zeros((n_iza, n_components))\n",
    "    T_iza[idxs_iza_train] = T_train[0:n_iza_train]\n",
    "    T_iza[idxs_iza_test] = T_test[0:n_iza_test]\n",
    "    \n",
    "    g = h5py.File(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structures.hdf5', 'w')\n",
    "    for tdx, t in enumerate(T_iza):\n",
    "        g.create_dataset(str(tdx).zfill(n_digits_iza), data=t)\n",
    "        \n",
    "    g.attrs['n_components'] = n_components\n",
    "    g.attrs['alpha'] = alpha\n",
    "    g.attrs['regularization'] = regularization\n",
    "        \n",
    "    g.close()\n",
    "                    \n",
    "    # Pickle the models\n",
    "    # Copy the dict so we can make the numpy arrays lists\n",
    "    kpcovr_dict = kpcovr.__dict__.copy()\n",
    "\n",
    "    # Convert arrays to lists\n",
    "    for k, v in kpcovr_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            kpcovr_dict[k] = v.tolist()\n",
    "\n",
    "    # Save\n",
    "    with open(f'{model_dir}/kpcovr.json', 'w') as f:\n",
    "        json.dump(kpcovr_dict, f)\n",
    "    \n",
    "    # Rescale to raw decision function\n",
    "    dfp_train = dfp_train * df_scale + df_center\n",
    "    dfp_test = dfp_test * df_scale + df_center\n",
    "\n",
    "    # Predict classes based on KPCovRized decision functions\n",
    "    predicted_cantons_train = df_to_class(dfp_train, df_type, n_classes, use_df_sums=True)\n",
    "    predicted_cantons_test = df_to_class(dfp_test, df_type, n_classes, use_df_sums=True)\n",
    "    \n",
    "    # Save KPCovR decision function predictions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        if df_type == 'ovo':\n",
    "            n_df = n_classes * (n_classes - 1) // 2\n",
    "        else:\n",
    "            n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "\n",
    "    df_deem[idxs_deem_train] = dfp_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = dfp_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = dfp_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = dfp_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save KPCovR class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/kpcovr_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/kpcovr_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
