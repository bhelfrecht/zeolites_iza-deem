{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/helfrech/.config/matplotlib/stylelib/cosmo.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In /home/helfrech/.config/matplotlib/stylelib/cosmoLarge.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read\n",
    "\n",
    "# ML\n",
    "from kernels import (build_kernel, linear_kernel, gaussian_kernel,\n",
    "                     linear_kernel_diag, gaussian_kernel_diag,\n",
    "                     linear_kernel_tri, gaussian_kernel_tri,\n",
    "                     diag_indices, tri_indices)\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generalize for different kernels\n",
    "# TODO: iterative gaussian KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type for loading FPS indices\n",
    "dt = np.dtype({'names': ['idxs', 'distances'], 'formats': ['i8', 'f8']})\n",
    "\n",
    "# Distance cutoff for unique environments\n",
    "unique_cutoff = 1.0E-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram bin setup\n",
    "bin_min = -4\n",
    "bin_max = 1\n",
    "n_bins = 101\n",
    "bin_edges = np.logspace(bin_min, bin_max, n_bins)\n",
    "dx = np.diff(bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file header for histograms\n",
    "column_labels = 'bin_edges bin_widths probability_density'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_histogram(model_file, soaps_A, soaps_A_name='soaps_A', \n",
    "                        soaps_B=None, soaps_B_name='soaps_B', \n",
    "                        header=None, work_dir='.'):\n",
    "    \n",
    "    if soaps_B is None:\n",
    "        soaps_B_name = soaps_A_name\n",
    "    \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Distance matrix for different datasets\n",
    "    if soaps_B is not None:\n",
    "        \n",
    "        # Compute kernel diagonals for each dataset\n",
    "        KAA = gaussian_kernel_diag(soaps_A, soaps_A, gamma=gamma, k=0)\n",
    "        KAA = np.reshape(KAA, (-1, 1))\n",
    "        KBB = gaussian_kernel_diag(soaps_B, soaps_B, gamma=gamma, k=0)\n",
    "\n",
    "        # Compute kernel between datasets\n",
    "        DAB = -2.0*build_kernel(soaps_A, soaps_B, kernel=kernel_type, gamma=gamma)\n",
    "\n",
    "        # Compute kernel distance\n",
    "        DAB += KAA + KBB \n",
    "        DAB = np.sqrt(DAB)\n",
    "                \n",
    "        # Compute minimum distance for each environment in each dataset\n",
    "        # min_AB = distance from an A to the nearest B for every A\n",
    "        # min_BA = distance from a B to the nearest A for every B\n",
    "        DAB_min_AB = np.amin(DAB, axis=1)\n",
    "        DAB_min_BA = np.amin(DAB, axis=0)\n",
    "\n",
    "        # Compute distance histograms\n",
    "        H, _ = np.histogram(DAB, bins=bin_edges, density=True)\n",
    "        H_min_AB, _ = np.histogram(DAB_min_AB, bins=bin_edges, density=True)\n",
    "        H_min_BA, _ = np.histogram(DAB_min_BA, bins=bin_edges, density=True)\n",
    "\n",
    "        # Compute KDEs\n",
    "        #kde = gaussian_kde(DAB)\n",
    "        #H_kde = kde(bin_edges[0:-1])\n",
    "        #\n",
    "        #kde_min_AB = gaussian_kde(DAB_min_AB)\n",
    "        #H_min_AB_kde = kde_min_AB(bin_edges[0:-1])\n",
    "        #\n",
    "        #kde_min_BA = gaussian_kde(DAB_min_BA)\n",
    "        #H_min_BA_kde = kde_min_BA(bin_edges[0:-1])\n",
    "\n",
    "        # Save histograms\n",
    "        header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "        np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_B_name}_histogram_structures.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "                   header=header)\n",
    "        np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_B_name}_min_histogram_structures.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, H_min_AB)),\n",
    "                   header=header)\n",
    "        np.savetxt(f'{work_dir}/{soaps_B_name}-{soaps_A_name}_min_histogram_structures.dat',\n",
    "                   np.column_stack((bin_edges[0:-1], dx, H_min_BA)),\n",
    "                   header=header)\n",
    "\n",
    "        # Save KDEs\n",
    "        #np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_B_name}_kde_structures.dat', \n",
    "        #           np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "        #           header=header)\n",
    "        #np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_B_name}_min_kde_structures.dat', \n",
    "        #           np.column_stack((bin_edges[0:-1], dx, H_min_AB_kde)),\n",
    "        #           header=header)\n",
    "        #np.savetxt(f'{work_dir}/{soaps_B_name}-{soaps_A_name}_min_kde_structures.dat',\n",
    "        #           np.column_stack((bin_edges[0:-1], dx, H_min_BA_kde)),\n",
    "        #           header=header)\n",
    "    \n",
    "    # Distance matrix within same dataset\n",
    "    else:\n",
    "        \n",
    "        # TODO: do this without computing the full kernel (use gaussian_tri)\n",
    "        # Compute kernel diagonals\n",
    "        DAB = build_kernel(soaps_A, soaps_A, kernel=kernel_type, gamma=gamma)\n",
    "        KBB = np.diag(DAB)\n",
    "        KAA = KBB.reshape((-1, 1))\n",
    "\n",
    "        # Compute kernel between datasets\n",
    "        DAB = -2.0*DAB\n",
    "\n",
    "        # Compute squared kernel distance\n",
    "        DAB += KAA + KBB\n",
    "        \n",
    "        # Compute minimum distance for each environment in each dataset\n",
    "        # Flatten to remove diagonal\n",
    "        DAB_min = DAB.flatten()\n",
    "\n",
    "        # Remove diagonal entries\n",
    "        diag_idxs = np.arange(0, DAB.size, DAB.shape[0] + 1)\n",
    "        DAB_min = np.sqrt(np.delete(DAB_min, diag_idxs))\n",
    "\n",
    "        # Reshape into matrix\n",
    "        DAB_min = np.reshape(DAB_min, (DAB.shape[0], DAB.shape[0] - 1))\n",
    "        DAB_min = np.amin(DAB_min, axis=1)\n",
    "\n",
    "        # Triangular indices\n",
    "        triu_idxs = np.triu_indices(len(soaps_A), k=1)\n",
    "\n",
    "        # Compute kernel distance\n",
    "        DAB = np.sqrt(DAB[triu_idxs])\n",
    "        \n",
    "        # Compute distance histograms\n",
    "        H, _ = np.histogram(DAB, bins=bin_edges, density=True)\n",
    "        H_min, _ = np.histogram(DAB_min, bins=bin_edges, density=True)\n",
    "\n",
    "        # Compute KDEs\n",
    "        #kde = gaussian_kde(DAB)\n",
    "        #H_kde = kde(bin_edges[0:-1])\n",
    "        #\n",
    "        #kde_min = gaussian_kde(DAB_min)\n",
    "        #H_min_kde = kde_min(bin_edges[0:-1])\n",
    "\n",
    "        # Save histograms\n",
    "        header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "        np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_A_name}_histogram_structures.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "                   header=header)\n",
    "        np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_A_name}_min_histogram_structures.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, H_min)),\n",
    "                   header=header)\n",
    "\n",
    "        # Save KDEs\n",
    "        #np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_A_name}_kde_structures.dat', \n",
    "        #           np.column_stack((bin_edges[0:-1], dx, H_kde)),\n",
    "        #           header=header)\n",
    "        #np.savetxt(f'{work_dir}/{soaps_A_name}-{soaps_A_name}_min_kde_structures.dat', \n",
    "        #           np.column_stack((bin_edges[0:-1], dx, H_min_kde)),\n",
    "        #           header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM-DEEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data files\n",
    "    idxs_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Histograms/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "        \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Set header for output file\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "\n",
    "    # Load indices of unique environments\n",
    "    deem_unique = np.loadtxt(idxs_file, dtype=dt)\n",
    "    deem_unique = deem_unique[deem_unique['distances'] > unique_cutoff]\n",
    "    idxs = deem_unique['idxs']\n",
    "    \n",
    "    # Load unique SOAPs\n",
    "    deem_soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Compute histograms for structures\n",
    "    structure_histogram(model_file, deem_soaps, soaps_A_name='DEEM_10k',\n",
    "                         header=header, work_dir=work_dir)\n",
    "    \n",
    "    deem_soaps = np.vstack(deem_soaps)\n",
    "    deem_soaps = deem_soaps[idxs, :]\n",
    "    \n",
    "    # Initialize histogram bins\n",
    "    H = np.zeros(n_bins-1)\n",
    "    DAB_min = np.zeros(len(deem_soaps))\n",
    "    \n",
    "    # Distance histogram diagonals\n",
    "    KBB = gaussian_kernel_diag(deem_soaps, deem_soaps, gamma=gamma, k=0)\n",
    "    KAA = KBB\n",
    "    \n",
    "    # Iteratively build KAB and the distance histogram\n",
    "    for sdx, soap in enumerate(tqdm(deem_soaps)):\n",
    "        \n",
    "        # Compute one row of the kernel\n",
    "        soap = np.reshape(soap, (1, -1)) # Add extra dimension for cdist in gaussian_kernel\n",
    "        DAB = -2.0*gaussian_kernel(soap, deem_soaps, gamma=gamma)\n",
    "        DAB = np.reshape(DAB, (-1)) # Remove extra dimension\n",
    "        \n",
    "        # Compute squared distance for that row\n",
    "        DAB += KAA[sdx] + KBB\n",
    "        \n",
    "        # Compute minimum distance for the row, excluding self\n",
    "        # (np.delete does not modify in-place)\n",
    "        DAB_min[sdx] = np.sqrt(np.amin(np.delete(DAB, sdx)))\n",
    "        \n",
    "        # Compute distance, taking only distances above the main diagonal\n",
    "        # to avoid zero self-distances and double-counting\n",
    "        DAB = np.sqrt(DAB[sdx+1:])\n",
    "        \n",
    "        # TODO: compute KDE\n",
    "                \n",
    "        # Compute histogram\n",
    "        h, _ = np.histogram(DAB, bins=bin_edges, density=False)\n",
    "        \n",
    "        # Increment histogram\n",
    "        H += h\n",
    "     \n",
    "    # Normalize the histogram\n",
    "    H /= dx*np.sum(H)\n",
    "    \n",
    "    # Compute histogram of minimum distances\n",
    "    H_min, _ = np.histogram(DAB_min, bins=bin_edges, density=True)\n",
    "    \n",
    "    # Save\n",
    "    np.savetxt(f'{work_dir}/DEEM_10k-DEEM_10k_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/DEEM_10k-DEEM_10k_min_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min)),\n",
    "               header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_min = np.loadtxt('../Processed_Data/Histograms/6.0/DEEM_10k-DEEM_10k_min_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.loadtxt('../Processed_Data/Histograms/6.0/DEEM_10k-DEEM_10k_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA-IZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "iza_cantons = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', \n",
    "                         dtype={'names': ['structures', 'cantons'], 'formats': ['U3', 'i8']})\n",
    "\n",
    "# Load IZA Si atom counts\n",
    "n_Si_iza = np.loadtxt('../Processed_Data/IZA_226/n_Si.dat', dtype=int)\n",
    "\n",
    "struct_cantons = iza_cantons['cantons']\n",
    "env_cantons = np.repeat(struct_cantons, n_Si_iza)\n",
    "\n",
    "canton_labels = np.unique(struct_cantons)\n",
    "print(canton_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA structures to check that cantons are connected to the correct structure\n",
    "iza = read('../Raw_Data/GULP/IZA_226/IZA.xyz', index=':')\n",
    "ids = [i.info['Filename'][4:7] for i in iza]\n",
    "print(all(ids == iza_cantons['structures']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data files   \n",
    "    idxs_file_iza = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_iza = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    \n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Histograms/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "        \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Set header for output file\n",
    "    header = f'kernel:{kernel_type} gamma:{gamma}\\n' + column_labels\n",
    "\n",
    "    # Load indices of unique environments\n",
    "    iza_unique = np.loadtxt(idxs_file_iza, dtype=dt)\n",
    "    iza_unique = iza_unique[iza_unique['distances'] > unique_cutoff]\n",
    "    idxs_iza = iza_unique['idxs']\n",
    "    \n",
    "    # Load unique SOAPs\n",
    "    iza_soaps = load_structures_from_hdf5(soaps_file_iza, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Build histograms for all structures\n",
    "    structure_histogram(model_file, iza_soaps, soaps_A_name='IZA_226',\n",
    "                         header=header, work_dir=work_dir)\n",
    "    \n",
    "    iza_soaps_stacked = np.vstack(iza_soaps)\n",
    "    iza_soaps_stacked = iza_soaps_stacked[idxs_iza, :]\n",
    "    \n",
    "    # Associate each environment with a structure index\n",
    "    #split_idxs = np.cumsum(n_Si_iza)\n",
    "    #cantons = np.zeros(iza_soaps.shape[0], dtype=int)\n",
    "    #for idx, i in enumerate(idxs_iza):\n",
    "    #    struct_number = np.nonzero((split_idxs-i) > 0)[0][0]\n",
    "    #    cantons[idx] = iza_cantons[struct_number]['cantons']\n",
    "    \n",
    "    # Compute kernel diagonals\n",
    "    DAB = gaussian_kernel(iza_soaps_stacked, iza_soaps_stacked, gamma=gamma)\n",
    "    KBB = np.diag(DAB)\n",
    "    KAA = KBB.reshape((-1, 1))\n",
    "\n",
    "    # Compute kernel between datasets\n",
    "    DAB = -2.0*DAB\n",
    "\n",
    "    # Compute squared kernel distance\n",
    "    DAB += KAA + KBB\n",
    "    \n",
    "    # Loop over cantons\n",
    "    for cdx, canton_A in enumerate(canton_labels):\n",
    "        \n",
    "        # Get IZA structures in the specified canton\n",
    "        struct_canton_idxs_A = np.nonzero(struct_cantons == canton_A)[0]\n",
    "        \n",
    "        # Get IZA environments in the specified canton\n",
    "        env_canton_idxs_A = np.nonzero(env_cantons[idxs_iza] == canton_A)[0]\n",
    "        DAB_canton_A = DAB[env_canton_idxs_A, :]\n",
    "\n",
    "        # Loop over cantons\n",
    "        for canton_B in canton_labels[cdx:]:\n",
    "            \n",
    "            # Get IZA structures in the specified secondary canton\n",
    "            struct_canton_idxs_B = np.nonzero(struct_cantons == canton_B)[0]\n",
    "        \n",
    "            # Get IZA environments in the specified secondary canton\n",
    "            env_canton_idxs_B = np.nonzero(env_cantons[idxs_iza] == canton_B)[0]\n",
    "            DAB_canton_AB = DAB_canton_A[:, env_canton_idxs_B]\n",
    "\n",
    "            # Self-distances\n",
    "            if canton_A == canton_B:\n",
    "                \n",
    "                # Build histograms for structures,\n",
    "                # but skip canton 4, which is a single structure (RWY)\n",
    "                if canton_A != 4:\n",
    "                    structure_histogram(model_file, \n",
    "                                         [iza_soaps[i] for i in struct_canton_idxs_A], \n",
    "                                         soaps_A_name=f'IZA_226_{canton_A}',\n",
    "                                         header=header, work_dir=work_dir)\n",
    "\n",
    "                # Flatten to remove diagonal\n",
    "                DAB_canton_min_AB = DAB_canton_AB.flatten()\n",
    "                \n",
    "                # Remove diagonal entries\n",
    "                diag_idxs = np.arange(0, DAB_canton_AB.size, DAB_canton_AB.shape[0] + 1)\n",
    "                DAB_canton_min_AB = np.sqrt(np.delete(DAB_canton_min_AB, diag_idxs))\n",
    "                \n",
    "                # Reshape into matrix\n",
    "                DAB_canton_min_AB = np.reshape(DAB_canton_min_AB, \n",
    "                                               (DAB_canton_AB.shape[0], DAB_canton_AB.shape[0] - 1))\n",
    "                \n",
    "                # Compute minimum distance for each environment in each dataset\n",
    "                DAB_canton_min_AB = np.amin(DAB_canton_min_AB, axis=1)\n",
    "                \n",
    "                # Upper triangular indices\n",
    "                triu_idxs = np.triu_indices(DAB_canton_AB.shape[0], k=1)\n",
    "\n",
    "                # Compute kernel distance\n",
    "                DAB_canton_AB = np.sqrt(DAB_canton_AB[triu_idxs])\n",
    "                \n",
    "                # Compute distance histograms\n",
    "                h, _ = np.histogram(DAB_canton_AB, bins=bin_edges, density=True)\n",
    "                h_min, _ = np.histogram(DAB_canton_min_AB, bins=bin_edges, density=True)\n",
    "                \n",
    "                # Compute KDEs\n",
    "                #kde = gaussian_kde(DAB_canton_AB)\n",
    "                #h_kde = kde(bin_edges[0:-1])\n",
    "                #\n",
    "                #kde_min = gaussian_kde(DAB_canton_min_AB)\n",
    "                #h_min_kde = kde_min(bin_edges[0:-1])\n",
    "                \n",
    "                # Save histograms\n",
    "                header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "                np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_histogram.dat',\n",
    "                           np.column_stack((bin_edges[0:-1], dx, h)), header=header)\n",
    "                np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_min_histogram.dat', \n",
    "                           np.column_stack((bin_edges[0:-1], dx, h_min)), header=header)\n",
    "                \n",
    "                # Save KDEs\n",
    "                #np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_kde.dat',\n",
    "                #           np.column_stack((bin_edges[0:-1], dx, h_kde)), header=header)\n",
    "                #np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_min_kde.dat', \n",
    "                #           np.column_stack((bin_edges[0:-1], dx, h_min_kde)), header=header)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Build histograms for structures\n",
    "                structure_histogram(model_file, \n",
    "                             [iza_soaps[i] for i in struct_canton_idxs_A], \n",
    "                             soaps_A_name=f'IZA_226_{canton_A}',\n",
    "                             soaps_B=[iza_soaps[i] for i in struct_canton_idxs_B],\n",
    "                             soaps_B_name=f'IZA_226_{canton_B}',\n",
    "                             header=header, work_dir=work_dir)\n",
    "\n",
    "                # Compute kernel distance\n",
    "                DAB_canton_AB = np.sqrt(DAB_canton_AB)\n",
    "\n",
    "                # Compute minimum distance for each environment in each dataset\n",
    "                # min_AB = distance from an A to the nearest B for every A\n",
    "                # min_BA = distance from a B to the nearest A for every B\n",
    "                DAB_canton_min_AB = np.amin(DAB_canton_AB, axis=1)\n",
    "                DAB_canton_min_BA = np.amin(DAB_canton_AB, axis=0)\n",
    "                \n",
    "                # Compute distance histograms\n",
    "                h, _ = np.histogram(DAB_canton_AB, bins=bin_edges, density=True)\n",
    "                h_min_AB, _ = np.histogram(DAB_canton_min_AB, bins=bin_edges, density=True)\n",
    "                h_min_BA, _ = np.histogram(DAB_canton_min_BA, bins=bin_edges, density=True)\n",
    "                \n",
    "                # Compute KDEs\n",
    "                #kde = gaussian_kde(DAB_canton_AB)\n",
    "                #h_kde = kde(bin_edges[0:-1])\n",
    "                #\n",
    "                #kde_min_AB = gaussian_kde(DAB_canton_min_AB)\n",
    "                #h_min_AB_kde = kde_min_AB(bin_edges[0:-1])\n",
    "                #\n",
    "                #kde_min_BA = gaussian_kde(DAB_canton_min_BA)\n",
    "                #h_min_BA_kde = kde_min_BA(bin_edges[0:-1])\n",
    "                \n",
    "                # Save histograms\n",
    "                header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "                np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_histogram.dat',\n",
    "                           np.column_stack((bin_edges[0:-1], dx, h)), header=header)\n",
    "                np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_min_histogram.dat', \n",
    "                           np.column_stack((bin_edges[0:-1], dx, h_min_AB)), header=header)\n",
    "                np.savetxt(f'{work_dir}/IZA_226_{canton_B}-IZA_226_{canton_A}_min_histogram.dat', \n",
    "                           np.column_stack((bin_edges[0:-1], dx, h_min_BA)), header=header)\n",
    "                \n",
    "                # Save KDEs\n",
    "                #np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_kde.dat',\n",
    "                #           np.column_stack((bin_edges[0:-1], dx, h_kde)), header=header)\n",
    "                #np.savetxt(f'{work_dir}/IZA_226_{canton_A}-IZA_226_{canton_B}_min_kde.dat', \n",
    "                #           np.column_stack((bin_edges[0:-1], dx, h_min_AB_kde)), header=header)\n",
    "                #np.savetxt(f'{work_dir}/IZA_226_{canton_B}-IZA_226_{canton_A}_min_kde.dat', \n",
    "                #           np.column_stack((bin_edges[0:-1], dx, h_min_BA_kde)), header=header)\n",
    "        \n",
    "    # Compute histograms on whole IZA database\n",
    "    # Flatten to remove diagonal            \n",
    "    DAB_min = DAB.flatten()\n",
    "        \n",
    "    # Remove diagonal entries\n",
    "    diag_idxs = np.arange(0, DAB.size, DAB.shape[0] + 1)\n",
    "    DAB_min = np.sqrt(np.delete(DAB_min, diag_idxs))\n",
    "    \n",
    "    # Reshape into matrix\n",
    "    DAB_min = np.reshape(DAB_min, (DAB.shape[0], DAB.shape[0] - 1))\n",
    "    \n",
    "    # Compute minimum distance for each environment\n",
    "    DAB_min = np.amin(DAB_min, axis=1)\n",
    "    \n",
    "    # Triangular indices\n",
    "    triu_idxs = np.triu_indices(iza_soaps_stacked.shape[0], k=1)\n",
    "    \n",
    "    # Compute kernel distance\n",
    "    DAB = np.sqrt(DAB[triu_idxs])\n",
    "\n",
    "    # Compute distance histograms\n",
    "    H, _ = np.histogram(DAB, bins=bin_edges, density=True)\n",
    "    H_min, _ = np.histogram(DAB_min, bins=bin_edges, density=True)\n",
    "    \n",
    "    # Compute KDEs\n",
    "    #kde = gaussian_kde(DAB)\n",
    "    #H_kde = kde(bin_edges[0:-1])\n",
    "    #\n",
    "    #kde_min = gaussian_kde(DAB_min)\n",
    "    #H_min_kde = kde_min(bin_edges[0:-1])\n",
    "\n",
    "    # Save histograms\n",
    "    np.savetxt(f'{work_dir}/IZA_226-IZA_226_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/IZA_226-IZA_226_min_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min)),\n",
    "               header=header)\n",
    "    \n",
    "    # Save KDEs\n",
    "    #np.savetxt(f'{work_dir}/IZA_226-IZA_226_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_kde)),\n",
    "    #           header=header)\n",
    "    #np.savetxt(f'{work_dir}/IZA_226-IZA_226_min_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_min_kde)),\n",
    "    #           header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We will probably get an error in the above cell with the 4th canton (RWY), as the minimum distances might all be smaller than the cutoff, and therefore the histogram will return all NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_min = np.loadtxt('../Processed_Data/Histograms/6.0/IZA_226-IZA_226_min_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.loadtxt('../Processed_Data/Histograms/6.0/IZA_226-IZA_226_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD-COD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data files        \n",
    "    idxs_file_cod = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_cod = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    \n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Histograms/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "        \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Set header for output file\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "     \n",
    "    # Load indices of unique environments\n",
    "    cod_unique = np.loadtxt(idxs_file_cod, dtype=dt)\n",
    "    cod_unique = cod_unique[cod_unique['distances'] > unique_cutoff]\n",
    "    idxs_cod = cod_unique['idxs']\n",
    "    \n",
    "    # Load unique SOAPs\n",
    "    cod_soaps = load_structures_from_hdf5(soaps_file_cod, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Build histograms for structures\n",
    "    structure_histogram(model_file, cod_soaps, soaps_A_name='COD_196',\n",
    "                         header=header, work_dir=work_dir)\n",
    "    \n",
    "    cod_soaps = np.vstack(cod_soaps)\n",
    "    cod_soaps = cod_soaps[idxs_cod, :]\n",
    "    \n",
    "    # Compute kernel diagonals\n",
    "    DAB = gaussian_kernel(cod_soaps, cod_soaps, gamma=gamma)\n",
    "    KBB = np.diag(DAB)\n",
    "    KAA = KBB.reshape((-1, 1))\n",
    "\n",
    "    # Compute kernel between datasets\n",
    "    DAB = -2.0*DAB\n",
    "\n",
    "    # Compute squared kernel distance\n",
    "    DAB += KAA + KBB\n",
    "    \n",
    "    # Compute minimum distance for each environment in each dataset\n",
    "    # Flatten to remove diagonal\n",
    "    DAB_min = DAB.flatten()\n",
    "    \n",
    "    # Remove diagonal entries\n",
    "    diag_idxs = np.arange(0, DAB.size, DAB.shape[0] + 1)\n",
    "    DAB_min = np.sqrt(np.delete(DAB_min, diag_idxs))\n",
    "    \n",
    "    # Reshape into matrix\n",
    "    DAB_min = np.reshape(DAB_min, (DAB.shape[0], DAB.shape[0] - 1))\n",
    "    DAB_min = np.amin(DAB_min, axis=1)\n",
    "    \n",
    "    # Triangular indices\n",
    "    triu_idxs = np.triu_indices(cod_soaps.shape[0], k=1)\n",
    "    \n",
    "    # Compute kernel distance\n",
    "    DAB = np.sqrt(DAB[triu_idxs])\n",
    "\n",
    "    # Compute distance histograms\n",
    "    H, _ = np.histogram(DAB, bins=bin_edges, density=True)\n",
    "    H_min, _ = np.histogram(DAB_min, bins=bin_edges, density=True)\n",
    "    \n",
    "    # Compute KDEs\n",
    "    #kde = gaussian_kde(DAB)\n",
    "    #H_kde = kde(bin_edges[0:-1])\n",
    "    #\n",
    "    #kde_min = gaussian_kde(DAB_min)\n",
    "    #H_min_kde = kde_min(bin_edges[0:-1])\n",
    "\n",
    "    # Save histograms\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "    np.savetxt(f'{work_dir}/COD_196-COD_196_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/COD_196-COD_196_min_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min)),\n",
    "               header=header)\n",
    "    \n",
    "    # Save KDEs\n",
    "    #np.savetxt(f'{work_dir}/COD_196-COD_196_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_kde)),\n",
    "    #           header=header)\n",
    "    #np.savetxt(f'{work_dir}/COD_196-COD_196_min_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_min_kde)),\n",
    "    #           header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H_min = np.loadtxt('../Processed_Data/Histograms/6.0/COD_196-COD_196_min_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H_min, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.loadtxt('../Processed_Data/Histograms/6.0/COD_196-COD_196_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA-DEEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data files\n",
    "    idxs_file_deem = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_deem = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        \n",
    "    idxs_file_iza = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_iza = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    \n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Histograms/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "        \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Set header for output file\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "\n",
    "    # Load indices of unique environments\n",
    "    deem_unique = np.loadtxt(idxs_file_deem, dtype=dt)\n",
    "    deem_unique = deem_unique[deem_unique['distances'] > unique_cutoff]\n",
    "    idxs_deem = deem_unique['idxs']\n",
    "    \n",
    "    iza_unique = np.loadtxt(idxs_file_iza, dtype=dt)\n",
    "    iza_unique = iza_unique[iza_unique['distances'] > unique_cutoff]\n",
    "    idxs_iza = iza_unique['idxs']\n",
    "    \n",
    "    # Load unique SOAPs\n",
    "    deem_soaps = load_structures_from_hdf5(soaps_file_deem, datasets=None, concatenate=False)\n",
    "    iza_soaps = load_structures_from_hdf5(soaps_file_iza, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Build histograms for structures\n",
    "    structure_histogram(model_file, iza_soaps, soaps_A_name='IZA_226',\n",
    "                         soaps_B=deem_soaps, soaps_B_name='DEEM_10k',\n",
    "                         header=header, work_dir=work_dir)\n",
    "    \n",
    "    deem_soaps = np.vstack(deem_soaps)\n",
    "    deem_soaps = deem_soaps[idxs_deem, :]\n",
    "    \n",
    "    iza_soaps_stacked = np.vstack(iza_soaps)\n",
    "    iza_soaps_stacked = iza_soaps_stacked[idxs_iza, :]\n",
    "    \n",
    "    # Associate each environment with a structure index\n",
    "    #split_idxs = np.cumsum(n_Si_iza)\n",
    "    #cantons = np.zeros(iza_soaps.shape[0], dtype=int)\n",
    "    #for idx, i in enumerate(idxs_iza):\n",
    "    #    struct_number = np.nonzero((split_idxs-i) > 0)[0][0]\n",
    "    #    cantons[idx] = iza_cantons[struct_number]['cantons']\n",
    "        \n",
    "    # Initialize histogram bins\n",
    "    H = np.zeros(n_bins-1)\n",
    "    H_min_AB = np.zeros(n_bins-1)\n",
    "    H_min_BA = np.zeros(n_bins-1)\n",
    "    \n",
    "    # Loop over cantons\n",
    "    for canton in tqdm(canton_labels):\n",
    "        \n",
    "        # Get IZA structures in the specified canton\n",
    "        canton_idxs = np.nonzero(struct_cantons == canton)[0]\n",
    "        \n",
    "        # Compute histograms for structures\n",
    "        structure_histogram(model_file, \n",
    "                             [iza_soaps[i] for i in canton_idxs], \n",
    "                             soaps_A_name=f'IZA_226_{canton}',\n",
    "                             soaps_B=deem_soaps, soaps_B_name='DEEM_10k',\n",
    "                             header=header, work_dir=work_dir)\n",
    "        \n",
    "        # Get IZA environments in the specified canton\n",
    "        canton_idxs = np.nonzero(env_cantons[idxs_iza] == canton)[0]\n",
    "        \n",
    "        # Compute kernel diagonals for each dataset\n",
    "        # A = IZA\n",
    "        # B = DEEM\n",
    "        KAA = gaussian_kernel_diag(iza_soaps_stacked[canton_idxs], \n",
    "                                   iza_soaps_stacked[canton_idxs], \n",
    "                                   gamma=gamma, k=0)\n",
    "        KAA = np.reshape(KAA, (-1, 1))\n",
    "        KBB = gaussian_kernel_diag(deem_soaps, deem_soaps, gamma=gamma, k=0)\n",
    "\n",
    "        # Compute kernel between datasets\n",
    "        DAB = -2.0*gaussian_kernel(iza_soaps_stacked[canton_idxs], \n",
    "                                   deem_soaps, \n",
    "                                   gamma=gamma)\n",
    "\n",
    "        # Compute kernel distance\n",
    "        DAB += KAA + KBB \n",
    "        DAB = np.sqrt(DAB)\n",
    "\n",
    "        # Compute minimum distance for each environment in each dataset\n",
    "        # min_AB = distance from an A to the nearest B for every A\n",
    "        # min_BA = distance from a B to the nearest A for every B\n",
    "        DAB_min_AB = np.amin(DAB, axis=1)\n",
    "        DAB_min_BA = np.amin(DAB, axis=0)\n",
    "\n",
    "        # Compute cantonal distance histograms\n",
    "        h, _ = np.histogram(DAB, bins=bin_edges, density=False)\n",
    "        h_min_AB, _ = np.histogram(DAB_min_AB, bins=bin_edges, density=False)\n",
    "        h_min_BA, _ = np.histogram(DAB_min_BA, bins=bin_edges, density=False)\n",
    "                        \n",
    "        # Increment total histograms\n",
    "        H += h\n",
    "        H_min_AB += h_min_AB\n",
    "        H_min_BA += h_min_BA\n",
    "        \n",
    "        # Normalize cantonal histograms \n",
    "        # (do without \"/=\" b/c need to promote to float)\n",
    "        h = h / (dx*np.sum(h))\n",
    "        h_min_AB = h_min_AB / (dx*np.sum(h_min_AB))\n",
    "        h_min_BA = h_min_BA / (dx*np.sum(h_min_BA))\n",
    "        \n",
    "        # Save cantonal histograms\n",
    "        header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "        np.savetxt(f'{work_dir}/IZA_226_{canton}-DEEM_10k_histogram.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, h)),\n",
    "                   header=header)\n",
    "        np.savetxt(f'{work_dir}/IZA_226_{canton}-DEEM_10k_min_histogram.dat', \n",
    "                   np.column_stack((bin_edges[0:-1], dx, h_min_AB)),\n",
    "                   header=header)\n",
    "        np.savetxt(f'{work_dir}/DEEM_10k-IZA_226_{canton}_min_histogram.dat',\n",
    "                   np.column_stack((bin_edges[0:-1], dx, h_min_BA)),\n",
    "                   header=header)\n",
    "        \n",
    "    # Normalize the total histograms\n",
    "    H /= dx*np.sum(H)\n",
    "    H_min_AB /= dx*np.sum(H_min_AB)\n",
    "    H_min_BA /= dx*np.sum(H_min_BA)\n",
    "    \n",
    "    # Save histograms\n",
    "    np.savetxt(f'{work_dir}/IZA_226-DEEM_10k_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/IZA_226-DEEM_10k_min_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min_AB)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/DEEM_10k-IZA_226_min_histogram.dat',\n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min_BA)),\n",
    "               header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H_min_AB, width=dx, align='edge', alpha=0.5)\n",
    "plt.bar(bin_edges[0:-1], H_min_BA, width=dx, align='edge', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_min_AB = np.loadtxt('../Processed_Data/Histograms/6.0/IZA_226-DEEM_10k_min_histogram_structures.dat')[:, -1]\n",
    "H_min_BA = np.loadtxt('../Processed_Data/Histograms/6.0/DEEM_10k-IZA_226_min_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H_min_AB, width=dx, align='edge', alpha=0.5)\n",
    "plt.bar(bin_edges[0:-1], H_min_BA, width=dx, align='edge', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.loadtxt('../Processed_Data/Histograms/6.0/IZA_226-DEEM_10k_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD-DEEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data files\n",
    "    idxs_file_deem = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_deem = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "        \n",
    "    idxs_file_cod = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}/FPS_unique.idxs'\n",
    "    soaps_file_cod = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    \n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Histograms/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "        \n",
    "    # Load kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Set header for output file\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "     \n",
    "    # Load indices of unique environments\n",
    "    deem_unique = np.loadtxt(idxs_file_deem, dtype=dt)\n",
    "    deem_unique = deem_unique[deem_unique['distances'] > unique_cutoff]\n",
    "    idxs_deem = deem_unique['idxs']\n",
    "    \n",
    "    cod_unique = np.loadtxt(idxs_file_cod, dtype=dt)\n",
    "    cod_unique = cod_unique[cod_unique['distances'] > unique_cutoff]\n",
    "    idxs_cod = cod_unique['idxs']\n",
    "    \n",
    "    # Load unique SOAPs\n",
    "    deem_soaps = load_structures_from_hdf5(soaps_file_deem, datasets=None, concatenate=False)\n",
    "    cod_soaps = load_structures_from_hdf5(soaps_file_cod, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Build histograms for structures\n",
    "    structure_histogram(model_file, cod_soaps, soaps_A_name='COD_196',\n",
    "                         soaps_B=deem_soaps, soaps_B_name='DEEM_10k',\n",
    "                         header=header, work_dir=work_dir)\n",
    "    \n",
    "    deem_soaps = np.vstack(deem_soaps)\n",
    "    deem_soaps = deem_soaps[idxs_deem, :]\n",
    "    \n",
    "    cod_soaps = np.vstack(cod_soaps)\n",
    "    cod_soaps = cod_soaps[idxs_cod, :]\n",
    "    \n",
    "    # Compute kernel diagonals for each dataset\n",
    "    # A = COD\n",
    "    # B = DEEM\n",
    "    KAA = gaussian_kernel_diag(cod_soaps, cod_soaps, gamma=gamma, k=0)\n",
    "    KAA = np.reshape(KAA, (-1, 1))\n",
    "    KBB = gaussian_kernel_diag(deem_soaps, deem_soaps, gamma=gamma, k=0)\n",
    "\n",
    "    # Compute kernel between datasets\n",
    "    DAB = -2.0*gaussian_kernel(cod_soaps, deem_soaps, gamma=gamma)\n",
    "\n",
    "    # Compute kernel distance\n",
    "    DAB += KAA + KBB \n",
    "    DAB = np.sqrt(DAB)\n",
    "\n",
    "    # Compute minimum distance for each environment in each dataset\n",
    "    # min_AB = distance from an A to the nearest B for every A\n",
    "    # min_BA = distance from a B to the nearest A for every B\n",
    "    DAB_min_AB = np.amin(DAB, axis=1)\n",
    "    DAB_min_BA = np.amin(DAB, axis=0)\n",
    "\n",
    "    # Compute distance histograms\n",
    "    H, _ = np.histogram(DAB, bins=bin_edges, density=True)\n",
    "    H_min_AB, _ = np.histogram(DAB_min_AB, bins=bin_edges, density=True)\n",
    "    H_min_BA, _ = np.histogram(DAB_min_BA, bins=bin_edges, density=True)\n",
    "    \n",
    "    # Compute KDEs\n",
    "    #kde = gaussian_kde(DAB)\n",
    "    #H_kde = kde(bin_edges[0:-1])\n",
    "    #\n",
    "    #kde_min_AB = gaussian_kde(DAB_min_AB)\n",
    "    #H_min_AB_kde = kde_min_AB(bin_edges[0:-1])\n",
    "    #\n",
    "    #kde_min_BA = gaussian_kde(DAB_min_BA)\n",
    "    #H_min_BA_kde = kde_min_BA(bin_edges[0:-1])\n",
    "\n",
    "    # Save histograms\n",
    "    header = f'kernel: {kernel_type} gamma: {gamma}\\n' + column_labels\n",
    "    np.savetxt(f'{work_dir}/COD_196-DEEM_10k_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/COD_196-DEEM_10k_min_histogram.dat', \n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min_AB)),\n",
    "               header=header)\n",
    "    np.savetxt(f'{work_dir}/DEEM_10k-COD_196_min_histogram.dat',\n",
    "               np.column_stack((bin_edges[0:-1], dx, H_min_BA)),\n",
    "               header=header)\n",
    "    \n",
    "    # Save KDEs\n",
    "    #np.savetxt(f'{work_dir}/COD_196-DEEM_10k_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H)),\n",
    "    #           header=header)\n",
    "    #np.savetxt(f'{work_dir}/COD_196-DEEM_10k_min_kde.dat', \n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_min_AB_kde)),\n",
    "    #           header=header)\n",
    "    #np.savetxt(f'{work_dir}/DEEM_10k-COD_196_min_kde.dat',\n",
    "    #           np.column_stack((bin_edges[0:-1], dx, H_min_BA_kde)),\n",
    "    #           header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H_min_AB, width=dx, align='edge', alpha=0.5)\n",
    "plt.bar(bin_edges[0:-1], H_min_BA, width=dx, align='edge', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_min_AB = np.loadtxt('../Processed_Data/Histograms/6.0/COD_196-DEEM_10k_min_histogram_structures.dat')[:, -1]\n",
    "H_min_BA = np.loadtxt('../Processed_Data/Histograms/6.0/DEEM_10k-COD_196_min_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H_min_AB, width=dx, align='edge', alpha=0.5)\n",
    "plt.bar(bin_edges[0:-1], H_min_BA, width=dx, align='edge', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.loadtxt('../Processed_Data/Histograms/6.0/COD_196-DEEM_10k_histogram_structures.dat')[:, -1]\n",
    "plt.bar(bin_edges[0:-1], H, width=dx, align='edge')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
