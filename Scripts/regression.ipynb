{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "from regression import SparseKRR\n",
    "from kernels import build_kernel\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "train_idxs = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "test_idxs = np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_structures = train_idxs.size + test_idxs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set structure labels for loading from the HDF5 file\n",
    "n_digits = len(str(n_structures - 1))\n",
    "datasets = [str(i).zfill(n_digits) for i in train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processeed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "representative_soaps = {}\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = '../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    n_Si = np.loadtxt('{work_dir}/n_Si.dat', dtype=int)\n",
    "    split_idxs = np.cumsum(n_Si)[0:-1]\n",
    "    representative_idxs = np.loadtxt('{work_dir}/FPS_representatives.idxs', dtype=int)\n",
    "    soaps_file = '{work_dir}/soaps.hdf5'\n",
    "    representative_soaps['{cutoff}'] = build_representatives_from_hdf5(soaps_file, representative_idxs, split_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_skrr_oos(cutoff, datasets, property_name,\n",
    "                   soaps_file, ref_soaps_file,\n",
    "                   rep_idxs_file, model_file,\n",
    "                   skrr_file, work_dir='.'):\n",
    "    \n",
    "    # Read SOAPs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Load reference SOAPs\n",
    "    ref_soaps = load_structures_from_hdf5(ref_soaps_file, datasets=datasets, concatenate=True)\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = ref_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Unpickle the kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Unpickle the reference models\n",
    "    with open(model_file, 'r') as f:\n",
    "        skrr_dict = json.load(f)\n",
    "        \n",
    "    # Turn lists into arrays\n",
    "    for k, v in skrr_dict.items():\n",
    "        if isinstance(v, list):\n",
    "            skrr_dict[k] = np.array(v)\n",
    "            \n",
    "    # Build kernels\n",
    "    KNM = build_kernel(soaps, representative_soaps,\n",
    "                       kernel=kernel_type, gamma=gamma)\n",
    "    KNM_environments = build_kernel(np.vstack(soaps, axis=0), representative_soaps,\n",
    "                                    kernel=kernel_type, gamma=gama)\n",
    "        \n",
    "    # Initialize SKRR model\n",
    "    skrr = SparseKRR()\n",
    "    skrr.__dict__ = skrr_dict\n",
    "\n",
    "    # Predict based on the loaded SparseKRR object\n",
    "    Yp_structures = skrr.transform(KNM)\n",
    "    Yp_environments = skrr.transform(KNM_environments)\n",
    "\n",
    "    np.savetxt(f'{work_dir}/predicted_structure_{property_name}.dat', Yp_structures)\n",
    "    np.savetxt(f'{work_dir}/predicted_environment_{property_name}.dat', Yp_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set property names for loading\n",
    "property_names = ['volumes', 'energies']\n",
    "\n",
    "# Load structure properties\n",
    "structure_properties = {}\n",
    "for pn in property_names:\n",
    "    structure_properties[pn] = np.loadtxt(f'../Processed_Data/DEEM_10k/Data/structure_{pn}.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_10k/Models/{cutoff}'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "    \n",
    "    # Set SOAP files\n",
    "    soaps_file = f'{work_dir}/soaps.hdf5'\n",
    "    rep_idxs_file = f'{work_dir}/FPS_representatives.idxs'\n",
    "            \n",
    "    # Read SOAPs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "    \n",
    "    # Build representative SOAPs\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = np.vstack([soaps[i] for i in train_idxs])\n",
    "    representative_soaps = representative_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Loop over structure properties\n",
    "    for pn, Y in structure_properties.items():\n",
    "        \n",
    "        # Load model parameters\n",
    "        model_file = f'{data_dir}/{pn}_mae_parameters.json'\n",
    "\n",
    "        # Load kernel parameters\n",
    "        with open(model_file, 'r') as f:\n",
    "            model_dict = json.load(f)\n",
    "\n",
    "        kernel_type = model_dict['kernel_type']\n",
    "        kernel_parameters = model_dict['kernel_parameters']\n",
    "\n",
    "        # Build kernels\n",
    "        KMM = build_kernel(representative_soaps, representative_soaps,\n",
    "                           kernel=kernel_type, gamma=gamma)\n",
    "        KNM = build_kernel(soaps, representative_soaps,\n",
    "                           kernel=kernel_type, gamma=gamma)\n",
    "        KNM_environments = build_kernel(np.vstack(soaps), representative_soaps,\n",
    "                                        kernel=kernel_type, gamma=gamma)\n",
    "    \n",
    "        # Prepare properties and scaling\n",
    "        Yc = Y - np.mean(Y[train_idxs])\n",
    "        delta = np.var(Yc[train_idxs]) * KMM.shape[0] / np.trace(KMM)\n",
    "        \n",
    "        # Load regression parameters\n",
    "        sigma = model_dict['sigma']\n",
    "        reg = model_dict['reg']\n",
    "    \n",
    "        # Initialize sparse KRR\n",
    "        skrr = SparseKRR(sigma=sigma, reg=reg, rcond=None)\n",
    "        skrr.fit(delta*KNM[train_idxs, :], delta*KMM, delta*Yc[train_idxs])\n",
    "        \n",
    "        # Pickle the models\n",
    "        # Copy the dict so we can make the numpy arrays lists\n",
    "        skrr_dict = skrr.__dict__.copy()\n",
    "        \n",
    "        # Convert arrays to lists\n",
    "        for k, v in skrr_dict.items():\n",
    "            if isinstance(v, np.ndarray):\n",
    "                skrr_dict[k] = v.tolist()\n",
    "        \n",
    "        # Save\n",
    "        with open(f'{data_dir}/skrr_{pn}.json', 'w') as f:\n",
    "            json.dump(skrr_dict, f)\n",
    "        \n",
    "        # Predict properties\n",
    "        Yp_structures = np.zeros(len(soaps))\n",
    "        Yp_structures[train_idxs] = skrr.transform(KNM[train_idxs, :])\n",
    "        Yp_structures[test_idxs] = skrr.transform(KNM[test_idxs, :])\n",
    "        Yp_environments = skrr.transform(KNM_environments)\n",
    "        \n",
    "        # Save predicted properties\n",
    "        np.savetxt(f'{work_dir}/predicted_structure_{pn}.dat', Yp_structures)\n",
    "        np.savetxt(f'{work_dir}/predicted_environment_{pn}.dat', Yp_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA_226 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set the working directory\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "      \n",
    "    # Set the SOAP files\n",
    "    soaps_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    ref_soaps_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    rep_idxs_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_representatives.idxs'\n",
    "\n",
    "    # Compute SKRR predictions for each property\n",
    "    for pn in structure_properties.keys():\n",
    "        model_file = f'../Processed_Data/DEEM_10/Models/{cutoff}/{pn}_mae_parameters.json'\n",
    "        skrr_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/skrr_{pn}.json'\n",
    "        transform_skrr_oos(cutoff, datasets, pn,\n",
    "                           soaps_file, ref_soaps_file,\n",
    "                           rep_idxs_file, model_file,\n",
    "                           skrr_file, work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD_196 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
