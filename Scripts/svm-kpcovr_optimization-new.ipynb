{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from regression import PCovR, KPCovR, SparseKPCovR\n",
    "from regression import LR, KRR\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "from kernels import center_kernel, center_kernel_fast\n",
    "from kernels import center_kernel_oos, center_kernel_oos_fast\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import project_utils as utils\n",
    "from tools import load_json, save_json\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/kernel-tutorials')\n",
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/KernelPCovR/analysis/scripts')\n",
    "# from utilities.sklearn_covr.kpcovr import KernelPCovR as KPCovR2\n",
    "# from utilities.sklearn_covr.pcovr import PCovR as PCovR2\n",
    "from helpers import l_regr, l_kpcovr, l_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regression_losses(lrs_train, lrs_test, lrts_train, ltrs_test, \n",
    "                              y_train, y_test, yp_train, yp_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lr_test = np.sum(l_regr(y_test, yp_test))\n",
    "    lr_train = np.sum(l_regr(y_train, yp_train))\n",
    "\n",
    "    return lr_train, lr_test\n",
    "\n",
    "def compute_kernel_projection_losses(k_train, k_test, k_test_test, t_train, t_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lp_test = np.sum(l_kpcovr(k_train=k_train,\n",
    "                              k_test=k_test,\n",
    "                              k_test_test=k_test_test,\n",
    "                              t_train=t_train, t_test=t_test))\n",
    "\n",
    "    lp_train = np.sum(l_kpcovr(k_train=k_train,\n",
    "                               t_train=t_train, t_test=t_test))\n",
    "\n",
    "    return lp_train, lp_test\n",
    "\n",
    "def compute_linear_projection_losses(x_train, x_test, xr_train, xr_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lp_train = np.sum(l_proj(x_train, xr=xr_train))\n",
    "    lp_test = np.sum(l_proj(x_test, xr=xr_test))\n",
    "\n",
    "    return lp_train, lp_test\n",
    "\n",
    "def load_covr_losses(filename):\n",
    "    \n",
    "    # Load losses\n",
    "    loss_matrix_shape = (len(alphas), len(regularizations))\n",
    "    covr_errors = np.loadtxt(filename)\n",
    "    alpha_matrix = np.reshape(covr_errors['alpha'], loss_matrix_shape)\n",
    "    reg_matrix = np.reshape(covr_errors['regularization'], loss_matrix_shape)\n",
    "    lr_train_matrix = np.reshape(np.mean(covr_errors['lr_train'], axis=1), loss_matrix_shape)\n",
    "    lr_test_matrix = np.reshape(np.mean(covr_errors['lr_test'], axis=1), loss_matrix_shape)\n",
    "    lp_train_matrix = np.reshape(np.mean(covr_errors['lp_train'], axis=1), loss_matrix_shape)\n",
    "    lp_test_matrix = np.reshape(np.mean(covr_errors['lp_test'], axis=1), loss_matrix_shape)\n",
    "    alphas = alpha_matrix[:, 0]\n",
    "    opt_reg_idx = np.unravel_index(np.argmin(lr_test_matrix + lp_test_matrix), \n",
    "                                   lr_test_matrix.shape)[1]\n",
    "    \n",
    "    return alphas, opt_reg_idx, \\\n",
    "        lr_train_matrix, lr_test_matrix, lp_train_matrix, lp_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set indices for Deem\n",
    "idxs_deem = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_deem = idxs_deem.size + np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_deem_train_file = '../Processed_Data/Models/deem_train_ksvc-kpcovr.idxs'\n",
    "idxs_deem_test_file = '../Processed_Data/Models/deem_test_ksvc-kpcovr.idxs'\n",
    "\n",
    "# Load indices for kernel building\n",
    "try:\n",
    "    idxs_deem_train = np.loadtxt(deem_train_file, dtype=int)\n",
    "    idxs_deem_test = np.loadtxt(deem_test_file, dtype=int)\n",
    "    n_deem_train = len(idxs_deem_train)\n",
    "    n_deem_test = len(idxs_deem_test)\n",
    "    \n",
    "    # Check to make sure the test and train set sizes are correct\n",
    "    print(n_deem_train, n_deem_test)\n",
    "\n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "\n",
    "    n_deem_train = 5000\n",
    "    n_deem_test = 2750\n",
    "    \n",
    "    # Deem is already shuffled, don't need to do so here\n",
    "    idxs_deem_train = idxs_deem[0:n_deem_train]\n",
    "    idxs_deem_test = idxs_deem[n_deem_train:n_deem_train+n_deem_test]\n",
    "    \n",
    "    np.savetxt(idxs_deem_train_file, idxs_deem_train, fmt='%d')\n",
    "    np.savetxt(idxs_deem_test_file, idxs_deem_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "cantons_iza = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)\n",
    "RWY = np.nonzero(cantons_iza == 4)[0][0]\n",
    "\n",
    "cantons_iza = np.delete(cantons_iza, RWY)\n",
    "n_iza = len(cantons_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_iza_train_file = '../Processed_Data/Models/iza_train_ksvc-kpcovr.idxs'\n",
    "idxs_iza_test_file = '../Processed_Data/Models/iza_test_ksvc-kpcovr.idxs'\n",
    "\n",
    "# Load indices for kernel building\n",
    "try:\n",
    "    idxs_iza_train = np.loadtxt(idxs_iza_train_file, dtype=int)\n",
    "    idxs_iza_test = np.loadtxt(idxs_iza_test_file, dtype=int)\n",
    "    \n",
    "    # Check to make sure the test and train set sizes are correct\n",
    "    print(len(idxs_iza_train), len(idxs_iza_test))\n",
    "\n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "\n",
    "    idxs_iza_train = np.arange(0, n_iza)\n",
    "    idxs_iza_test = np.arange(0, n_iza)\n",
    "    \n",
    "    np.savetxt(idxs_iza_train_file, idxs_iza_train, fmt='%d')\n",
    "    np.savetxt(idxs_iza_test_file, idxs_iza_test, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons_train = {}\n",
    "cantons_test = {}\n",
    "\n",
    "cantons_train[4] = np.concatenate((cantons_iza[idxs_iza_train], cantons_deem[idxs_deem_train]))\n",
    "cantons_test[4] = np.concatenate((cantons_iza[idxs_iza_test], cantons_deem[idxs_deem_test]))\n",
    "\n",
    "cantons_train[2] = np.concatenate((np.ones(len(idxs_iza_train), dtype=int),\n",
    "                                   np.ones(len(idxs_deem_train), dtype=int) * 2))\n",
    "cantons_test[2] = np.concatenate((np.ones(len(idxs_iza_test), dtype=int),\n",
    "                                  np.ones(len(idxs_deem_test), dtype=int) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsets = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_iza_train_kernel_file = '../Processed_Data/Models/iza_train_ksvc-kpcovr_optimization.idxs'\n",
    "idxs_iza_test_kernel_file = '../Processed_Data/Models/iza_test_ksvc-kpcovr_optimization.idxs'\n",
    "\n",
    "# Load IZA array splits for optimization\n",
    "try:\n",
    "    idxs_iza_train_kernel = [np.asarray(i) for i in load_json(idxs_iza_train_kernel_file)]\n",
    "    idxs_iza_test_kernel = [np.asarray(i) for i in load_json(idxs_iza_test_kernel_file)]\n",
    "    \n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "    \n",
    "    idxs_iza = np.arange(0, n_iza)\n",
    "    n_iza_train_kernel = int(n_iza / 2)\n",
    "    n_iza_test_kernel = n_iza - n_iza_train_kernel\n",
    "\n",
    "    idxs_iza_train_kernel = []\n",
    "    idxs_iza_test_kernel = []\n",
    "    for n in range(0, n_subsets):\n",
    "        np.random.shuffle(idxs_iza)\n",
    "        idxs_iza_train_kernel.append(idxs_iza[0:n_iza_train_kernel])\n",
    "        idxs_iza_test_kernel.append(idxs_iza[n_iza_train_kernel:])\n",
    "        \n",
    "    save_json(idxs_iza_train_kernel, [array.tolist() for array in idxs_iza_train_kernel_file])\n",
    "    save_json(idxs_iza_test_kernel, [array.tolist() for array in idxs_iza_test_kernel_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_deem_train_kernel_file = '../Processed_Data/Models/deem_train_ksvc-kpcovr_optimization.idxs'\n",
    "idxs_deem_test_kernel_file = '../Processed_Data/Models/deem_test_ksvc-kpcovr_optimization.idxs'\n",
    "\n",
    "# Load DEEM array splits for optimization\n",
    "try:\n",
    "    idxs_deem_train_kernel = [np.asarray(i) for i in load_json(idxs_deem_train_kernel_file)]\n",
    "    idxs_deem_test_kernel = [np.asarray(i) for i in load_json(idxs_deem_test_kernel_file)]\n",
    "\n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "\n",
    "    idxs_deem_train_kernel = np.arange(idxs_iza_train.size, idxs_iza_train.size+idxs_deem_train.size)\n",
    "    np.random.shuffle(idxs_deem_train_kernel)\n",
    "    idxs_deem_train_kernel = np.split(idxs_deem_train_kernel, n_subsets)\n",
    "\n",
    "    idxs_deem_test_kernel = np.arange(idxs_iza_test.size, idxs_iza_test.size+idxs_deem_test.size)\n",
    "    np.random.shuffle(idxs_deem_test_kernel)\n",
    "    idxs_deem_test_kernel = np.split(idxs_deem_test_kernel, n_subsets)\n",
    "    \n",
    "    save_json(idxs_deem_train_kernel, [array.tolist() for array in idxs_deem_train_kernel_file])\n",
    "    save_json(idxs_deem_test_kernel, [array.tolist() for array in idxs_deem_test_kernel_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DEEM and IZA indices\n",
    "idxs_train_kernel = [np.concatenate((iza, deem)) for iza, deem in \n",
    "                     zip(idxs_iza_train_kernel, idxs_deem_train_kernel)]\n",
    "idxs_test_kernel = [np.concatenate((iza, deem)) for iza, deem in\n",
    "                    zip(idxs_iza_test_kernel, idxs_deem_test_kernel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datatype for saving KSVC classification accuracies and KPCovR losses\n",
    "dt_covr_list = [('alpha', 'f8'),\n",
    "                ('reg', 'f8'),\n",
    "                ('lr_train', 'f8', (n_subsets,)),\n",
    "                ('lr_test', 'f8', (n_subsets)),\n",
    "                ('lp_train', 'f8', (n_subsets,)),\n",
    "                ('lp_test', 'f8', (n_subsets))]\n",
    "\n",
    "dt_svm_list = [('C', 'f8'),\n",
    "               ('class_accuracy_train', 'f8', (n_subsets,)),\n",
    "               ('class_accuracy_test', 'f8', (n_subsets,))]\n",
    "\n",
    "save_json(dt_covr_list, '../Processed_Data/Models/covr_optimization_dtype.json')\n",
    "save_json(dt_svm_list, '../Processed_Data/Models/svm_optimization_dtype.json')\n",
    "\n",
    "dt_covr = np.dtype(dt_covr_list)\n",
    "dt_svm = np.dtype(dt_svm_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_deem_train_all_file = '../Processed_Data/Models/deem_train_ksvc-kpcovr_evaluation.idxs'\n",
    "idxs_deem_test_all_file = '../Processed_Data/Models/deem_test_ksvc-kpcovr_evaluation.idxs'\n",
    "\n",
    "# Load DEEM indices for running the models on the full optimization set\n",
    "try:\n",
    "    idxs_deem_train_all = np.loadtxt(idxs_deem_train_all_file, dtype=int)\n",
    "    idxs_deem_test_all = np.loadtxt(idxs_deem_test_all_file, dtype=int)\n",
    "    \n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "\n",
    "    idxs_deem_train_all = np.concatenate(idxs_deem_train_kernel)\n",
    "    idxs_deem_test_all = np.concatenate(idxs_deem_test_kernel)\n",
    "    \n",
    "    np.savetxt(idxs_deem_train_all_file, idxs_deem_train_all, fmt='%d')\n",
    "    np.savetxt(idxs_deem_test_all_file, idxs_deem_test_all, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_iza_train_all_file = '../Processed_Data/Models/iza_train_ksvc-kpcovr_evaluation.idxs'\n",
    "idxs_iza_test_all_file = '../Processed_Data/Models/iza_test_ksvc-kpcovr_evaluation.idxs'\n",
    "\n",
    "# Load IZA indices for running the models on the full optimization set\n",
    "try:\n",
    "    idxs_iza_train_all = np.loadtxt(idxs_iza_train_all_file, dtype=int)\n",
    "    idxs_iza_test_all = np.loadtxt(idxs_iza_test_all_file, dtype=int)\n",
    "\n",
    "# Compute indices if they don't exist\n",
    "except IOError:\n",
    "\n",
    "    idxs_iza = np.arange(0, n_iza)\n",
    "    n_iza_train = n_iza // 2\n",
    "    n_iza_test = n_iza - n_iza_train\n",
    "\n",
    "    np.random.shuffle(idxs_iza)\n",
    "    idxs_iza_train_all = idxs_iza[0:n_iza_train]\n",
    "    idxs_iza_test_all = idxs_iza[n_iza_train:]\n",
    "    \n",
    "    np.savetxt(idxs_iza_train_all_file, idxs_iza_train_all, fmt='%d')\n",
    "    np.savetxt(idxs_iza_test_all_file, idxs_iza_test_all, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DEEM and IZA indices\n",
    "idxs_train_all = np.concatenate((idxs_iza_train_all, idxs_deem_train_all))\n",
    "idxs_test_all = np.concatenate((idxs_iza_test_all, idxs_deem_test_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_10k'\n",
    "iza_name = 'IZA_226onDEEM_10k'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parameter optimization handling with defaults dictated here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model parameters\n",
    "# TODO: or use .get_params()?\n",
    "# TODO: should we make sure break_ties=True?\n",
    "svc_kwargs = dict(linear=dict(penalty='l2',\n",
    "                              loss='squared_hinge',\n",
    "                              dual=False,\n",
    "                              multi_class='ovr',\n",
    "                              class_weight=None,\n",
    "                              fit_intercept=True,\n",
    "                              intercept_scaling=1.0,\n",
    "                              tol=1.0E-3,\n",
    "                              C=1.0),\n",
    "                  kernel=dict(kernel='precomputed',\n",
    "                              decision_function_shape='ovr',\n",
    "                              class_weight=None,\n",
    "                              break_ties=False,\n",
    "                              tol=1.0E-3,\n",
    "                              C=1.0))\n",
    "\n",
    "n_components = 2\n",
    "pcovr_kwargs = dict(linear=dict(n_components=n_components, alpha=0.0, regularization=1.0E-12),\n",
    "                    kernel=dict(n_components=n_components, alpha=0.0, regularization=1.0E-12))\n",
    "\n",
    "C = np.logspace(-5, 5, 11)\n",
    "alphas = np.linspace(0.0, 1.0, 11)\n",
    "regularizations = np.logspace(-12, -1, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index slices for saving KSVC and KPCovR outputs\n",
    "deem_train_slice = slice(n_iza_train, None)\n",
    "deem_test_slice = slice(n_iza_test, None)\n",
    "iza_train_slice = slice(0, n_iza_train)\n",
    "iza_test_slice = slice(0, n_iza_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kernels or compute if they don't exist\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "        \n",
    "        if not os.path.exists(work_dir):\n",
    "            os.makedirs(work_dir)\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{work_dir}/structure_kernels_optimization.hdf5'\n",
    "        kernel_parameter_file = f'{work_dir}/volumes_mae_parameters.json'\n",
    "\n",
    "        if not os.path.exists(kernel_file):\n",
    "            \n",
    "            # SOAP files (atomwise, FPS'ed features)\n",
    "            deem_file = f'{deem_dir}/{cutoff}/soaps.hdf5'\n",
    "            iza_file = f'{iza_dir}/{cutoff}/soaps.hdf5'\n",
    "\n",
    "            # Assemble the train and test set SOAPs from IZA and DEEM\n",
    "            soaps_train, soaps_test = utils.load_soaps(deem_file, iza_file,\n",
    "                                                       idxs_deem_train, idxs_deem_test,\n",
    "                                                       idxs_iza_train, idxs_iza_test,\n",
    "                                                       idxs_iza_delete=[RWY])\n",
    "\n",
    "            # Compute kernels\n",
    "            # TODO: this can be consolidated if doing linear KRR\n",
    "            if kernel_type == 'gaussian':\n",
    "                kernel_parameters = load_json(kernel_parameter_file)\n",
    "                kernel_parameters.pop('sigma')\n",
    "                kernel_parameters.pop('regularization')\n",
    "            else:\n",
    "                kernel_parameters = dict(kernel='linear', zeta=1)\n",
    "            utils.compute_kernels(soaps_train, soaps_test, kernel_file=kernel_file, **kernel_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize KernelSVC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    for kernel_type in tqdm(('linear', 'gaussian'), desc='Kernel', leave=False):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "        \n",
    "        # Load kernels\n",
    "        kernel_file = f'{work_dir}/structure_kernels_optimization.hdf5'\n",
    "        K_train, K_test, K_test_test = utils.load_kernels(kernel_file)\n",
    "        \n",
    "        for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "            \n",
    "            # Output setup\n",
    "            output_dir = f'{n_cantons}-class'\n",
    "            \n",
    "            if not os.path.exists(f'{work_dir}/{output_dir}'):\n",
    "                os.makedirs(f'{work_dir}/{output_dir}')\n",
    "            \n",
    "            svc_parameter_file = f'{work_dir}/{output_dir}/svc_parameters.json'\n",
    "            svc_parameters = svc_kwargs['kernel'].copy()\n",
    "            \n",
    "            svm_errors = []\n",
    "            \n",
    "            for cdx, c in enumerate(tqdm(C, desc='C', leave=False)):\n",
    "                \n",
    "                class_accuracy_train = np.zeros(n_subsets)\n",
    "                class_accuracy_test = np.zeros(n_subsets)\n",
    "                \n",
    "                for n in tqdm(range(0, n_subsets), desc='n', leave=False):\n",
    "                    \n",
    "                    # Slice kernels\n",
    "                    idxs_train = idxs_train_kernel[n]\n",
    "                    idxs_test = idxs_test_kernel[n]\n",
    "                    \n",
    "                    k_train = K_train[idxs_train, :][:, idxs_train]\n",
    "                    k_test = K_test[idxs_test, :][:, idxs_train]\n",
    "                    k_test_test = K_test_test[idxs_test, :][:, idxs_test]\n",
    "                    \n",
    "                    # Center and scale kernels\n",
    "                    k_train, [k_test], [k_test_test] = \\\n",
    "                        utils.preprocess_kernels(k_train, [k_test], [k_test_test])\n",
    "                    \n",
    "                    y_train = cantons_train[n_cantons][idxs_train]\n",
    "                    y_test = cantons_test[n_cantons][idxs_test]\n",
    "                    \n",
    "                    # Run KSVC\n",
    "                    df_train, df_test, predicted_cantons_train, predicted_cantons_test = \\\n",
    "                        utils.do_svc(k_train, k_test, y_train, y_test, \n",
    "                                     svc_type='kernel', **svc_parameters, C=c, outputs=['scores'])\n",
    "                    \n",
    "                    class_accuracy_train[n] = train_score\n",
    "                    class_accuracy_test[n] = test_score\n",
    "                    \n",
    "                    # Collect errors\n",
    "                    model = np.array([(c, class_accuracy_train, class_accuracy_test)], dtype=dt_svm)\n",
    "                    svm_errors.append(model)\n",
    "            \n",
    "            svm_errors = np.concatenate(svm_errors)\n",
    "            \n",
    "            # Stack the arrays in a writable form so we can save in plain text instead of npy/npz\n",
    "            save_structured_array(f'{work_dir}/{output_dir}/svm_optimization.dat',\n",
    "                                  svm_errors, dt_svm)\n",
    "            \n",
    "            # TODO: use accuracy only on IZA structures?\n",
    "            class_accuracy_train_avg = np.mean(svm_errors['class_accuracy_train'], axis=1)\n",
    "            class_accuracy_test_avg = np.mean(svm_errors['class_accuracy_test'], axis=1)\n",
    "            \n",
    "            # Save KSVC parameters\n",
    "            idx_C = np.argmax(class_accuracy_test_avg)\n",
    "            C_opt = svm_errors['C'][idx_C]\n",
    "            svc_parameters.update(C=C_opt)\n",
    "            save_json(svc_parameter_file, svc_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal KernelSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    for kernel_type in tqdm(('linear', 'gaussian'), desc='Kernel', leave=False):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "        \n",
    "        # Load kernels\n",
    "        kernel_file = f'{work_dir}/structure_kernels_optimization.hdf5'\n",
    "        K_train, K_test, K_test_test = utils.load_kernels(kernel_file)\n",
    "        \n",
    "        # Since we need decision functions for all IZA structures in the train set,\n",
    "        # predict all structures\n",
    "        k_train_all = K_train[:, idxs_train_all]\n",
    "        k_test_all = K_test[:, idxs_train_all]\n",
    "\n",
    "        k_train = k_train_all[idxs_train_all, :]\n",
    "        k_test = K_test_all[idxs_test_all, :]\n",
    "        \n",
    "        k_train, k_test, k_train_all, k_test_all = \\\n",
    "            utils.preprocess_kernels(k_train, K_test=[k_test, k_train_all, k_test_all])\n",
    "        \n",
    "        for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "            \n",
    "            # Output setup\n",
    "            output_dir = f'{n_cantons}-class'\n",
    "            svc_parameter_file = f'{work_dir}/{output_dir}/svc_parameters.json'\n",
    "            svc_df_train_file = f'{work_dir}/{output_dir}/svc_structure_dfs_train_optimization.dat'\n",
    "            svc_df_test_file = f'{work_dir}/{output_dir}/svc_structure_dfs_test_optimization.dat'\n",
    "                    \n",
    "            # Assemble properties\n",
    "            y_train = cantons_train[n_cantons][idxs_train_all]\n",
    "            y_test = cantons_test[n_cantons][idxs_test_all]\n",
    "            \n",
    "            # KSVC\n",
    "            svc_parameters = load_json(svc_parameter_file)\n",
    "            df_train, df_test, train_score, test_score = \\\n",
    "                utils.do_svc(k_train, k_test, y_train, y_test, svc_type='kernel',\n",
    "                             outputs=['decision_functions', 'scores'])\n",
    "            \n",
    "            # Save decision functions for KRR test and KPCovR optimization\n",
    "            np.savetxt(svc_df_train_file, df_train)\n",
    "            np.savetxt(svc_df_test_file, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    for kernel_type in tqdm(('linear', 'gaussian'), desc='Kernel', leave=False):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "        \n",
    "        # Load kernels\n",
    "        kernel_file = f'{work_dir}/structure_kernels_optimization.hdf5'\n",
    "        K_train, K_test, K_test_test = utils.load_kernels(kernel_file)\n",
    "        \n",
    "        # Since we need decision functions for all IZA structures in the train set,\n",
    "        # predict all structures\n",
    "        k_train_all = K_train[:, idxs_train_all]\n",
    "        k_test_all = K_test[:, idxs_train_all]\n",
    "\n",
    "        k_train = k_train_all[idxs_train_all, :]\n",
    "        k_test = K_test_all[idxs_test_all, :]\n",
    "\n",
    "        k_train, k_test, k_train_all, k_test_all = \\\n",
    "            utils.preprocess_kernels(k_train, K_test=[k_test, k_train_all, k_test_all])\n",
    "        \n",
    "        for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "            \n",
    "            # Load decision functions\n",
    "            input_dir = f'{n_cantons}-class'\n",
    "            svc_df_train_file = f'{work_dir}/{input_dir}/svc_structure_dfs_train_optimization.dat'\n",
    "            svc_df_test_file = f'{work_dir}/{input_dir}/svc_structure_dfs_test_optimization.dat'\n",
    "            \n",
    "            df_train = np.loadtxt(svc_df_train_file)\n",
    "            df_test = np.loadtxt(svc_df_test_file)\n",
    "            \n",
    "            # Center and scale the decision functions\n",
    "            df_train, df_test, df_center, df_scale = \\\n",
    "                utils.preprocess_data(df_train, df_test)\n",
    "\n",
    "            # Check that KRR can reproduce the decision functions\n",
    "            utils.regression_check(K_train, K_test, df_train, df_test, regression_type='kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize KPCovR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    for kernel_type in tqdm(('linear', 'gaussian'), desc='Kernel', leave=False):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "        \n",
    "        # Load kernels\n",
    "        kernel_file = f'{work_dir}/structure_kernels_optimization.hdf5'\n",
    "        K_train, K_test, K_test_test = utils.load_kernels(kernel_file)\n",
    "        \n",
    "        for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "            \n",
    "            # Prepare outputs\n",
    "            output_dir = f'{n_cantons}-class' \n",
    "            pcovr_parameter_file = f'{work_dir}/{output_dir}/pcovr_parameters.json'\n",
    "            svc_parameter_file = f'{work_dir}/{output_dir}/svc_parameters.json'\n",
    "            \n",
    "            # Load decision functions\n",
    "            svc_df_train_file = f'{work_dir}/{output_dir}/svc_structure_dfs_optimization.dat'\n",
    "            svc_df_test_file = f'{work_dir}/{output_dir}/svc_structure_dfs_optimization.dat'\n",
    "            \n",
    "            df_train = np.loadtxt(svc_df_train_file)\n",
    "            df_test = np.loadtxt(svc_df_test_file)\n",
    "            \n",
    "            kpcovr_errors = []\n",
    "            \n",
    "            for adx, a in enumerate(tqdm(alphas, desc='Alpha', leave=False)):\n",
    "                for rdx, r in enumerate(tqdm(regularizations, desc='Reg', leave=False)):\n",
    "                    \n",
    "                    # Initialize matrices of losses\n",
    "                    lr_train = np.zeros(n_subsets)\n",
    "                    lp_train = np.zeros(n_subsets)\n",
    "                    lr_test = np.zeros(n_subsets)\n",
    "                    lp_test = np.zeros(n_subsets)\n",
    "                                        \n",
    "                    for n in tqdm(range(0, n_subsets), desc='n', leave=False):\n",
    "                        \n",
    "                        # Assemble kernels\n",
    "                        idxs_train = idxs_train_kernel[n]\n",
    "                        k_train = K_train[idxs_train, :][:, idxs_train]\n",
    "\n",
    "                        idxs_test = idxs_test_kernel[n]\n",
    "                        k_test = K_test[idxs_test, :][:, idxs_train]\n",
    "                        k_test_test = K_test_test[idxs_test, :][:, idxs_test]\n",
    "                        \n",
    "                        k_train, [k_test], [k_test_test] = \\\n",
    "                            utils.preprocess_kernels(k_train, K_test=[k_test], K_test_test=[k_test_test])\n",
    "\n",
    "                        # Assemble properties\n",
    "                        y_train, y_test, y_center, y_scale = \\\n",
    "                            utils.preprocess_data(df_train[idxs_train], df_test[idxs_test])\n",
    "                        \n",
    "                        # Run KPCovR\n",
    "                        kpcovr_parameters = pcovr_kwargs['kernel'].copy()\n",
    "                        t_train, t_test, yp_train, yp_test = \\\n",
    "                            utils.do_pcovr(k_train, k_test, y_train, y_test, \n",
    "                                           pcovr_type='kernel', **pcovr_parameters,\n",
    "                                           alpha=a, regularization=r)\n",
    "\n",
    "                        # Post process the KPCovR decision functions\n",
    "                        # (i.e., turn them back into canton predictions)\n",
    "                        predicted_cantons_train, predicted_cantons_test = \\\n",
    "                            utils.postprocess_decision_functions(yp_train, yp_test, y_center, y_scale,\n",
    "                                                                 df_type=svc_parameters['decision_function_shape'],\n",
    "                                                                 n_classes=n_cantons)\n",
    "\n",
    "                        # Compute regression and projection (KPCovR) losses for each canton\n",
    "                        # TODO: losses on just IZA?\n",
    "                        lr_train[n], lr_test[n] = \\\n",
    "                            utils.compute_regression_losses(y_train, y_test, yp_train, yp_test)\n",
    "                        \n",
    "                        lp_train[n], lp_test[n] = \\\n",
    "                            utils.compute_kernel_projection_losses(k_train, k_test, k_test_test, \n",
    "                                                                   t_train, t_test)\n",
    "                     \n",
    "                    # Collect errors\n",
    "                    model = np.array([(a, r, lr_train, lr_test, lp_train, lp_test)], dtype=dt_covr)\n",
    "                    kpcovr_errors.append(model)\n",
    "            \n",
    "            kpcovr_errors = np.concatenate(kpcovr_errors)\n",
    "                    \n",
    "            save_structured_array(f'{work_dir}/{output_dir}/kpcovr_optimization.dat', \n",
    "                                  kpcovr_errors, dt_kpcovr)\n",
    "            \n",
    "            # Extract optimal hyperparameters\n",
    "            lr_avg_test = np.mean(kpcovr_errors['lr_test'], axis=1)\n",
    "            lp_avg_test = np.mean(kpcovr_errors['lp_test'], axis=1)\n",
    "            \n",
    "            idx_opt = np.argmin(lr_avg_test + lp_avg_test)\n",
    "            \n",
    "            opt_alpha = kpcovr_errors['alpha'][idx_opt]\n",
    "            opt_reg = kpcovr_errors['regularization'][idx_opt]\n",
    "\n",
    "            print(opt_alpha, opt_reg)\n",
    "\n",
    "            # Save KPCovR parameters\n",
    "            kpcovr_parameters = dict(n_components=n_components,\n",
    "                                     regularization=opt_reg,\n",
    "                                     alpha=opt_alpha)\n",
    "            \n",
    "            save_json(kpcovr_parameters, kpcovr_parameter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        kernel_name = kernel_type.capitalize()\n",
    "        \n",
    "        work_dir = f'{model_dir}/{cutoff}/Kernel_Models/{kernel_name}/KSVC-KPCovR'\n",
    "\n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            print(f'Cutoff: {cutoff}, Kernel: {kernel_name}, {n_cantons}-Class')\n",
    "            \n",
    "            output_dir = f'{n_cantons}-class'\n",
    "            \n",
    "            loss_file = f'{work_dir}/{output_dir}/kpcovr_optimization.dat'\n",
    "            \n",
    "            # Load losses\n",
    "            alphas, opt_reg_idx, \\\n",
    "            lr_train_matrix, lr_test_matrix, \\\n",
    "            lp_train_matrix, lp_test_matrix = load_covr_losses(loss_file)\n",
    "            \n",
    "            fig = plt.figure(figsize=(7.0, 7.0))\n",
    "            axs_loss_sum_train = fig.add_subplot(1, 2, 1)\n",
    "            axs_loss_sum_test = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the train set\n",
    "            axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_train.semilogy(alphas, lp_train_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx]+lp_train_matrix[:, opt_reg_idx], \n",
    "                                        'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_train.legend()\n",
    "            axs_loss_sum_train.set_title('Train')\n",
    "            axs_loss_sum_train.set_xlabel('alpha')\n",
    "            axs_loss_sum_train.set_ylabel('loss')\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the test set\n",
    "            axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_test.semilogy(alphas, lp_test_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx]+lp_test_matrix[:, opt_reg_idx],\n",
    "                                       'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_test.legend()\n",
    "            axs_loss_sum_test.set_title('Test')\n",
    "            axs_loss_sum_test.set_xlabel('alpha')\n",
    "            axs_loss_sum_test.set_ylabel('loss')\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model setup\n",
    "n_species = 2\n",
    "group_names = {'power': ['OO', 'OSi', 'SiSi'], 'radial': ['O', 'Si']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize LinearSVC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/LSVC-LPCovR'\n",
    "    \n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        \n",
    "        soaps_train, soaps_test = utils.load_soaps(deem_file, iza_file,\n",
    "                                                   idxs_deem_train, idxs_deem_test,\n",
    "                                                   idxs_iza_train, idxs_iza_test,\n",
    "                                                   idxs_iza_delete=[RWY],\n",
    "                                                   train_test_concatenate=True)\n",
    "        \n",
    "        # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "        soaps_train, soaps_test = utils.preprocess_soaps(soaps_train, soaps_test)\n",
    "        \n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):            \n",
    "            \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "\n",
    "                # Prepare outputs\n",
    "                output_dir = f'{n_cantons}-class/{spectrum_name}/{species_pairing}'\n",
    "                \n",
    "                if not os.path.exists(f'{work_dir}/{output_dir}'):\n",
    "                    os.makedirs(f'{work_dir}/{output_dir}')\n",
    "                \n",
    "                svc_parameter_file = f'{work_dir}/{output_dir}/svc_parameters.json'\n",
    "                svc_parameters = svc_kwargs['linear'].copy()\n",
    "\n",
    "                for cdx, c in enumerate(tqdm(C), desc='C', leave=False):\n",
    "                    svc_parameters.update(C=c)\n",
    "\n",
    "                    for n in tqdm(range(0, n_subsets), desc='n', leave=False):\n",
    "                        \n",
    "                        # Slice SOAPs\n",
    "                        idxs_train = idxs_train_kernel[n]\n",
    "                        idxs_test = idxs_test_kernel[n]\n",
    "\n",
    "                        x_train = soaps_train[idxs_train, feature_idxs]\n",
    "                        x_test = soaps_test[idxs_test, feature_idxs]\n",
    "                        \n",
    "                        # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "                        x_train, x_test = utils.preprocess_soaps(x_train, x_test)\n",
    "\n",
    "                        y_train = cantons_train[n_cantons][idxs_train]\n",
    "                        y_test = cantons_test[n_cantons][idxs_test]\n",
    "\n",
    "                        # Run LSVC\n",
    "                        train_score, test_score = \\\n",
    "                            utils.do_svc(x_train, x_test, y_train, y_test, \n",
    "                                         svc_type='linear', **svc_parameters, outputs=['scores'])\n",
    "\n",
    "                        class_accuracy_train[n] = train_score\n",
    "                        class_accuracy_test[n] = test_score\n",
    "                    \n",
    "                    model = np.array([(c, class_accuracy_train, class_accuracy_test)], dtype=dt_svm)\n",
    "                    svm_errors.append(model)\n",
    "            \n",
    "            svm_errors = np.concatenate(svm_errors)\n",
    "            \n",
    "            # Stack the arrays in a writable form so we can save in plain text instead of npy/npz\n",
    "            save_structured_array(f'{work_dir}/{output_dir}/svm_optimization.dat',\n",
    "                                  svm_errors, dt_svm)\n",
    "            \n",
    "            # TODO: use accuracy only on IZA structures?\n",
    "            class_accuracy_train_avg = np.mean(svm_errors['class_accuracy_train'], axis=1)\n",
    "            class_accuracy_test_avg = np.mean(svm_errors['class_accuracy_test'], axis=1)\n",
    "            \n",
    "            # Save LSVC parameters\n",
    "            idx_C = np.argmax(class_accuracy_test_avg)\n",
    "            C_opt = svm_errors['C'][idx_C]\n",
    "            svc_parameters.update(C=C_opt)\n",
    "            save_json(svc_parameter_file, svc_parameters)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/LSVC-LPCovR'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        \n",
    "        soaps_train, soaps_test = utils.load_soaps(deem_file, iza_file,\n",
    "                                                   idxs_deem_train, idxs_deem_test,\n",
    "                                                   idxs_iza_train, idxs_iza_test,\n",
    "                                                   idxs_iza_delete=[RWY],\n",
    "                                                   train_test_concatenate=True)\n",
    "        \n",
    "        # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "        soaps_train, soaps_test = utils.preprocess_soaps(soaps_train, soaps_test)\n",
    "        \n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):                        \n",
    "            \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "\n",
    "                # Prepare outputs\n",
    "                output_dir = f'{n_cantons}-class/{spectrum_name}/{species_pairing}'\n",
    "                svc_parameter_file = f'{work_dir}/{output_dir}/svc_parameters.json'\n",
    "\n",
    "                svc_df_train_file = f'{work_dir}/{output_dir}/svc_structure_dfs_train_optimization.dat'\n",
    "                svc_df_test_file = f'{work_dir}/{output_dir}/svc_structure_dfs_test_optimization.dat'\n",
    "\n",
    "                # Assemble properties\n",
    "                y_train = cantons_train[n_cantons][idxs_train_all]\n",
    "                y_test = cantons_test[n_cantons][idxs_test_all]\n",
    "                \n",
    "                x_train = soaps_train[idxs_train_all, feature_idxs]\n",
    "                x_test = soaps_test[idxs_train_all, feature_idxs]\n",
    "                \n",
    "                # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "                x_train, x_test = utils.preprocess_soaps(x_train, x_test)\n",
    "\n",
    "                # LSVC\n",
    "                svc_parameters = load_json(svc_parameter_file)\n",
    "                df_train, df_test, train_score, test_score = \\\n",
    "                    utils.do_svc(soaps_train, soaps_test, y_train, y_test, svc_type='linear',\n",
    "                                 outputs=['decision_functions', 'scores'])\n",
    "\n",
    "                # Save decision functions for LR test and PCovR optimization\n",
    "                np.savetxt(svc_df_train_file, df_train)\n",
    "                np.savetxt(svc_df_test_file, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/LSVC-LPCovR'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "\n",
    "        soaps_train, soaps_test = utils.load_soaps(deem_file, iza_file,\n",
    "                                                   idxs_deem_train, idxs_deem_test,\n",
    "                                                   idxs_iza_train, idxs_iza_test,\n",
    "                                                   idxs_iza_delete=[RWY],\n",
    "                                                   train_test_concatenate=True)\n",
    "        \n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):\n",
    "            \n",
    "            x_train = soaps_train[:, feature_idxs]\n",
    "            x_test = soaps_test[:, feature_idxs]\n",
    "\n",
    "            # Preprocess the SOAPs like the decision functions\n",
    "            # (i.e., center and scale) for the regression.\n",
    "            x_train, x_test, x_center, x_scale = \\\n",
    "                utils.preprocess_data(x_train, x_test)\n",
    "                        \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "            \n",
    "                # Load decision functions\n",
    "                input_dir = f'{n_cantons}-class/{spectrum_name}/{species_pairing}'\n",
    "\n",
    "                # Files to store IZA and DEEM KSVC decision functions\n",
    "                svc_df_train_file = f'{work_dir}/{input_dir}/svc_structure_dfs_train_optimization.dat'\n",
    "                svc_df_test_file = f'{work_dir}/{input_dir}/svc_structure_dfs_test_optimization.dat'\n",
    "\n",
    "                df_train = np.loadtxt(svc_df_train_file)\n",
    "                df_test = np.loadtxt(svc_df_test_file)\n",
    "\n",
    "                # Center and scale the decision functions\n",
    "                df_train, df_test, df_center, df_scale = \\\n",
    "                    utils.preprocess_data(df_train, df_test)\n",
    "\n",
    "                # Check that LR can reproduce the decision functions\n",
    "                utils.regression_check(x_train, x_test, df_train, df_test, regression_type='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize PCovR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/LSVC-LPCovR'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        \n",
    "        soaps_train, soaps_test = utils.load_soaps(deem_file, iza_file,\n",
    "                                                   idxs_deem_train, idxs_deem_test,\n",
    "                                                   idxs_iza_train, idxs_iza_test,\n",
    "                                                   idxs_iza_delete=[RWY],\n",
    "                                                   train_test_concatenate=True)\n",
    "        \n",
    "        # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "        soaps_train, soaps_test = utils.preprocess_soaps(soaps_train, soaps_test)\n",
    "        \n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):\n",
    "                        \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "                \n",
    "                # Prepare outputs\n",
    "                output_dir = f'{n_cantons}-class/{spectrum_name}/{species_pairing}'\n",
    "                pcovr_parameter_file = f'{work_dir}/{output_dir}/pcovr_parameters.json'\n",
    "\n",
    "                # Load decision functions\n",
    "                svc_df_train_file = f'{work_dir}/{output_dir}/svc_structure_dfs_train_optimization.dat'\n",
    "                svc_df_test_file = f'{work_dir}/{output_dir}/svc_structure_dfs_test_optimization.dat'\n",
    "\n",
    "                df_train = np.loadtxt(svc_df_train_file)\n",
    "                df_test = np.loadtxt(svc_df_test_file)\n",
    "\n",
    "                for adx, a in enumerate(tqdm(alphas, desc='Alpha', leave=False)):\n",
    "                    for rdx, r in enumerate(tqdm(regularizations, desc='Reg', leave=False)):\n",
    "\n",
    "                        # Initialize matrices of losses\n",
    "                        lr_train = np.zeros(n_subsets)\n",
    "                        lp_train = np.zeros(n_subsets)\n",
    "                        lr_test = np.zeros(n_subsets)\n",
    "                        lp_test = np.zeros(n_subsets)\n",
    "\n",
    "                        for n in tqdm(range(0, n_subsets), desc='n', leave=False):\n",
    "\n",
    "                            # Slice the SOAPs\n",
    "                            idxs_train = idxs_train_kernel[n]\n",
    "                            idxs_test = idxs_test_kernel[n]\n",
    "\n",
    "                            x_train = soaps_train[idxs_train, feature_idxs]\n",
    "                            x_test = soaps_test[idxs_test, feature_idxs]\n",
    "\n",
    "                            # Scale the SOAPs so they are of a 'usable' magnitude for the SVC\n",
    "                            x_train, x_test = utils.preprocess_soaps(x_train, x_test)\n",
    "\n",
    "                            y_train = df_train[idxs_train]\n",
    "                            y_test = df_test[idxs_test]\n",
    "\n",
    "                            # Assemble properties\n",
    "                            y_train, y_test, y_center, y_scale = \\\n",
    "                                utils.preprocess_data(y_train, y_test)\n",
    "\n",
    "                            # Run PCovR\n",
    "                            pcovr_parameters = pcovr_kwargs['linear'].copy()\n",
    "                            t_train, t_test, yp_train, yp_test, xr_train, xr_test = \\\n",
    "                                utils.do_pcovr(x_train, x_test, y_train, y_test, \n",
    "                                               pcovr_type='linear', compute_xr=True,\n",
    "                                               **pcovr_parameters,\n",
    "                                               alpha=a, regularization=r)\n",
    "\n",
    "                            # Post process the PCovR decision functions\n",
    "                            # (i.e., turn them back into canton predictions)\n",
    "                            predicted_cantons_train, predicted_cantons_test = \\\n",
    "                                utils.postprocess_decision_functions(yp_train, yp_test, y_center, y_scale)\n",
    "\n",
    "                            # Collect regression and projection (PCovR) losses for each canton\n",
    "                            # TODO: losses on just IZA?\n",
    "                            lr_train[n], lr_test[n] = \\\n",
    "                                utils.compute_regression_losses(y_train, y_test, yp_train, yp_test)\n",
    "\n",
    "                            lp_train[n], lp_test[n] = \\\n",
    "                                utils.compute_linear_projection_losses(x_train, x_test,\n",
    "                                                                       xr_train, xr_test)\n",
    "\n",
    "                        # Collect errors\n",
    "                        model = np.array([(a, r, lr_train, lr_test, lp_train, lp_test)], dtype=dt_covr)\n",
    "                        pcovr_errors.append(model)\n",
    "\n",
    "                pcovr_errors = np.concatenate(pcovr_errors)\n",
    "\n",
    "                save_structured_array(f'{work_dir}/{output_dir}/pcovr_optimization.dat', \n",
    "                                      pcovr_errors, dt_kpcovr)\n",
    "\n",
    "                # Extract optimal hyperparameters\n",
    "                lr_avg_test = np.mean(kpcovr_errors['lr_test'], axis=1)\n",
    "                lp_avg_test = np.mean(kpcovr_errors['lp_test'], axis=1)\n",
    "\n",
    "                idx_opt = np.argmin(lr_avg_test + lp_avg_test)\n",
    "\n",
    "                opt_alpha = kpcovr_errors['alpha'][idx_opt]\n",
    "                opt_reg = kpcovr_errors['regularization'][idx_opt]\n",
    "\n",
    "                print(opt_alpha, opt_reg)\n",
    "\n",
    "                # Save KPCovR parameters\n",
    "                pcovr_parameters.update(alpha=opt_alpha,\n",
    "                                        regularization=opt_reg)\n",
    "\n",
    "                save_json(pcovr_parameters, pcovr_parameter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/LSVC-LPCovR'\n",
    "    \n",
    "    for spectrum_type in ('power', 'radial'):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(group_names[spectrum_type], feature_groups):\n",
    "                        \n",
    "            for n_cantons in (2, 4):\n",
    "                \n",
    "                print(f'Cutoff: {cutoff}, Spectrum: {spectrum_name}, Species: {species_pairing}, {n_cantons}-Class')\n",
    "                \n",
    "                output_dir = f'{n_cantons}-class/{spectrum_name}/{species_pairing}'\n",
    "            \n",
    "                loss_file = f'{work_dir}/{output_dir}/pcovr_optimization.dat'\n",
    "\n",
    "                # Load losses\n",
    "                alphas, opt_reg_idx, \\\n",
    "                lr_train_matrix, lr_test_matrix, \\\n",
    "                lp_train_matrix, lp_test_matrix = load_covr_losses(loss_file)\n",
    "\n",
    "                fig = plt.figure(figsize=(7.0, 7.0))\n",
    "                axs_loss_sum_train = fig.add_subplot(1, 2, 1)\n",
    "                axs_loss_sum_test = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "                # Sum of projection and regression loss over all cantons for the train set\n",
    "                axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "                axs_loss_sum_train.semilogy(alphas, lp_train_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "                axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx]\n",
    "                                            + lp_train_matrix[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "                axs_loss_sum_train.legend()\n",
    "                axs_loss_sum_train.set_title('Train')\n",
    "                axs_loss_sum_train.set_xlabel('alpha')\n",
    "                axs_loss_sum_train.set_ylabel('loss')\n",
    "\n",
    "                # Sum of projection and regression loss over all cantons for the test set\n",
    "                axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "                axs_loss_sum_test.semilogy(alphas, lp_test_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "                axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx]\n",
    "                                           + lp_test_matrix[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "                axs_loss_sum_test.legend()\n",
    "                axs_loss_sum_test.set_title('Test')\n",
    "                axs_loss_sum_test.set_xlabel('alpha')\n",
    "                axs_loss_sum_test.set_ylabel('loss')\n",
    "\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    if C_override is not None:\n",
    "        C = C_override\n",
    "    else:\n",
    "        C = model_dict['C']\n",
    "    \n",
    "    # TODO: if using LogisticRegression, need to optimize with LogisticRegression, but re-use C for now\n",
    "    # NOTE: no ovo option for LogisticRegression\n",
    "    # NOTE: logistic regression seems to want to predict everything as DEEM, moreso than SVM\n",
    "    logr = LogisticRegression(penalty='l2', dual=False, C=C, \n",
    "                              fit_intercept=True, solver='saga',\n",
    "                              multi_class='auto', max_iter=1000, tol=1.0E-3)\n",
    "    \n",
    "    logr.fit(soaps_train[cutoff], cantons_train)\n",
    "    \n",
    "    df_train = logr.decision_function(soaps_train[cutoff])\n",
    "    df_test = logr.decision_function(soaps_test[cutoff])\n",
    "    \n",
    "    predicted_cantons_train = logr.predict(soaps_train[cutoff])\n",
    "    predicted_cantons_test = logr.predict(soaps_test[cutoff])\n",
    "    \n",
    "    probabilities_train = logr.predict_proba(soaps_train[cutoff])\n",
    "    probabilities_test = logr.predict_proba(soaps_test[cutoff])\n",
    "    \n",
    "    # Save decision functions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "    \n",
    "    df_deem[idxs_deem_train] = df_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = df_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = df_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = df_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save LogR class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "    # Save LogR probabilities\n",
    "    probabilities_deem = np.zeros((n_deem, n_classes))\n",
    "    probabilities_deem[idxs_deem_train] = probabilities_train[n_iza_train:]\n",
    "    probabilities_deem[idxs_deem_test] = probabilities_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    probabilities_iza = np.zeros((n_iza, n_classes))\n",
    "    probabilities_iza[idxs_iza_train] = probabilities_train[0:n_iza_train]\n",
    "    probabilities_iza[idxs_iza_test] = probabilities_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "#########\n",
    "    print(logr.score(soaps_train[cutoff], cantons_train))\n",
    "    print(logr.score(soaps_test[cutoff], cantons_test))\n",
    "    print(logr.coef_)\n",
    "    w = reshape_soaps(logr.coef_, 3, 12, 9)\n",
    "    density = compute_soap_density(12, 9, cutoff, w,\n",
    "                                   np.linspace(0, cutoff, 50),\n",
    "                                   np.linspace(-1, 1, 50),\n",
    "                                   chunk_size_r=10, chunk_size_p=10)\n",
    "#########"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logr.n_iter_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(probabilities_deem)\n",
    "print(probabilities_iza)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem, axis=1))\n",
    "print(np.amax(probabilities_iza, axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem))\n",
    "print(np.amax(probabilities_iza))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(probabilities_deem[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.hist(probabilities_iza[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
