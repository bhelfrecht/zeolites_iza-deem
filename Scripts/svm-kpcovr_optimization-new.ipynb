{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/helfrech/.config/matplotlib/stylelib/cosmo.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In /home/helfrech/.config/matplotlib/stylelib/cosmoLarge.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "/home/helfrech/ENVIRONMENTS/ZEOLITES/lib/python3.6/_collections_abc.py:841: MatplotlibDeprecationWarning:\n",
      "\n",
      "\n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ML\n",
    "from regression import PCovR, KPCovR, SparseKPCovR\n",
    "from regression import LR, KRR\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "from kernels import center_kernel, center_kernel_fast\n",
    "from kernels import center_kernel_oos, center_kernel_oos_fast\n",
    "from soap import compute_soap_density, reshape_soaps\n",
    "from soap import rrw_neighbors, make_tuples\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read\n",
    "from ase.neighborlist import neighbor_list\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5, df_to_class\n",
    "from tools import load_json\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/kernel-tutorials')\n",
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/KernelPCovR/analysis/scripts')\n",
    "# from utilities.sklearn_covr.kpcovr import KernelPCovR as KPCovR2\n",
    "# from utilities.sklearn_covr.pcovr import PCovR as PCovR2\n",
    "from helpers import l_regr, l_kpcovr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_soaps(deem_file, iza_file, \n",
    "               idxs_deem_train, idxs_deem_test, \n",
    "               idxs_iza_train, idxs_iza_test,\n",
    "               idxs_deem_delete=[], idxs_iza_delete=[]):\n",
    "    \n",
    "    # Load SOAPs\n",
    "    soaps_deem = load_structures_from_hdf5(deem_file, datasets=None, concatenate=False)\n",
    "    for i in sorted(idxs_deem_delete, reverse=True):\n",
    "        soaps_deem.pop(i)\n",
    "    \n",
    "    soaps_iza = load_structures_from_hdf5(iza_file, datasets=None, concatenate=False)\n",
    "    for i in sorted(idxs_iza_delete, reverse=True):\n",
    "        soaps_iza.pop(i)\n",
    "        \n",
    "    # Build the train and test sets\n",
    "    deem_train = [soaps_deem[i] for i in idxs_deem_train]\n",
    "    deem_test = [soaps_deem[i] for i in idxs_deem_test]\n",
    "    iza_train = [soaps_iza[i] for i in idxs_iza_train]\n",
    "    iza_test = [soap_iza[i] for i in idxs_iza_test]\n",
    "    \n",
    "    soaps_train = iza_train + deem_train\n",
    "    soaps_test = iza_test + deem_test\n",
    "    \n",
    "    return soaps_train, soaps_test\n",
    "\n",
    "def preprocess_soaps(soaps_train, soaps_test):\n",
    "        \n",
    "    # Can also do other scaling/centering here -- \n",
    "    # this is mostly just to get the SOAPs to a 'usable' magnitude\n",
    "    soaps_scale = np.std(soaps_train)\n",
    "    soaps_train_scaled = soaps_train / soaps_scale\n",
    "    soaps_test_scaled = soaps_test / soaps_scale\n",
    "    \n",
    "    return soaps_train, soaps_test\n",
    "\n",
    "def load_data(deem_file, iza_file,\n",
    "              idxs_deem_train, idxs_deem_test,\n",
    "              idxs_iza_train, idxs_iza_test):\n",
    "    \n",
    "    deem_data = np.loadtxt(deem_file)\n",
    "    iza_data = np.loadtxt(iza_file)\n",
    "        \n",
    "    deem_data_train = deem_data[idxs_deem_train]\n",
    "    deem_data_test = deem_data[idxs_deem_test]\n",
    "    \n",
    "    iza_data_train = iza_data[idxs_iza_train]\n",
    "    iza_data_test = iza_data[idxs_iza_test]\n",
    "    \n",
    "    train_data = np.concatenate((iza_data_train, deem_data_train))\n",
    "    test_data = np.concatenate((iza_data_test, deem_data_test))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def load_kernels(kernel_file):\n",
    "    \n",
    "    # Load the kernels\n",
    "    f = h5py.File(kernel_file, 'r')\n",
    "\n",
    "    K_train = f['K_train'][:]\n",
    "    K_test = f['K_test'][:]\n",
    "    K_test_test = f['K_test_test'][:]\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return K_train, K_test, K_test_test\n",
    "    \n",
    "def compute_kernels(soaps_train, soaps_test, **kwargs):\n",
    "    \n",
    "\n",
    "    # Build kernel between all DEEM and all IZA\n",
    "    K_train = build_kernel(soaps_train, soaps_train, **kwargs)\n",
    "    K_test = build_kernel(soaps_test, soaps_train, **kwargs)\n",
    "    K_test_test = build_kernel(soaps_test, soaps_test, **kwargs)\n",
    "        \n",
    "    if save:\n",
    "        \n",
    "        # Save kernels for later\n",
    "        g = h5py.File(kernel_file, 'w')\n",
    "\n",
    "        g.create_dataset('K_train', data=K_train)\n",
    "        g.create_dataset('K_test', data=K_test)\n",
    "        g.create_dataset('K_test_test', data=K_test_test)\n",
    "\n",
    "        for k, v in kernel_parameters.items():\n",
    "            g.attrs[k] = v\n",
    "\n",
    "        g.close()\n",
    "    \n",
    "    return K_train, K_test, K_test_test\n",
    "    \n",
    "def preprocess_kernels(K_train, K_test, K_test_test):\n",
    "    \n",
    "    # Center and scale kernels\n",
    "    K_test_test = center_kernel_oos(K_test_test, K_bridge=K_test, K_ref=K_train)\n",
    "    K_test = center_kernel_fast(K_test, K_ref=K_train)\n",
    "    K_train = center_kernel_fast(K_train)\n",
    "\n",
    "    K_scale = np.trace(K_train) / K_train.shape[0]\n",
    "    \n",
    "    K_test_test /= K_scale\n",
    "    K_test /= K_scale\n",
    "    K_train /= K_scale\n",
    "    \n",
    "    return K_train, K_test, K_test_test\n",
    "\n",
    "def do_svc(train_data, test_data, train_classes, test_classes,\n",
    "           svc_type='linear', outputs=['decision_functions', 'predictions', 'scores'], **kwargs):\n",
    "    \n",
    "    if svc_type == 'kernel':\n",
    "        svc = SVC(**kwargs)\n",
    "        \n",
    "    elif svc_type == 'linear':\n",
    "        svc = LinearSVC(**kwargs)\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: invalid svc_type; valid choices are 'kernel' and 'linear'\")\n",
    "        return\n",
    "\n",
    "    svc.fit(train_data, train_classes)\n",
    "    \n",
    "    output_list = []\n",
    "    \n",
    "    # Structure in this way to return in the same order as given in the outputs list\n",
    "    for out in outputs:\n",
    "        if out == 'decision_functions':\n",
    "            df_train = svc.decision_function(train_data)\n",
    "            df_test = svc.decision_function(test_data)\n",
    "            output_list.extend((df_train, df_test))\n",
    "            \n",
    "        elif out == 'predictions':\n",
    "            predicted_train_classes = svc.predict(train_data)\n",
    "            predicted_test_classes = svc.predict(test_data)\n",
    "            output_list.extend((predicted_train_classes, predicted_test_classes))\n",
    "        \n",
    "            print(classification_report(test_classes, predicted_test_classes))\n",
    "            print(confusion_matrix(test_classes, predicted_test_classes))\n",
    "            \n",
    "        elif out == 'scores':\n",
    "            train_score = svc.score(train_data, train_classes)\n",
    "            test_score = svc.score(test_data, test_classes)\n",
    "            output_list.extend((train_scores, test_scores))\n",
    "\n",
    "            print(train_score)\n",
    "            print(test_score)\n",
    "            \n",
    "    return output_list\n",
    "           \n",
    "def regression_check(train_data, test_data,\n",
    "                     train_target, test_target,\n",
    "                     regression_type='linear'):\n",
    "    \n",
    "    if regression_type == 'linear':\n",
    "        regression_func = LR\n",
    "        \n",
    "    elif regression_type == 'kernel':\n",
    "        regression_func = KRR\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: invalid regression type; use 'linear' or 'kernel'\")\n",
    "        return\n",
    "\n",
    "    # Test KRR on decision functions\n",
    "    # NOTE: KRR can't predict the test set\n",
    "    # decision function very well -- why? <-- TODO: is this only for LinearSVC or also SVC?\n",
    "    \n",
    "#     regressor = KernelRidge(alpha=1.0E-12, kernel='precomputed')\n",
    "#     regressor.fit(train_data, train_target)\n",
    "#     predicted_train_target = regressor.predict(train_data)\n",
    "#     predicted_test_target = regressor.predict(test_data)\n",
    "\n",
    "    regressor = regression_func(regularization=1.0E-12)\n",
    "    regressor.fit(train_data, train_target)\n",
    "    predicted_train_target = regressor.transform(train_data)\n",
    "    predicted_test_target = regressor.transform(test_data)\n",
    "    \n",
    "    print(np.mean(np.abs(predicted_train_target - train_target), axis=0))\n",
    "    print(np.mean(np.abs(predicted_test_target - test_target), axis=0))\n",
    "\n",
    "def preprocess_decision_functions(df_train, df_test):\n",
    "    df_center = np.mean(df_train, axis=0)\n",
    "    \n",
    "    df_train -= df_center\n",
    "    df_test -= df_center\n",
    "    \n",
    "    if df_train.ndim == 1:\n",
    "        df_scale = np.linalg.norm(df_train) / np.sqrt(df_train.size)\n",
    "    else:\n",
    "        df_scale = np.linalg.norm(df_train, axis=0) / np.sqrt(df_train.shape[0] / df_train.shape[1])\n",
    "    \n",
    "    df_train /= df_scale\n",
    "    df_test /= df_scale\n",
    "    \n",
    "    return df_train, df_test, df_center, df_scale\n",
    "\n",
    "def postprocess_decision_functions(df_train, df_test, df_center, df_scale):\n",
    "    \n",
    "    # Rescale to raw decision function\n",
    "    dfp_train = dfp_train * df_scale + df_center\n",
    "    dfp_test = dfp_test * df_scale + df_center\n",
    "\n",
    "    # Predict classes based on KPCovRized decision functions\n",
    "    predicted_cantons_train = df_to_class(dfp_train, df_type, n_classes)\n",
    "    predicted_cantons_test = df_to_class(dfp_test, df_type, n_classes)\n",
    "    \n",
    "def split_and_save(train_data, test_data, \n",
    "                   train_idxs, test_idxs, \n",
    "                   train_slice, test_slice, \n",
    "                   hdf5_attrs=None, \n",
    "                   output, output_format='%f'):\n",
    "    \n",
    "    # Save KPCovR class predictions\n",
    "    n_samples = len(train_data) + len(test_data)\n",
    "    data = np.zeros((n_samples, train_data.shape[1]))\n",
    "    data[train_idxs] = train_data[train_slice]\n",
    "    data[test_idxs] = test_data[test_slice]\n",
    "    \n",
    "    if hdf5_attrs is not None:\n",
    "        n_digits = len(str(n_samples - 1))\n",
    "        g = h5py.File(output, 'w')\n",
    "        for ddx, d in enumerate(data):\n",
    "            g.create_dataset(str(ddx).zfill(n_digits), data=d)\n",
    "\n",
    "        for k, v in hdf5_attrs.items():\n",
    "            g.attrs[k] = v\n",
    "\n",
    "        g.close()\n",
    "    else:\n",
    "        np.savetxt(output, data, fmt=output_format)\n",
    "\n",
    "def do_covr(train_data, test_data, \n",
    "            train_targets, test_targets,\n",
    "            covr_type='linear', **covr_parameters):\n",
    "    \n",
    "    if covr_type == 'linear':\n",
    "        covr_func = PCovR\n",
    "    elif covr_type == 'kernel':\n",
    "        covr_func = KPCovR\n",
    "    else:\n",
    "        print(\"Error: invalid CovR type; use 'linear' or 'gaussian'\")\n",
    "    \n",
    "    covr = covr_func(**covr_parameters)\n",
    "    \n",
    "    covr.fit(train_data, train_targets)\n",
    "    \n",
    "    T_train = covr.transform_K(train_data)\n",
    "    predicted_train_target = covr.transform_Y(train_data)\n",
    "    T_test = covr.transform_K(test_data)\n",
    "    predicted_test_target = covr.transform_Y(test_data)\n",
    "    \n",
    "    predicted_train_target = np.squeeze(dfp_train) # TODO: move the squeezing to the KPCovR function\n",
    "    predicted_test_target = np.squeeze(dfp_test)\n",
    "    \n",
    "    return T_train, T_test, predicted_train_target, predicted_test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set indices for Deem\n",
    "idxs_deem = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_deem = idxs_deem.size + np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select DEEM sample for training/testing (already shuffled)\n",
    "n_deem_train= 5000\n",
    "n_deem_test = 2750\n",
    "idxs_deem_train = idxs_deem[0:n_deem_train]\n",
    "idxs_deem_test = idxs_deem[n_deem_train:n_deem_train+n_deem_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy DEEM cantons\n",
    "cantons_deem = np.ones(n_deem, dtype=int) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "cantons_iza = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)\n",
    "RWY = np.nonzero(cantons_iza == 4)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_iza = np.delete(cantons_iza, RWY)\n",
    "n_iza = len(cantons_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_iza = np.ones(n_iza, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select IZA sample\n",
    "idxs_iza_train = np.arange(0, n_iza)\n",
    "idxs_iza_test = np.arange(0, n_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons_train = np.concatenate((cantons_iza[idxs_iza_train], cantons_deem[idxs_deem_train]))\n",
    "cantons_test = np.concatenate((cantons_iza[idxs_iza_test], cantons_deem[idxs_deem_test]))\n",
    "n_classes = np.amax(cantons_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assign slices for saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "deem_dir = '../Processed_Data/DEEM_10k/Data'\n",
    "iza_dir = '../Processed_Data/IZA_226onDEEM_10k/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global model parameters\n",
    "# TODO: or use .get_params()?\n",
    "svc_kwargs = dict(linear=dict(penalty='l2',\n",
    "                              loss='squared_hinge',\n",
    "                              dual=False,\n",
    "                              multi_class='ovr',\n",
    "                              class_weight=None,\n",
    "                              fit_intercept=True,\n",
    "                              intercept_scaling=1.0,\n",
    "                              tol=1.0E-3),\n",
    "                  kernel=dict(kernel='precomputed',\n",
    "                              decision_function_shape='ovr',\n",
    "                              class_weight=None,\n",
    "                              break_ties=False,\n",
    "                              tol=1.0E-3))\n",
    "\n",
    "covr_args = dict(linear=dict(n_components=None, alpha=0.0, regularization=1.0E-12),\n",
    "                 kernel=dict(n_components=None, alpha=0.0, regularization=1.0E-12))\n",
    "\n",
    "C = np.logspace(-5, 5, 11)\n",
    "alphas = np.linspace(0.0, 1.0, 11)\n",
    "regularizations = np.logspace(-12, -1, 12)\n",
    "# TODO: how to best assign n_components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subsets = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DEEM into subsets for optimization and validation\n",
    "idxs_deem_train_kernel = np.arange(idxs_iza_train.size, idxs_iza_train.size+idxs_deem_train.size)\n",
    "np.random.shuffle(idxs_deem_train_kernel)\n",
    "idxs_deem_train_kernel = np.split(idxs_deem_train_kernel, n_subsets)\n",
    "\n",
    "idxs_deem_test_kernel = np.arange(idxs_iza_test.size, idxs_iza_test.size+idxs_deem_test.size)\n",
    "np.random.shuffle(idxs_deem_test_kernel)\n",
    "idxs_deem_test_kernel = np.split(idxs_deem_test_kernel, n_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split IZA into subsets for optimization and validation\n",
    "idxs_iza = np.arange(0, n_iza)\n",
    "n_iza_train_kernel = int(n_iza / 2)\n",
    "n_iza_test_kernel = n_iza - n_iza_train_kernel\n",
    "\n",
    "idxs_iza_train_kernel = []\n",
    "idxs_iza_test_kernel = []\n",
    "for n in range(0, n_subsets):\n",
    "    np.random.shuffle(idxs_iza)\n",
    "    idxs_iza_train_kernel.append(idxs_iza[0:n_iza_train_kernel])\n",
    "    idxs_iza_test_kernel.append(idxs_iza[n_iza_train_kernel:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DEEM and IZA indices\n",
    "idxs_train_kernel = [np.concatenate((iza, deem)) for iza, deem in \n",
    "                     zip(idxs_iza_train_kernel, idxs_deem_train_kernel)]\n",
    "idxs_test_kernel = [np.concatenate((iza, deem)) for iza, deem in\n",
    "                    zip(idxs_iza_test_kernel, idxs_deem_test_kernel)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IZA indices for training and testing\n",
    "idxs_iza = np.arange(0, n_iza)\n",
    "n_iza_train = int(n_iza / 2)\n",
    "n_iza_test = n_iza - n_iza_train\n",
    "\n",
    "np.random.shuffle(idxs_iza)\n",
    "idxs_iza_train_all = idxs_iza[0:n_iza_train]\n",
    "idxs_iza_test_all = idxs_iza[n_iza_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEEM indices for training and testing\n",
    "idxs_deem_train_all = np.concatenate(idxs_deem_train_kernel)\n",
    "idxs_deem_test_all = np.concatenate(idxs_deem_test_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DEEM and IZA indices\n",
    "idxs_train_all = np.concatenate((idxs_iza_train_all, idxs_deem_train_all))\n",
    "idxs_test_all = np.concatenate((idxs_iza_test_all, idxs_deem_test_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the kernels or compute if they don't exist\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{kernel_model_dir}/{kernel_type.capitalize()}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "        kernel_parameter_file = f'{kernel_model_dir}/{kerknel_type.capitalize()}/{cutoff}/volumes_mae_parameters.json'\n",
    "\n",
    "        if not os.path.exists(kernel_file):\n",
    "            \n",
    "            # SOAP files (atomwise, FPS'ed features)\n",
    "            deem_file = f'{deem_dir}/{cutoff}/soaps.hdf5'\n",
    "            iza_file = f'{iza_dir}/{cutoff}/soaps.hdf5'\n",
    "\n",
    "            # Assemble the train and test set SOAPs from IZA and DEEM\n",
    "            soaps_train, soaps_test = load_soaps(deem_file, iza_file,\n",
    "                                                 idxs_deem_train, idxs_deem_test,\n",
    "                                                 idxs_iza_train, idxs_iza_test,\n",
    "                                                 idxs_iza_delete=[RWY])\n",
    "\n",
    "            # Compute kernels\n",
    "            kernel_parameters = load_json(kernel_parameter_file)\n",
    "            K_train, K_test, K_test_test = \\\n",
    "                compute_kernels(soaps_train, soaps_test, **kernel_parameters, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize KernelSVC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{kernel_model_dir}/{kernel_type.capitalize()}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "        \n",
    "        # Load kernels\n",
    "        K_train, K_test, K_test_test = load_kernels(kernel_file)\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            # TODO: MOVE KERNELS BEFORE RUNNING!\n",
    "            \n",
    "            # Directory to put all the output data in\n",
    "            output_dir = f'KSVC-KPCovR/{n_cantons}-class/{kernel_type.capitalize()}'\n",
    "            \n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # SOAP files (atomwise, FPS'ed features)\n",
    "            deem_file = f'{deem_dir}/{cutoff}/soaps.hdf5'\n",
    "            iza_file = f'{iza_dir}/{cutoff}/soaps.hdf5'\n",
    "            \n",
    "            # File to store kernels for re-use\n",
    "            kernel_file = f'{model_dir}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC decision functions\n",
    "            ksvc_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            ksvc_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC class predictions\n",
    "            ksvc_canton_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_cantons.dat'\n",
    "            ksvc_canton_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_cantons.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR projections\n",
    "            kpcovr_projection_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structures.hdf5' \n",
    "            kpcovr_projection_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structures.hdf5'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR decision functions\n",
    "            kpcovr_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structure_dfs.dat'\n",
    "            kpcovr_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structure_dfs.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR class predictions\n",
    "            kpcovr_canton_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structure_cantons.dat'\n",
    "            kpcovr_canton_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structure_cantons.dat'\n",
    "            \n",
    "            # Files containing the hyperparameters for the kernel, KSVC, and KPCovR\n",
    "            kernel_parameter_file = f'{model_dir}/{cutoff}/volumes_mae_parameters.json'\n",
    "            ksvc_parameter_file = f'{model_dir}/{cutoff}/ksvc_parameters.json'\n",
    "            kpcovr_parameter_file = f'{model_dir}/{cutoff}/{output_dir}/kpcovr_parameters.json'\n",
    "            \n",
    "            for cdx, c in enumerate(C):\n",
    "                \n",
    "                # TODO: swap order so kernel operations aren't re-done all the time\n",
    "                for n in range(0, n_subsets):\n",
    "                    \n",
    "                    idxs_train = idxs_train_kernel[n]\n",
    "                    idxs_test = idxs_test_kernel[n]\n",
    "                    \n",
    "                    k_train = K_train[idxs_train, :][:, idxs_train]\n",
    "                    k_test = K_test[idxs_test, :][:, idxs_train]\n",
    "                    k_test_test = K_test_test[idxs_test, :][:, idxs_test]\n",
    "                    \n",
    "                    # Center and scale kernels\n",
    "                    k_train, k_test, k_test_test = \\\n",
    "                        preprocess_kernels(k_train, k_test, k_test_test)\n",
    "                    \n",
    "                    y_train = cantons_train[idxs_train]\n",
    "                    y_test = cantons_test[idxs_test]\n",
    "                    \n",
    "                    # Run KSVC\n",
    "                    ksvc_parameters = load_json(ksvc_parameter_file)\n",
    "                    \n",
    "                    # TODO: change this to return scores only here\n",
    "                    df_train, df_test, predicted_cantons_train, predicted_cantons_test = \\\n",
    "                        do_svc(k_train, k_test, y_train, y_test, \n",
    "                               svc_type='kernel', **ksvc_parameters, C=c)\n",
    "                    \n",
    "                    class_accuracy_train[cdx, n] = train_score\n",
    "                    class_accuracy_test[cdx, n] = test_score\n",
    "            \n",
    "            # TODO: use accuracy only on IZA structures?\n",
    "            class_accuracy_train_avg = np.mean(class_accuracy_train, axis=1)\n",
    "            class_accuracy_test_avg = np.mean(class_accuracy_test, axis=1)\n",
    "            \n",
    "            idx_C = np.argmax(class_accuracy_test_avg)\n",
    "            C_opt = C[idx_C]\n",
    "            ksvc_parameters.update(C=C_opt)\n",
    "            save_json(ksvc_parameter_file, ksvc_parameters)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal KernelSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{kernel_model_dir}/{kernel_type.capitalize()}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "        \n",
    "        # Load kernels\n",
    "        K_train, K_test, K_test_test = load_kernels(kernel_file)\n",
    "        \n",
    "        # TODO: how to handle the \"extra\" kernels?\n",
    "        # Since we need decision functions for all IZA structures in the train set,\n",
    "        # predict all structures based on the truncated train set below\n",
    "        k_train_all = K_train[:, idxs_train_all]\n",
    "        k_test_all = K_test[:, idxs_train_all]\n",
    "\n",
    "        k_train = k_train_all[idxs_train_all, :]\n",
    "        k_test = K_test_all[idxs_test_all, :]\n",
    "\n",
    "        k_test = center_kernel_fast(k_test, K_ref=k_train)\n",
    "        k_test_all = center_kernel_fast(k_test_all, K_ref=k_train)\n",
    "        k_train_all = center_kernel_fast(k_train_all, K_ref=k_train)\n",
    "        k_train = center_kernel_fast(k_train)\n",
    "\n",
    "        k_scale = np.trace(k_train) / k_train.shape[0]\n",
    "        k_test /= k_scale\n",
    "        k_test_all /= k_scale\n",
    "        k_train_all /= k_scale\n",
    "        k_train /= k_scale\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            # Assemble properties\n",
    "            y_train = cantons_train[idxs_train_all]\n",
    "            y_test = cantons_test[idxs_test_all]\n",
    "            \n",
    "            svc = SVC(kernel='precomputed', decision_function_shape=model_dict['df_type'], \n",
    "                      class_weight=model_dict['class_weight'], C=model_dict['C'])\n",
    "            \n",
    "            svc.fit(k_train, y_train)\n",
    "\n",
    "            print(svc.score(k_train, y_train))\n",
    "            print(svc.score(k_test, y_test))\n",
    "\n",
    "            df_train[cutoff] = svc.decision_function(k_train_all) \n",
    "            df_test[cutoff] = svc.decision_function(k_test_all)\n",
    "            \n",
    "            # TODO: save decision functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{kernel_model_dir}/{kernel_type.capitalize()}/{cutoff}/structure_kernels.hdf5'\n",
    "        \n",
    "        # Load kernels\n",
    "        K_train, K_test, K_test_test = load_kernels(kernel_file)\n",
    "        \n",
    "        # TODO: how to handle the \"extra\" kernels?\n",
    "        # Since we need decision functions for all IZA structures in the train set,\n",
    "        # predict all structures based on the truncated train set below\n",
    "        k_train_all = K_train[:, idxs_train_all]\n",
    "        k_test_all = K_test[:, idxs_train_all]\n",
    "\n",
    "        k_train = k_train_all[idxs_train_all, :]\n",
    "        k_test = K_test_all[idxs_test_all, :]\n",
    "\n",
    "        k_test = center_kernel_fast(k_test, K_ref=k_train)\n",
    "        k_test_all = center_kernel_fast(k_test_all, K_ref=k_train)\n",
    "        k_train_all = center_kernel_fast(k_train_all, K_ref=k_train)\n",
    "        k_train = center_kernel_fast(k_train)\n",
    "\n",
    "        k_scale = np.trace(k_train) / k_train.shape[0]\n",
    "        k_test /= k_scale\n",
    "        k_test_all /= k_scale\n",
    "        k_train_all /= k_scale\n",
    "        k_train /= k_scale\n",
    "\n",
    "        # Center and scale kernels\n",
    "        k_train, k_test, k_test_test = \\\n",
    "            preprocess_kernels(k_train, k_test, k_test_test)\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            # Directory to put all the output data in\n",
    "            output_dir = f'KSVC-KPCovR/{n_cantons}-class/{kernel_type.capitalize()}'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC decision functions\n",
    "            ksvc_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            ksvc_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            \n",
    "            df_train, df_test = load_data(ksvc_df_deem_file, ksvc_df_iza_file,\n",
    "                                          idxs_deem_train, idxs_deem_test,\n",
    "                                          idxs_iza_train, idxs_iza_test)\n",
    "            \n",
    "            # Center and scale the decision functions\n",
    "            df_train, df_test, df_center, df_scale = \\\n",
    "                preprocess_decision_functions(df_train, df_test)\n",
    "\n",
    "            # Check that KRR can reproduce the decision functions\n",
    "            regression_check(K_train, K_test, df_train, df_test, regression_type='kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize KPCovR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # File to store kernels for re-use\n",
    "        kernel_file = f'{kernel_model_dir}/{kernel_type.capitalize()}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "        \n",
    "        # Load kernels\n",
    "        K_train, K_test, K_test_test = load_kernels(kernel_file)\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            for adx, a in enumerate(alphas):\n",
    "                for rdx, r in enumerate(regularizations):\n",
    "                    \n",
    "                    # TODO: swap order so kernel operations aren't re-done all the time\n",
    "                    for n in range(0, n_subsets):\n",
    "                        \n",
    "                        # Assemble kernels\n",
    "                        idxs_train = idxs_train_kernel[n]\n",
    "                        k_train = K_train[cutoff][idxs_train, :][:, idxs_train]\n",
    "\n",
    "                        idxs_test = idxs_test_kernel[n]\n",
    "                        k_test = K_test[cutoff][idxs_test, :][:, idxs_train]\n",
    "                        k_test_test = K_test_test[cutoff][idxs_test, :][:, idxs_test]\n",
    "\n",
    "                        k_test_test = center_kernel_oos_fast(k_test_test, k_test, k_train)\n",
    "                        k_test = center_kernel_fast(k_test, K_ref=k_train)\n",
    "                        k_train = center_kernel_fast(k_train)\n",
    "\n",
    "                        k_scale = np.trace(k_train) / k_train.shape[0]\n",
    "                        k_test_test /= k_scale\n",
    "                        k_test /= k_scale\n",
    "                        k_train /= k_scale\n",
    "\n",
    "                        # Assemble properties\n",
    "                        y_train = df_train[cutoff][idxs_train]\n",
    "                        y_test = df_test[cutoff][idxs_test]            \n",
    "\n",
    "                        y_center = np.mean(y_train, axis=0)\n",
    "                        y_train = y_train - y_center\n",
    "                        y_test = y_test - y_center\n",
    "\n",
    "                        y_scale = np.linalg.norm(y_train, axis=0) \\\n",
    "                            / np.sqrt(y_train.shape[0] / y_train.shape[1])\n",
    "                        y_train = y_train / y_scale\n",
    "                        y_test = y_test / y_scale\n",
    "\n",
    "                        # Run KPCovR\n",
    "                        kpcovr_parameters = load_json(kpcovr_parameter_file)\n",
    "                        T_train, T_test, dfp_train, dfp_test = \\\n",
    "                            do_covr(k_train, k_test, df_train, df_test, covr_type='kernel')\n",
    "\n",
    "                        # Post process the KPCovR decision functions\n",
    "                        # (i.e., turn them back into canton predictions)\n",
    "                        predicted_cantons_train, predicted_cantons_test = \\\n",
    "                            postprocess_decision_functions(dfp_train, dfp_test, \n",
    "                                                           df_center, df_scale)\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR projections\n",
    "                        split_and_save(T_train, T_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=covr_parameters['kernel'],\n",
    "                                       output=kpcovr_projection_deem_file, output_format='%f')\n",
    "\n",
    "                        split_and_save(T_train, T_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_train),\n",
    "                                       hdf5_attrs=covr_parameters['kernel'],\n",
    "                                       output=kpcovr_projection_iza_file, output_format='%f')\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR decision functions\n",
    "                        split_and_save(dfp_train, dfp_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_df_deem_file, output_format='%f')\n",
    "\n",
    "                        split_and_save(dfp_train, dfp_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_test),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_df_iza_file, output_format='%f')\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR canton predictions\n",
    "                        split_and_save(predicted_cantons_train, predicted_cantons_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_cantons_deem_file, output_format='%d')\n",
    "\n",
    "                        split_and_save(predicted_cantons_train, predicted_cantons_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_test),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_cantons_iza_file, output_format='%d')\n",
    "\n",
    "                        # Collect regression and projection (KPCovR) losses for each canton\n",
    "                        lrs_test[adx, rdx, :] += l_regr(y_test, yp_test)\n",
    "                        lks_test[adx, rdx, :] += l_kpcovr(k_train=k_train,\n",
    "                                                    k_test=k_test,\n",
    "                                                    k_test_test=k_test_test,\n",
    "                                                    t_train=t_train, t_test=t_test)\n",
    "\n",
    "                        lrs_train[adx, rdx, :] += l_regr(y_train, yp_train)\n",
    "                        lks_train[adx, rdx, :] += l_kpcovr(k_train=k_train,\n",
    "                                                     t_train=t_train, t_test=t_test)\n",
    "\n",
    "                        # Total loss as sum of cantonwise losses\n",
    "                        ltrs_train[adx, rdx] += np.sum(lrs_train[adx, rdx, :])\n",
    "                        ltks_train[adx, rdx] += np.sum(lks_train[adx, rdx, :])\n",
    "\n",
    "                        ltrs_test[adx, rdx] += np.sum(lrs_test[adx, rdx, :])\n",
    "                        ltks_test[adx, rdx] += np.sum(lks_test[adx, rdx, :])\n",
    "\n",
    "\n",
    "                    # Average over the folds\n",
    "                    lrs_test[adx, rdx, :] /= n_subsets\n",
    "                    lks_test[adx, rdx, :] /= n_subsets\n",
    "\n",
    "                    ltrs_test[adx, rdx] /= n_subsets\n",
    "                    ltks_test[adx, rdx] /= n_subsets\n",
    "\n",
    "                    lrs_train[adx, rdx, :] /= n_subsets\n",
    "                    lks_train[adx, rdx, :] /= n_subsets\n",
    "\n",
    "                    ltrs_train[adx, rdx] /= n_subsets\n",
    "                    ltks_train[adx, rdx] /= n_subsets\n",
    "                    \n",
    "                    # TODO: save losses\n",
    "\n",
    "            # Extract optimal hyperparameters\n",
    "            opt_alpha_idx, opt_reg_idx = np.unravel_index(np.argmin(ltrs_test+ltks_test, axis=None), \n",
    "                                                          (ltrs_test+ltks_test).shape)\n",
    "\n",
    "            opt_alpha = alphas[opt_alpha_idx]\n",
    "            opt_reg = regularizations[opt_reg_idx]\n",
    "\n",
    "            print(opt_alpha, opt_reg)\n",
    "\n",
    "            # Save KPCovR parameters\n",
    "            kpcovr_parameters = dict(n_components=n_components,\n",
    "                                     regularization=opt_reg,\n",
    "                                     alpha=opt_alpha)\n",
    "\n",
    "            model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)    \n",
    "\n",
    "            with open(f'{model_dir}/kpcovr_parameters.json', 'w') as f:\n",
    "                json.dump(kpcovr_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load losses\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            fig = plt.figure(figsize=(7.0, 7.0))\n",
    "            axs_loss_train = fig.add_subplot(2, 2, 1)\n",
    "            axs_loss_sum_train = fig.add_subplot(2, 2, 2)\n",
    "            axs_loss_test = fig.add_subplot(2, 2, 3)\n",
    "            axs_loss_sum_test = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "            # Cantonwise losses for the train set\n",
    "            for i in range(0, 3):\n",
    "                axs_loss_train.semilogy(alphas, lrs_train[:, opt_reg_idx, i], 'o-', label=f'l_regr_{i+1}')\n",
    "                axs_loss_train.semilogy(alphas, lks_train[:, opt_reg_idx, i], 'o-', label=f'l_proj_{i+1}')\n",
    "                axs_loss_train.semilogy(alphas, lrs_train[:, opt_reg_idx, i]+lks_train[:, opt_reg_idx, i], \n",
    "                         'o-', label=f'l_regr+l_proj_{i+1}')\n",
    "\n",
    "            axs_loss_train.legend()\n",
    "            axs_loss_train.set_title('Train')\n",
    "            axs_loss_train.set_xlabel('alpha')\n",
    "            axs_loss_train.set_ylabel('loss')\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the train set\n",
    "            axs_loss_sum_train.semilogy(alphas, ltrs_train[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_train.semilogy(alphas, ltks_train[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_train.semilogy(alphas, ltrs_train[:, opt_reg_idx]+ltks_train[:, opt_reg_idx], \n",
    "                                    'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_train.legend()\n",
    "            axs_loss_sum_train.set_title('Train')\n",
    "            axs_loss_sum_train.set_xlabel('alpha')\n",
    "            axs_loss_sum_train.set_ylabel('loss')\n",
    "\n",
    "            # Cantonwise losses for the test set\n",
    "            for i in range(0, 3):\n",
    "                axs_loss_test.semilogy(alphas, lrs_test[:, opt_reg_idx, i], 'o-', label=f'l_regr_{i+1}')\n",
    "                axs_loss_test.semilogy(alphas, lks_test[:, opt_reg_idx, i], 'o-', label=f'l_proj_{i+1}')\n",
    "                axs_loss_test.semilogy(alphas, lrs_test[:, opt_reg_idx, i]+lks_test[:, opt_reg_idx, i], \n",
    "                         'o-', label=f'l_regr+l_proj_{i+1}')\n",
    "\n",
    "            axs_loss_test.legend()\n",
    "            axs_loss_test.set_title('Test')\n",
    "            axs_loss_test.set_xlabel('alpha')\n",
    "            axs_loss_test.set_ylabel('loss')\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the test set\n",
    "            axs_loss_sum_test.semilogy(alphas, ltrs_test[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_test.semilogy(alphas, ltks_test[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_test.semilogy(alphas, ltrs_test[:, opt_reg_idx]+ltks_test[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_test.legend()\n",
    "            axs_loss_sum_test.set_title('Test')\n",
    "            axs_loss_sum_test.set_xlabel('alpha')\n",
    "            axs_loss_sum_test.set_ylabel('loss')\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_species = 2\n",
    "n_species_pairs = n_species * (n_species + 1) // 2\n",
    "n_features = n_species_pairs * soap_hyperparameters['max_radial']**2 \\\n",
    "    * (soap_hyperparameters['max_angular'] + 1)\n",
    "feature_groups = extract_species_pair_groups(n_features, n_species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize LinearSVC parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load SOAPs\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            # TODO: MOVE KERNELS BEFORE RUNNING!\n",
    "            \n",
    "            # Directory to put all the output data in\n",
    "            output_dir = f'KSVC-KPCovR/{n_cantons}-class/{kernel_type.capitalize()}'\n",
    "            \n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            # SOAP files (atomwise, FPS'ed features)\n",
    "            deem_file = f'{deem_dir}/{cutoff}/soaps.hdf5'\n",
    "            iza_file = f'{iza_dir}/{cutoff}/soaps.hdf5'\n",
    "            \n",
    "            # File to store kernels for re-use\n",
    "            kernel_file = f'{model_dir}/{cutoff}/structure_kernels_optimization.hdf5'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC decision functions\n",
    "            ksvc_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            ksvc_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC class predictions\n",
    "            ksvc_canton_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_cantons.dat'\n",
    "            ksvc_canton_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_cantons.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR projections\n",
    "            kpcovr_projection_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structures.hdf5' \n",
    "            kpcovr_projection_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structures.hdf5'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR decision functions\n",
    "            kpcovr_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structure_dfs.dat'\n",
    "            kpcovr_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structure_dfs.dat'\n",
    "            \n",
    "            # Files to store IZA and DEEM KPCovR class predictions\n",
    "            kpcovr_canton_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/kpcovr_structure_cantons.dat'\n",
    "            kpcovr_canton_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/kpcovr_structure_cantons.dat'\n",
    "            \n",
    "            # Files containing the hyperparameters for the kernel, KSVC, and KPCovR\n",
    "            kernel_parameter_file = f'{model_dir}/{cutoff}/volumes_mae_parameters.json'\n",
    "            ksvc_parameter_file = f'{model_dir}/{cutoff}/ksvc_parameters.json'\n",
    "            kpcovr_parameter_file = f'{model_dir}/{cutoff}/{output_dir}/kpcovr_parameters.json'\n",
    "            \n",
    "            for cdx, c in enumerate(C):\n",
    "                \n",
    "                # TODO: swap order so kernel operations aren't re-done all the time\n",
    "                for n in range(0, n_subsets):\n",
    "                    \n",
    "                    idxs_train = idxs_train_kernel[n]\n",
    "                    idxs_test = idxs_test_kernel[n]\n",
    "                    \n",
    "                    # TODO: slice SOAPs\n",
    "                    \n",
    "                    y_train = cantons_train[idxs_train]\n",
    "                    y_test = cantons_test[idxs_test]\n",
    "                    \n",
    "                    # Run KSVC\n",
    "                    ksvc_parameters = load_json(ksvc_parameter_file)\n",
    "                    \n",
    "                    # TODO: change this to return scores only here\n",
    "                    df_train, df_test, predicted_cantons_train, predicted_cantons_test = \\\n",
    "                        do_svc(k_train, k_test, y_train, y_test, \n",
    "                               svc_type='kernel', **ksvc_parameters, C=c)\n",
    "                    \n",
    "                    class_accuracy_train[cdx, n] = train_score\n",
    "                    class_accuracy_test[cdx, n] = test_score\n",
    "            \n",
    "            # TODO: use accuracy only on IZA structures?\n",
    "            class_accuracy_train_avg = np.mean(class_accuracy_train, axis=1)\n",
    "            class_accuracy_test_avg = np.mean(class_accuracy_test, axis=1)\n",
    "            \n",
    "            idx_C = np.argmax(class_accuracy_test_avg)\n",
    "            C_opt = C[idx_C]\n",
    "            ksvc_parameters.update(C=C_opt)\n",
    "            save_json(ksvc_parameter_file, ksvc_parameters)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load SOAPs\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            # Assemble properties\n",
    "            y_train = cantons_train[idxs_train_all]\n",
    "            y_test = cantons_test[idxs_test_all]\n",
    "            \n",
    "            svc = SVC(kernel='precomputed', decision_function_shape=model_dict['df_type'], \n",
    "                      class_weight=model_dict['class_weight'], C=model_dict['C'])\n",
    "            \n",
    "            svc.fit(k_train, y_train)\n",
    "\n",
    "            print(svc.score(k_train, y_train))\n",
    "            print(svc.score(k_test, y_test))\n",
    "\n",
    "            df_train[cutoff] = svc.decision_function(k_train_all) \n",
    "            df_test[cutoff] = svc.decision_function(k_test_all)\n",
    "            \n",
    "            # TODO: save decision functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRR check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load SOAPs\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            # Directory to put all the output data in\n",
    "            output_dir = f'KSVC-KPCovR/{n_cantons}-class/{kernel_type.capitalize()}'\n",
    "            \n",
    "            # Files to store IZA and DEEM KSVC decision functions\n",
    "            ksvc_df_deem_file = f'{deem_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            ksvc_df_iza_file = f'{iza_dir}/{cutoff}/{output_dir}/ksvc_structure_dfs.dat'\n",
    "            \n",
    "            df_train, df_test = load_data(ksvc_df_deem_file, ksvc_df_iza_file,\n",
    "                                          idxs_deem_train, idxs_deem_test,\n",
    "                                          idxs_iza_train, idxs_iza_test)\n",
    "            \n",
    "            # Center and scale the decision functions\n",
    "            df_train, df_test, df_center, df_scale = \\\n",
    "                preprocess_decision_functions(df_train, df_test)\n",
    "\n",
    "            # Check that KRR can reproduce the decision functions\n",
    "            regression_check(K_train, K_test, df_train, df_test, regression_type='kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize PCovR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load SOAPs\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            for adx, a in enumerate(alphas):\n",
    "                for rdx, r in enumerate(regularizations):\n",
    "                    \n",
    "                    # TODO: swap order so kernel operations aren't re-done all the time\n",
    "                    for n in range(0, n_subsets):\n",
    "                        \n",
    "                        # TODO: slice SOAPs\n",
    "\n",
    "                        # Assemble properties\n",
    "                        y_train = df_train[cutoff][idxs_train]\n",
    "                        y_test = df_test[cutoff][idxs_test]            \n",
    "\n",
    "                        y_center = np.mean(y_train, axis=0)\n",
    "                        y_train = y_train - y_center\n",
    "                        y_test = y_test - y_center\n",
    "\n",
    "                        y_scale = np.linalg.norm(y_train, axis=0) \\\n",
    "                            / np.sqrt(y_train.shape[0] / y_train.shape[1])\n",
    "                        y_train = y_train / y_scale\n",
    "                        y_test = y_test / y_scale\n",
    "\n",
    "                        # Run KPCovR\n",
    "                        kpcovr_parameters = load_json(kpcovr_parameter_file)\n",
    "                        T_train, T_test, dfp_train, dfp_test = \\\n",
    "                            do_covr(k_train, k_test, df_train, df_test, covr_type='kernel')\n",
    "\n",
    "                        # Post process the KPCovR decision functions\n",
    "                        # (i.e., turn them back into canton predictions)\n",
    "                        predicted_cantons_train, predicted_cantons_test = \\\n",
    "                            postprocess_decision_functions(dfp_train, dfp_test, \n",
    "                                                           df_center, df_scale)\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR projections\n",
    "                        split_and_save(T_train, T_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=covr_parameters['kernel'],\n",
    "                                       output=kpcovr_projection_deem_file, output_format='%f')\n",
    "\n",
    "                        split_and_save(T_train, T_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_train),\n",
    "                                       hdf5_attrs=covr_parameters['kernel'],\n",
    "                                       output=kpcovr_projection_iza_file, output_format='%f')\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR decision functions\n",
    "                        split_and_save(dfp_train, dfp_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_df_deem_file, output_format='%f')\n",
    "\n",
    "                        split_and_save(dfp_train, dfp_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_test),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_df_iza_file, output_format='%f')\n",
    "\n",
    "                        # Save IZA and DEEM KPCovR canton predictions\n",
    "                        split_and_save(predicted_cantons_train, predicted_cantons_test,\n",
    "                                       idxs_deem_train, idxs_deem_test,\n",
    "                                       slice(n_iza_train, None), slice(n_iza_test, None),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_cantons_deem_file, output_format='%d')\n",
    "\n",
    "                        split_and_save(predicted_cantons_train, predicted_cantons_test,\n",
    "                                       idxs_iza_train, idxs_iza_test,\n",
    "                                       slice(0, n_iza_train), slice(0, n_iza_test),\n",
    "                                       hdf5_attrs=None, \n",
    "                                       output=kpcovr_cantons_iza_file, output_format='%d')\n",
    "\n",
    "                        # Collect regression and projection (KPCovR) losses for each canton\n",
    "                        lrs_test[adx, rdx, :] += l_regr(y_test, yp_test)\n",
    "                        lks_test[adx, rdx, :] += l_kpcovr(k_train=k_train,\n",
    "                                                    k_test=k_test,\n",
    "                                                    k_test_test=k_test_test,\n",
    "                                                    t_train=t_train, t_test=t_test)\n",
    "\n",
    "                        lrs_train[adx, rdx, :] += l_regr(y_train, yp_train)\n",
    "                        lks_train[adx, rdx, :] += l_kpcovr(k_train=k_train,\n",
    "                                                     t_train=t_train, t_test=t_test)\n",
    "\n",
    "                        # Total loss as sum of cantonwise losses\n",
    "                        ltrs_train[adx, rdx] += np.sum(lrs_train[adx, rdx, :])\n",
    "                        ltks_train[adx, rdx] += np.sum(lks_train[adx, rdx, :])\n",
    "\n",
    "                        ltrs_test[adx, rdx] += np.sum(lrs_test[adx, rdx, :])\n",
    "                        ltks_test[adx, rdx] += np.sum(lks_test[adx, rdx, :])\n",
    "\n",
    "\n",
    "                    # Average over the folds\n",
    "                    lrs_test[adx, rdx, :] /= n_subsets\n",
    "                    lks_test[adx, rdx, :] /= n_subsets\n",
    "\n",
    "                    ltrs_test[adx, rdx] /= n_subsets\n",
    "                    ltks_test[adx, rdx] /= n_subsets\n",
    "\n",
    "                    lrs_train[adx, rdx, :] /= n_subsets\n",
    "                    lks_train[adx, rdx, :] /= n_subsets\n",
    "\n",
    "                    ltrs_train[adx, rdx] /= n_subsets\n",
    "                    ltks_train[adx, rdx] /= n_subsets\n",
    "                    \n",
    "            # TODO: save losses\n",
    "\n",
    "            # Extract optimal hyperparameters\n",
    "            opt_alpha_idx, opt_reg_idx = np.unravel_index(np.argmin(ltrs_test+ltks_test, axis=None), \n",
    "                                                          (ltrs_test+ltks_test).shape)\n",
    "\n",
    "            opt_alpha = alphas[opt_alpha_idx]\n",
    "            opt_reg = regularizations[opt_reg_idx]\n",
    "\n",
    "            print(opt_alpha, opt_reg)\n",
    "\n",
    "            # Save KPCovR parameters\n",
    "            kpcovr_parameters = dict(n_components=n_components,\n",
    "                                     regularization=opt_reg,\n",
    "                                     alpha=opt_alpha)\n",
    "\n",
    "            model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)    \n",
    "\n",
    "            with open(f'{model_dir}/kpcovr_parameters.json', 'w') as f:\n",
    "                json.dump(kpcovr_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'kernel'\n",
    "for cutoff in cutoffs:\n",
    "    for kernel_type in ('linear', 'gaussian'):\n",
    "        \n",
    "        # TODO: load losses\n",
    "        \n",
    "        for n_cantons in (2, 4):\n",
    "            \n",
    "            fig = plt.figure(figsize=(7.0, 7.0))\n",
    "            axs_loss_train = fig.add_subplot(2, 2, 1)\n",
    "            axs_loss_sum_train = fig.add_subplot(2, 2, 2)\n",
    "            axs_loss_test = fig.add_subplot(2, 2, 3)\n",
    "            axs_loss_sum_test = fig.add_subplot(2, 2, 4)\n",
    "\n",
    "            # Cantonwise losses for the train set\n",
    "            for i in range(0, 3):\n",
    "                axs_loss_train.semilogy(alphas, lrs_train[:, opt_reg_idx, i], 'o-', label=f'l_regr_{i+1}')\n",
    "                axs_loss_train.semilogy(alphas, lks_train[:, opt_reg_idx, i], 'o-', label=f'l_proj_{i+1}')\n",
    "                axs_loss_train.semilogy(alphas, lrs_train[:, opt_reg_idx, i]+lks_train[:, opt_reg_idx, i], \n",
    "                         'o-', label=f'l_regr+l_proj_{i+1}')\n",
    "\n",
    "            axs_loss_train.legend()\n",
    "            axs_loss_train.set_title('Train')\n",
    "            axs_loss_train.set_xlabel('alpha')\n",
    "            axs_loss_train.set_ylabel('loss')\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the train set\n",
    "            axs_loss_sum_train.semilogy(alphas, ltrs_train[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_train.semilogy(alphas, ltks_train[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_train.semilogy(alphas, ltrs_train[:, opt_reg_idx]+ltks_train[:, opt_reg_idx], \n",
    "                                    'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_train.legend()\n",
    "            axs_loss_sum_train.set_title('Train')\n",
    "            axs_loss_sum_train.set_xlabel('alpha')\n",
    "            axs_loss_sum_train.set_ylabel('loss')\n",
    "\n",
    "            # Cantonwise losses for the test set\n",
    "            for i in range(0, 3):\n",
    "                axs_loss_test.semilogy(alphas, lrs_test[:, opt_reg_idx, i], 'o-', label=f'l_regr_{i+1}')\n",
    "                axs_loss_test.semilogy(alphas, lks_test[:, opt_reg_idx, i], 'o-', label=f'l_proj_{i+1}')\n",
    "                axs_loss_test.semilogy(alphas, lrs_test[:, opt_reg_idx, i]+lks_test[:, opt_reg_idx, i], \n",
    "                         'o-', label=f'l_regr+l_proj_{i+1}')\n",
    "\n",
    "            axs_loss_test.legend()\n",
    "            axs_loss_test.set_title('Test')\n",
    "            axs_loss_test.set_xlabel('alpha')\n",
    "            axs_loss_test.set_ylabel('loss')\n",
    "\n",
    "            # Sum of projection and regression loss over all cantons for the test set\n",
    "            axs_loss_sum_test.semilogy(alphas, ltrs_test[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "            axs_loss_sum_test.semilogy(alphas, ltks_test[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "            axs_loss_sum_test.semilogy(alphas, ltrs_test[:, opt_reg_idx]+ltks_test[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "            axs_loss_sum_test.legend()\n",
    "            axs_loss_sum_test.set_title('Test')\n",
    "            axs_loss_sum_test.set_xlabel('alpha')\n",
    "            axs_loss_sum_test.set_ylabel('loss')\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}'\n",
    "    with open(f'{model_dir}/ksvc_parameters.json', 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "        \n",
    "    if C_override is not None:\n",
    "        C = C_override\n",
    "    else:\n",
    "        C = model_dict['C']\n",
    "    \n",
    "    # TODO: if using LogisticRegression, need to optimize with LogisticRegression, but re-use C for now\n",
    "    # NOTE: no ovo option for LogisticRegression\n",
    "    # NOTE: logistic regression seems to want to predict everything as DEEM, moreso than SVM\n",
    "    logr = LogisticRegression(penalty='l2', dual=False, C=C, \n",
    "                              fit_intercept=True, solver='saga',\n",
    "                              multi_class='auto', max_iter=1000, tol=1.0E-3)\n",
    "    \n",
    "    logr.fit(soaps_train[cutoff], cantons_train)\n",
    "    \n",
    "    df_train = logr.decision_function(soaps_train[cutoff])\n",
    "    df_test = logr.decision_function(soaps_test[cutoff])\n",
    "    \n",
    "    predicted_cantons_train = logr.predict(soaps_train[cutoff])\n",
    "    predicted_cantons_test = logr.predict(soaps_test[cutoff])\n",
    "    \n",
    "    probabilities_train = logr.predict_proba(soaps_train[cutoff])\n",
    "    probabilities_test = logr.predict_proba(soaps_test[cutoff])\n",
    "    \n",
    "    # Save decision functions\n",
    "    if n_classes == 2:\n",
    "        df_deem = np.zeros(n_deem)\n",
    "        df_iza = np.zeros(n_iza)\n",
    "    else:\n",
    "        n_df = n_classes\n",
    "            \n",
    "        df_deem = np.zeros((n_deem, n_df))\n",
    "        df_iza = np.zeros((n_iza, n_df))\n",
    "    \n",
    "    df_deem[idxs_deem_train] = df_train[n_iza_train:]\n",
    "    df_deem[idxs_deem_test] = df_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_deem)\n",
    "    \n",
    "    df_iza[idxs_iza_train] = df_train[0:n_iza_train]\n",
    "    df_iza[idxs_iza_test] = df_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_dfs.dat', df_iza)\n",
    "    \n",
    "    # Save LogR class predictions\n",
    "    predicted_cantons_deem = np.zeros(n_deem)\n",
    "    predicted_cantons_deem[idxs_deem_train] = predicted_cantons_train[n_iza_train:]\n",
    "    predicted_cantons_deem[idxs_deem_test] = predicted_cantons_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_cantons.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    predicted_cantons_iza = np.zeros(n_iza)\n",
    "    predicted_cantons_iza[idxs_iza_train] = predicted_cantons_train[0:n_iza_train]\n",
    "    predicted_cantons_iza[idxs_iza_test] = predicted_cantons_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_cantons.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "    # Save LogR probabilities\n",
    "    probabilities_deem = np.zeros((n_deem, n_classes))\n",
    "    probabilities_deem[idxs_deem_train] = probabilities_train[n_iza_train:]\n",
    "    probabilities_deem[idxs_deem_test] = probabilities_test[n_iza_test:]\n",
    "    np.savetxt(f'../Processed_Data/DEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat',\n",
    "               predicted_cantons_deem, fmt='%d')\n",
    "    \n",
    "    probabilities_iza = np.zeros((n_iza, n_classes))\n",
    "    probabilities_iza[idxs_iza_train] = probabilities_train[0:n_iza_train]\n",
    "    probabilities_iza[idxs_iza_test] = probabilities_test[0:n_iza_test]\n",
    "    np.savetxt(f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/logr_structure_probabilities.dat', \n",
    "               predicted_cantons_iza, fmt='%d')\n",
    "    \n",
    "#########\n",
    "    print(logr.score(soaps_train[cutoff], cantons_train))\n",
    "    print(logr.score(soaps_test[cutoff], cantons_test))\n",
    "    print(logr.coef_)\n",
    "    w = reshape_soaps(logr.coef_, 3, 12, 9)\n",
    "    density = compute_soap_density(12, 9, cutoff, w,\n",
    "                                   np.linspace(0, cutoff, 50),\n",
    "                                   np.linspace(-1, 1, 50),\n",
    "                                   chunk_size_r=10, chunk_size_p=10)\n",
    "#########"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logr.n_iter_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(probabilities_deem)\n",
    "print(probabilities_iza)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem, axis=1))\n",
    "print(np.amax(probabilities_iza, axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(np.amax(probabilities_deem))\n",
    "print(np.amax(probabilities_iza))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.hist(probabilities_deem[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.hist(probabilities_iza[:, 3], bins=50, density=True, alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
