{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from soap import extract_species_pair_groups\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from tqdm.notebook import tqdm\n",
    "import project_utils as utils\n",
    "from tools import load_json, save_json\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')   \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7750 2250\n"
     ]
    }
   ],
   "source": [
    "# Load train and test set indices for Deem\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/train.idxs', dtype=int)\n",
    "deem_test_idxs = np.loadtxt('../Processed_Data/DEEM_330k/test.idxs', dtype=int)\n",
    "n_deem = len(deem_train_idxs) + len(deem_test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test set indices for IZA\n",
    "iza_train_idxs = np.loadtxt('../Processed_Data/IZA_226/train.idxs', dtype=int)\n",
    "iza_test_idxs = np.loadtxt('../Processed_Data/IZA_226/test.idxs', dtype=int)\n",
    "n_iza = len(iza_train_idxs) + len(iza_test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "iza_cantons = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM cantons\n",
    "deem_cantons_2 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_2-class.dat', dtype=int)\n",
    "deem_cantons_4 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_4-class.dat', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons = {}\n",
    "\n",
    "cantons[4] = np.concatenate((\n",
    "    iza_cantons, \n",
    "    deem_cantons_4\n",
    "))\n",
    "\n",
    "cantons[2] = np.concatenate((\n",
    "    np.ones(n_iza, dtype=int),\n",
    "    deem_cantons_2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy cantons for Deem\n",
    "dummy_cantons = {}\n",
    "dummy_cantons[2] = np.loadtxt('../Processed_Data/DEEM_330k/dummy_cantons_2-class.dat', dtype=int)\n",
    "dummy_cantons[4] = np.loadtxt('../Processed_Data/DEEM_330k/dummy_cantons_4-class.dat', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate IZA and Deem indices\n",
    "train_idxs = np.concatenate((iza_train_idxs, deem_train_idxs + n_iza))\n",
    "test_idxs = np.concatenate((iza_test_idxs, deem_test_idxs + n_iza))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_226'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {}\n",
    "class_names[2] = ['IZA', 'DEEM']\n",
    "class_names[4] = ['IZA1', 'IZA2', 'IZA3', 'DEEM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model setup\n",
    "n_species = 2\n",
    "group_names = {'power': ['OO', 'OSi', 'SiSi', \n",
    "                         'OO+OSi', 'OO+SiSi', 'OSi+SiSi',\n",
    "                         'OO+OSi+SiSi'], \n",
    "               'radial': ['O', 'Si', 'O+Si']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ee506a5b254569ac8bc2e1cf9eb55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cutoff', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Spectrum', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Species', max=7.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Classes', max=2.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    linear_dir = f'{model_dir}/{cutoff}/Linear_Models/SVC'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_soaps = utils.load_hdf5(iza_file)\n",
    "        \n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        f = h5py.File(deem_file, 'r')\n",
    "        deem_330k_dataset = f['0']\n",
    "        \n",
    "        soaps = np.vstack((iza_soaps[iza_train_idxs], deem_330k_dataset[deem_train_idxs]))\n",
    "\n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type,\n",
    "                                                     combinations=True)\n",
    "        \n",
    "        # TODO: set batches (like PCovR) if the pipeline isn't computationally feasible\n",
    "\n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):\n",
    "            \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "                \n",
    "                # Prepare outputs\n",
    "                output_dir = f'Linear_Models/SVC/{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                \n",
    "                os.makedirs(f'{deem_dir}/{cutoff}/{output_dir}', exist_ok=True)\n",
    "                os.makedirs(f'{iza_dir}/{cutoff}/{output_dir}', exist_ok=True)\n",
    "                \n",
    "                parameter_dir = f'{linear_dir}/{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                \n",
    "                # TODO: manually set the class weights?\n",
    "                svc_parameters = load_json(f'{parameter_dir}/svc_parameters_accuracy.json')\n",
    "                dummy_svc_parameters = load_json(f'{parameter_dir}/dummy_svc_parameters_accuracy.json')\n",
    "                \n",
    "                # IZA+DEEM classification\n",
    "                cache_dir = mkdtemp()\n",
    "                pipeline = Pipeline(\n",
    "                    [\n",
    "                        ('norm_scaler', utils.NormScaler(with_mean=False)),\n",
    "                        ('svc', LinearSVC(**svc_parameters))\n",
    "                    ],\n",
    "                    memory=cache_dir\n",
    "                )\n",
    "                \n",
    "                pipeline.fit(soaps[:, feature_idxs], cantons[n_cantons])\n",
    "                \n",
    "                # Read the IZA structures and compute decision functions\n",
    "                # and canton predictions\n",
    "                iza_dfs = pipeline.decision_function(iza_soaps[:, feature_idxs])\n",
    "                iza_predicted_cantons = pipeline.predict(iza_soaps[:, feature_idxs])\n",
    "                \n",
    "                np.savetxt(f'{iza_dir}/{cutoff}/{output_dir}/svc_structure_dfs.dat', iza_dfs)\n",
    "                np.savetxt(f'{iza_dir}/{cutoff}/{output_dir}/svc_structure_cantons.dat', iza_predicted_cantons)\n",
    "                \n",
    "                # Read the DEEM structures and compute decision functions\n",
    "                # and canton predictions\n",
    "                \n",
    "                # TODO: make sure this is computationally feasible, otherwise batches\n",
    "                # We could load up the whole dataset beforehand and probably make this faster\n",
    "                # since we wouldn't have to read the HDF5 for both the DFs and predictions,\n",
    "                # but the single-read seems to be memory-prohibitive and this \n",
    "                # multiple-read construction appears tolerably fast\n",
    "                # and also seems to use much less memory\n",
    "                deem_dfs = pipeline.decision_function(deem_330k_dataset[:, feature_idxs])\n",
    "                deem_predicted_cantons = pipeline.predict(deem_330k_dataset[:, feature_idxs])\n",
    "                \n",
    "                np.savetxt(f'{deem_dir}/{cutoff}/{output_dir}/svc_structure_dfs.dat', deem_dfs)\n",
    "                np.savetxt(f'{deem_dir}/{cutoff}/{output_dir}/svc_structure_cantons.dat', deem_predicted_cantons)\n",
    "                \n",
    "                # Save the SVC model and the scaler\n",
    "                for key, model in pipeline.named_steps:\n",
    "                    save_json(model.__dict__, f'{parameter_dir}/{key}.json', array_convert=True)\n",
    "                \n",
    "                rmtree(cache_dir)\n",
    "                \n",
    "                # Dummy DEEM classification\n",
    "                dummy_cache_dir = mkdtemp()\n",
    "                dummy_pipeline = Pipeline(\n",
    "                    [\n",
    "                        ('norm_scaler', utils.NormScaler(with_mean=False)),\n",
    "                        ('svc', LinearSVC(**dummy_svc_parameters))\n",
    "                    ],\n",
    "                    memory=dummy_cache_dir\n",
    "                )\n",
    "                dummy_pipeline.fit(deem_330k_dataset[train_idxs, feature_idxs], dummy_cantons[n_cantons])\n",
    "                # Could also just choose a subset of the DEEM 330k as test\n",
    "                dummy_deem_dfs = dummy_pipeline.decision_function(deem_330k_dataset[train_idxs, feature_idxs])\n",
    "                dummy_deem_predicted_cantons = dummy_pipeline.predict(deem_330k_dataset[train_idxs, feature_idxs])\n",
    "                \n",
    "                np.savetxt(f'{deem_dir}/{cutoff}/{output_dir}/dummy_svc_structure_dfs.dat', dummy_dfs)\n",
    "                np.savetxt(f'{deem_dir}/{cutoff}/{output_dir}/dummy_svc_structure_cantons.dat', dummy_predicted_cantons)\n",
    "                \n",
    "                # Save the dummy SVC model and the scaler\n",
    "                for key, model in pipeline.named_steps:\n",
    "                    save_json(model.__dict__, f'{parameter_dir}/dummy_{key}.json', array_convert=True)\n",
    "                \n",
    "                rmtree(dummy_cache_dir)\n",
    "                \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/SVC'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_soaps = utils.load_hdf5(iza_file)\n",
    "        \n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        f = h5py.File(deem_file, 'r')\n",
    "        deem_330k_dataset = f['0']\n",
    "        \n",
    "        soaps = np.vstack((iza_soaps[iza_train_idxs], deem_330k_dataset[deem_train_idxs]))\n",
    "        \n",
    "        n_features = soaps.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type,\n",
    "                                                     combinations=True)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):\n",
    "                        \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):              \n",
    "                \n",
    "                # Load decision functions\n",
    "                input_dir = f'{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                \n",
    "                # Load decision functions\n",
    "                iza_dfs = np.loadtxt(f'{iza_dir}/{cutoff}/{df_dir}/svc_structure_dfs.dat')\n",
    "                \n",
    "                deem_dfs = np.loadtxt(f'{deem_dir}/{cutoff}/{df_dir}/svc_structure_dfs.dat')\n",
    "                \n",
    "                dfs = np.concatenate((iza_dfs[iza_train_idxs], deem_dfs[deem_train_idxs]))\n",
    "                                \n",
    "                cache_dir = mkdtemp()\n",
    "                pipeline = Pipeline(\n",
    "                    [\n",
    "                        ('norm_scaler', utils.NormScaler()),\n",
    "                        ('ridge', TransformedTargetRegressor(\n",
    "                            regressor=Ridge(alpha=1.0E-12),\n",
    "                            transformer=utils.NormScaler(featurewise=True)\n",
    "                        ))\n",
    "                    ],\n",
    "                    memory=cache_dir\n",
    "                )\n",
    "                pipeline.fit(soaps[train_idxs], dfs[train_idxs])\n",
    "                # TODO: how to handle the train and test set deem?\n",
    "                predicted_dfs = pipeline.transform(soaps[test_idxs], dfs[test_idxs])\n",
    "                # TODO: score the results (MAE, RMSE)\n",
    "                rmtree(cache_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
