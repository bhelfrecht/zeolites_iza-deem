{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "from decomposition import IterativeSparseKPCA, KPCA\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "train_idxs = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "test_idxs = np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_structures = train_idxs.size + test_idxs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set structure labels for loading from the HDF5 file\n",
    "n_digits = len(str(n_structures - 1))\n",
    "datasets = [str(i).zfill(n_digits) for i in train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "representative_soaps = {}\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    n_Si = np.loadtxt(f'{work_dir}/n_Si.dat', dtype=int)\n",
    "    split_idxs = np.cumsum(n_Si)[0:-1]\n",
    "    representative_idxs = np.loadtxt(f'{work_dir}/FPS_representatives.idxs', dtype=int)\n",
    "    soaps_file = f'{work_dir}/soaps.hdf5'\n",
    "    representative_soaps[f'{cutoff}'] = build_representatives_from_hdf5(soaps_file, representative_idxs, split_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kpca = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_iskpca_oos(cutoff, datasets,\n",
    "                         soaps_file, ref_soaps_file, \n",
    "                         rep_idxs_file, model_file,\n",
    "                         iskpca_env_file, iskpca_struct_file, \n",
    "                         work_dir='.'):\n",
    "    \n",
    "    # Read OOS SOAPs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "    n_structures = len(soaps)\n",
    "    n_digits = len(str(n_structures - 1))\n",
    "    \n",
    "    # Load reference SOAPs\n",
    "    ref_soaps = load_structures_from_hdf5(ref_soaps_file, datasets=datasets, concatenate=True)\n",
    " \n",
    "    # Build representative SOAPs from training set (indices are relative to the training set)\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = ref_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Unpickle the kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "    \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Unpickle the reference models\n",
    "    with open(iskpca_env_file, 'r') as f:\n",
    "        iskpca_environments_dict = json.load(f)        \n",
    "    \n",
    "    with open(iskpca_struct_file, 'r') as f:\n",
    "        iskpca_structures_dict = json.load(f)\n",
    "    \n",
    "    # Turn lists into arrays\n",
    "    for k, v in iskpca_environments_dict.items():\n",
    "        if isinstance(v, list):\n",
    "            iskpca_environments_dict[k] = np.array(v)\n",
    "        \n",
    "    for k, v in iskpca_structures_dict.items():\n",
    "        if isinstance(v, list):\n",
    "            iskpca_structures_dict[k] = np.array(v)\n",
    "    \n",
    "    # Set model attributes\n",
    "    iskpca_environments = SparseKPCA()\n",
    "    iskpca_environments.__dict__ = iskpca_environments_dict\n",
    "       \n",
    "    iskpca_structures = SparseKPCA()\n",
    "    iskpca_structures.__dict__ = iskpca_structures_dict\n",
    "\n",
    "    # Initialize the KPCA output\n",
    "    g = h5py.File(f'{work_dir}/kpca_environments.hdf5', 'w')\n",
    "    h = h5py.File(f'{work_dir}/kpca_structures.hdf5', 'w')\n",
    "    \n",
    "    # Save kernel parameters to the HDF5 files\n",
    "    for file_obj in (g, h):\n",
    "        file_obj.attrs['kernel_type'] = kernel_type\n",
    "        file_obj.attrs['gamma'] = gamma        \n",
    "    \n",
    "    # Transform the data and save\n",
    "    for sdx, soap in enumerate(tqdm(soaps)):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        kpcai_environments = iskpca_environments.transform(KNMi)\n",
    "        kpcai_structures = iskpca_structures.transform(np.mean(KNMi, axis=0))\n",
    "        g.create_dataset(str(sdx).zfill(n_digits), data=kpcai_environments)\n",
    "        h.create_dataset(str(sdx).zfill(n_digits), data=kpcai_structures)\n",
    "        \n",
    "    g.close()\n",
    "    h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb882bee3c8140139de00be5c925ecd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d35678da974d0d83c09e47f0e060c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cba267a3944ea6a39e8fd10ee8936e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1db8bd2cf7b4496bbc7c5fa9effd7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_10k/Models/{cutoff}'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "    \n",
    "    soaps_file = f'{work_dir}/soaps.hdf5'\n",
    "    rep_idxs_file = f'{work_dir}/FPS_representatives.idxs'\n",
    "    model_file = f'{data_dir}/volumes_mae_parameters.json'\n",
    "    iskpca_env_file = f'{data_dir}/iskpca_environments.json'\n",
    "    iskpca_struct_file = f'{data_dir}/iskpca_structures.json'\n",
    "        \n",
    "    # Initialize SOAPs and KPCAs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "\n",
    "    # Build representative SOAPs from training set (indices are relative to the training set)\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = np.vstack([soaps[i] for i in train_idxs])\n",
    "    representative_soaps = representative_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Load kernel parameters (use kernel optimized for volume as benchmark)\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "    \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Build representative kernel\n",
    "    KMM = build_kernel(representative_soaps, representative_soaps,\n",
    "                       kernel=kernel_type, gamma=gamma)\n",
    "    \n",
    "    # Initialize sparse KPCA for environments\n",
    "    iskpca_environments = IterativeSparseKPCA(n_kpca=n_kpca)\n",
    "    iskpca_environments.initialize_fit(KMM)\n",
    "    \n",
    "    # Initialize sparse KPCA for structures\n",
    "    iskpca_structures = IterativeSparseKPCA(n_kpca=n_kpca)\n",
    "    iskpca_structures.initialize_fit(KMM)\n",
    "    \n",
    "    # Fit the sparse KPCA\n",
    "    for soap in tqdm(soaps):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        iskpca_environments.fit_batch(KNMi)\n",
    "        iskpca_structures.fit_batch(np.mean(KNMi, axis=0))\n",
    "        \n",
    "    # Finalize the KPCA fitting\n",
    "    iskpca_environments.finalize_fit()\n",
    "    iskpca_structures.finalize_fit()\n",
    "    \n",
    "    # Prepare the models for saving on disk\n",
    "    # Copy the dict so we can make the numpy arrays lists\n",
    "    iskpca_environments_dict = iskpca_environments.__dict__.copy()\n",
    "    iskpca_structures_dict = iskpca_structures.__dict__.copy()\n",
    "\n",
    "    # Convert arrays to lists\n",
    "    for k, v in iskpca_environments_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            iskpca_environments_dict[k] = v.tolist()\n",
    "            \n",
    "    for k, v in iskpca_structures_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            iskpca_structures_dict[k] = v.tolist()\n",
    "    \n",
    "    # Pickle the environment-based model\n",
    "    with open(iskpca_env_file, 'w') as f:\n",
    "        json.dump(iskpca_environments_dict, f)\n",
    "    \n",
    "    # Pickle the structure-based model\n",
    "    with open(iskpca_struct_file, 'w') as f:\n",
    "        json.dump(iskpca_structures_dict, f)\n",
    "    \n",
    "    # Initialize the KPCA output\n",
    "    g = h5py.File(f'{work_dir}/kpca_environments.hdf5', 'w')\n",
    "    h = h5py.File(f'{work_dir}/kpca_structures.hdf5', 'w')\n",
    "    \n",
    "    # Save kernel parameters to HDF5 files\n",
    "    for file_obj in (g, h):\n",
    "        file_obj.attrs['kernel_type'] = kernel_type\n",
    "        file_obj.attrs['gamma'] = gamma\n",
    "    \n",
    "    # Transform the data and save\n",
    "    for sdx, soap in enumerate(tqdm(soaps)):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        kpcai_environments = iskpca_environments.transform(KNMi)\n",
    "        kpcai_structures = iskpca_structures.transform(np.mean(KNMi, axis=0))\n",
    "        g.create_dataset(str(sdx).zfill(n_digits), data=kpcai_environments)\n",
    "        h.create_dataset(str(sdx).zfill(n_digits), data=kpcai_structures)\n",
    "        \n",
    "    g.close()\n",
    "    h.close()\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA_226 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set the working directory\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    # Set the required files\n",
    "    soaps_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    ref_soaps_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    rep_idxs_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_representatives.idxs'\n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    iskpca_env_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_environments.json'\n",
    "    iskpca_struct_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_structures.json'\n",
    "       \n",
    "    # Compute the SKPCA\n",
    "    transform_iskpca_oos(cutoff, datasets,\n",
    "                         soaps_file, ref_soaps_file, \n",
    "                         rep_idxs_file, model_file,\n",
    "                         iskpca_env_file, iskpca_struct_file, \n",
    "                         work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD_196 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
