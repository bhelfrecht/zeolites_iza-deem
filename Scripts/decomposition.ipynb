{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "from decomposition import IterativeSparseKPCA\n",
    "from kernels import build_kernel, linear_kernel, gaussian_kernel\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from project_utils import load_structures_from_hdf5\n",
    "from selection import FPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "train_idxs = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "test_idxs = np.loadtxt('../Processed_Data/DEEM_10k/test.idxs', dtype=int)\n",
    "\n",
    "# Total number of structures\n",
    "n_structures = train_idxs.size + test_idxs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set structure labels for loading from the HDF5 file\n",
    "n_digits = len(str(n_structures - 1))\n",
    "datasets = [str(i).zfill(n_digits) for i in train_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "representative_soaps = {}\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    n_Si = np.loadtxt(f'{work_dir}/n_Si.dat', dtype=int)\n",
    "    split_idxs = np.cumsum(n_Si)[0:-1]\n",
    "    representative_idxs = np.loadtxt(f'{work_dir}/FPS_representatives.idxs', dtype=int)\n",
    "    soaps_file = f'{work_dir}/soaps.hdf5'\n",
    "    representative_soaps[f'{cutoff}'] = build_representatives_from_hdf5(soaps_file, representative_idxs, split_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kpca = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_iskpca_oos(cutoff, datasets,\n",
    "                         soaps_file, ref_soaps_file, \n",
    "                         rep_idxs_file, model_file,\n",
    "                         iskpca_env_file, iskpca_struct_file, \n",
    "                         work_dir='.'):\n",
    "    \n",
    "    # Read OOS SOAPs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "    n_structures = len(soaps)\n",
    "    n_digits = len(str(n_structures - 1))\n",
    "    \n",
    "    # Load reference SOAPs\n",
    "    representative_soaps = load_structures_from_hdf5(ref_soaps_file, datasets=datasets, concatenate=True)\n",
    " \n",
    "    # Build representative SOAPs from training set (indices are relative to the training set)\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = representative_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Unpickle the kernel parameters\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "    \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Unpickle the reference models\n",
    "    with open(iskpca_env_file, 'r') as f:\n",
    "        iskpca_environments_dict = json.load(f)        \n",
    "    \n",
    "    with open(iskpca_struct_file, 'r') as f:\n",
    "        iskpca_structures_dict = json.load(f)\n",
    "    \n",
    "    # Turn lists into arrays\n",
    "    for k, v in iskpca_environments_dict.items():\n",
    "        if isinstance(v, list):\n",
    "            iskpca_environments_dict[k] = np.array(v)\n",
    "        \n",
    "    for k, v in iskpca_structures_dict.items():\n",
    "        if isinstance(v, list):\n",
    "            iskpca_structures_dict[k] = np.array(v)\n",
    "    \n",
    "    # Set model attributes\n",
    "    iskpca_environments = IterativeSparseKPCA()\n",
    "    iskpca_environments.__dict__ = iskpca_environments_dict\n",
    "       \n",
    "    iskpca_structures = IterativeSparseKPCA()\n",
    "    iskpca_structures.__dict__ = iskpca_structures_dict\n",
    "\n",
    "    # Initialize the KPCA output\n",
    "    g = h5py.File(f'{work_dir}/kpca_environments.hdf5', 'w')\n",
    "    h = h5py.File(f'{work_dir}/kpca_structures.hdf5', 'w')\n",
    "    \n",
    "    # Save kernel parameters to the HDF5 files\n",
    "    for file_obj in (g, h):\n",
    "        file_obj.attrs['kernel_type'] = kernel_type\n",
    "        file_obj.attrs['gamma'] = gamma        \n",
    "    \n",
    "    # Transform the data and save\n",
    "    for sdx, soap in enumerate(tqdm(soaps)):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        kpcai_environments = iskpca_environments.transform(KNMi)\n",
    "        kpcai_structures = iskpca_structures.transform(np.mean(KNMi, axis=0))\n",
    "        g.create_dataset(str(sdx).zfill(n_digits), data=kpcai_environments)\n",
    "        h.create_dataset(str(sdx).zfill(n_digits), data=kpcai_structures)\n",
    "        \n",
    "    g.close()\n",
    "    h.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_10k/Models/{cutoff}'\n",
    "    \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "    \n",
    "    soaps_file = f'{work_dir}/soaps.hdf5'\n",
    "    rep_idxs_file = f'{work_dir}/FPS_representatives.idxs'\n",
    "    model_file = f'{data_dir}/volumes_mae_parameters.json'\n",
    "    iskpca_env_file = f'{data_dir}/iskpca_environments.json'\n",
    "    iskpca_struct_file = f'{data_dir}/iskpca_structures.json'\n",
    "        \n",
    "    # Initialize SOAPs and KPCAs\n",
    "    soaps = load_structures_from_hdf5(soaps_file, datasets=None, concatenate=False)\n",
    "\n",
    "    # Build representative SOAPs from training set (indices are relative to the training set)\n",
    "    representative_idxs = np.loadtxt(rep_idxs_file, usecols=0, dtype=int)\n",
    "    representative_soaps = np.vstack([soaps[i] for i in train_idxs])\n",
    "    representative_soaps = representative_soaps[representative_idxs, :]\n",
    "    \n",
    "    # Load kernel parameters (use kernel optimized for volume as benchmark)\n",
    "    with open(model_file, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "    \n",
    "    kernel_type = model_dict['kernel_type']\n",
    "    gamma = model_dict['gamma']\n",
    "    \n",
    "    # Build representative kernel\n",
    "    KMM = build_kernel(representative_soaps, representative_soaps,\n",
    "                       kernel=kernel_type, gamma=gamma)\n",
    "    \n",
    "    # Initialize sparse KPCA for environments\n",
    "    iskpca_environments = IterativeSparseKPCA(n_kpca=n_kpca)\n",
    "    iskpca_environments.initialize_fit(KMM)\n",
    "    \n",
    "    # Initialize sparse KPCA for structures\n",
    "    iskpca_structures = IterativeSparseKPCA(n_kpca=n_kpca)\n",
    "    iskpca_structures.initialize_fit(KMM)\n",
    "    \n",
    "    # Fit the sparse KPCA\n",
    "    for soap in tqdm(soaps):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        iskpca_environments.fit_batch(KNMi)\n",
    "        iskpca_structures.fit_batch(np.mean(KNMi, axis=0))\n",
    "        \n",
    "    # Finalize the KPCA fitting\n",
    "    iskpca_environments.finalize_fit()\n",
    "    iskpca_structures.finalize_fit()\n",
    "    \n",
    "    # Prepare the models for saving on disk\n",
    "    # Copy the dict so we can make the numpy arrays lists\n",
    "    iskpca_environments_dict = iskpca_environments.__dict__.copy()\n",
    "    iskpca_structures_dict = iskpca_structures.__dict__.copy()\n",
    "\n",
    "    # Convert arrays to lists\n",
    "    for k, v in iskpca_environments_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            iskpca_environments_dict[k] = v.tolist()\n",
    "            \n",
    "    for k, v in iskpca_structures_dict.items():\n",
    "        if isinstance(v, np.ndarray):\n",
    "            iskpca_structures_dict[k] = v.tolist()\n",
    "    \n",
    "    # Pickle the environment-based model\n",
    "    with open(iskpca_env_file, 'w') as f:\n",
    "        json.dump(iskpca_environments_dict, f)\n",
    "    \n",
    "    # Pickle the structure-based model\n",
    "    with open(iskpca_struct_file, 'w') as f:\n",
    "        json.dump(iskpca_structures_dict, f)\n",
    "    \n",
    "    # Initialize the KPCA output\n",
    "    g = h5py.File(f'{work_dir}/kpca_environments.hdf5', 'w')\n",
    "    h = h5py.File(f'{work_dir}/kpca_structures.hdf5', 'w')\n",
    "    \n",
    "    # Save kernel parameters to HDF5 files\n",
    "    for file_obj in (g, h):\n",
    "        file_obj.attrs['kernel_type'] = kernel_type\n",
    "        file_obj.attrs['gamma'] = gamma\n",
    "    \n",
    "    # Transform the data and save\n",
    "    for sdx, soap in enumerate(tqdm(soaps)):\n",
    "        KNMi = build_kernel(soap, representative_soaps,\n",
    "                            kernel=kernel_type, gamma=gamma)\n",
    "        kpcai_environments = iskpca_environments.transform(KNMi)\n",
    "        kpcai_structures = iskpca_structures.transform(np.mean(KNMi, axis=0))\n",
    "        g.create_dataset(str(sdx).zfill(n_digits), data=kpcai_environments)\n",
    "        h.create_dataset(str(sdx).zfill(n_digits), data=kpcai_structures)\n",
    "        \n",
    "    g.close()\n",
    "    h.close()\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 6528/74999 [02:48<29:16, 38.99it/s]"
     ]
    }
   ],
   "source": [
    "# Build unique environments\n",
    "max_unique = 75000 # \"Safety\" measure\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    kpcas = load_structures_from_hdf5(f'{work_dir}/kpca_environments.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS (indices are relative to the whole dataset)\n",
    "    unique, distances = FPS(kpcas, n=max_unique)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique_kpca.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete the SOAPs so we aren't carrying them around\n",
    "    del kpcas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA_226 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set the working directory\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    # Set the required files\n",
    "    soaps_file = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    ref_soaps_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    rep_idxs_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_representatives.idxs'\n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    iskpca_env_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_environments.json'\n",
    "    iskpca_struct_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_structures.json'\n",
    "       \n",
    "    # Compute the SKPCA\n",
    "    transform_iskpca_oos(cutoff, datasets,\n",
    "                         soaps_file, ref_soaps_file, \n",
    "                         rep_idxs_file, model_file,\n",
    "                         iskpca_env_file, iskpca_struct_file, \n",
    "                         work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unique environments\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    kpcas = load_structures_from_hdf5(f'{work_dir}/kpca_environments.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS\n",
    "    unique, distances = FPS(kpcas, n=-1)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique_kpca.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del kpcas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD_196 on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Set the working directory\n",
    "    work_dir = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}'\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.mkdir(work_dir)\n",
    "        \n",
    "    # Set the required files\n",
    "    soaps_file = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    ref_soaps_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/soaps.hdf5'\n",
    "    rep_idxs_file = f'../Processed_Data/DEEM_10k/Data/{cutoff}/FPS_representatives.idxs'\n",
    "    model_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/volumes_mae_parameters.json'\n",
    "    iskpca_env_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_environments.json'\n",
    "    iskpca_struct_file = f'../Processed_Data/DEEM_10k/Models/{cutoff}/iskpca_structures.json'\n",
    "       \n",
    "    # Compute the SKPCA\n",
    "    transform_iskpca_oos(cutoff, datasets,\n",
    "                         soaps_file, ref_soaps_file, \n",
    "                         rep_idxs_file, model_file,\n",
    "                         iskpca_env_file, iskpca_struct_file, \n",
    "                         work_dir=work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build unique environments\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    kpcas = load_structures_from_hdf5(f'{work_dir}/kpca_environments.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS\n",
    "    unique, distances = FPS(kpcas, n=-1)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique_kpca.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del kpcas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_kpca = load_structures_from_hdf5('../Processed_Data/DEEM_10k/Data/3.5/kpca_environments.hdf5',\n",
    "                                      datasets=None, concatenate=True)\n",
    "iza_kpca = load_structures_from_hdf5('../Processed_Data/IZA_226onDEEM_10k/Data/3.5/kpca_environments.hdf5',\n",
    "                                     datasets=None, concatenate=True)\n",
    "cod_kpca = load_structures_from_hdf5('../Processed_Data/COD_196onDEEM_10k/Data/3.5/kpca_environments.hdf5',\n",
    "                                     datasets=None, concatenate=True)\n",
    "cantons = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for environments\n",
    "n_Si_iza = np.loadtxt('../Processed_Data/IZA_226/n_Si.dat', dtype=int)\n",
    "cantons = np.repeat(cantons, n_Si_iza)\n",
    "\n",
    "c1_idxs = np.nonzero(cantons == 1)[0]\n",
    "c2_idxs = np.nonzero(cantons == 2)[0]\n",
    "c3_idxs = np.nonzero(cantons == 3)[0]\n",
    "c4_idxs = np.nonzero(cantons == 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for structures\n",
    "c1_idxs = np.nonzero(cantons == 1)[0]\n",
    "c2_idxs = np.nonzero(cantons == 2)[0]\n",
    "c3_idxs = np.nonzero(cantons == 3)[0]\n",
    "c4_idxs = np.nonzero(cantons == 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(deem_kpca[::500, 0], deem_kpca[::500, 1], c='gray', alpha=0.1)\n",
    "#plt.scatter(iza_kpca[:, 0], iza_kpca[:, 1], c='red', alpha=0.1)\n",
    "plt.scatter(iza_kpca[c1_idxs, 0], iza_kpca[c1_idxs, 1], c='red', alpha=0.5)\n",
    "plt.scatter(iza_kpca[c2_idxs, 0], iza_kpca[c2_idxs, 1], c='blue', alpha=0.5)\n",
    "plt.scatter(iza_kpca[c3_idxs, 0], iza_kpca[c3_idxs, 1], c='green', alpha=0.5)\n",
    "plt.scatter(iza_kpca[c4_idxs, 0], iza_kpca[c4_idxs, 1], c='black', alpha=0.5)\n",
    "#plt.scatter(cod_kpca[:, 0], cod_kpca[:, 1], c='blue', alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "with open('../Processed_Data/DEEM_10k/Models/3.5/energies_mae_parameters.json', 'r') as f:\n",
    "    kernel_params = json.load(f)\n",
    "    \n",
    "print(kernel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_soaps = load_structures_from_hdf5('../Processed_Data/DEEM_10k/Data/3.5/soaps.hdf5',\n",
    "                                       datasets=None, concatenate=True)\n",
    "iza_soaps = load_structures_from_hdf5('../Processed_Data/IZA_226onDEEM_10k/Data/3.5/soaps.hdf5',\n",
    "                                       datasets=None, concatenate=True)\n",
    "cod_soaps = load_structures_from_hdf5('../Processed_Data/COD_196onDEEM_10k/Data/3.5/soaps.hdf5',\n",
    "                                      datasets=None, concatenate=True)\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf', gamma=10)\n",
    "T = kpca.fit_transform(deem_soaps[0::500])\n",
    "Ti = kpca.transform(iza_soaps[0::500])\n",
    "Tc = kpca.transform(cod_soaps[0::500])\n",
    "\n",
    "plt.scatter(T[:, 0], T[:, 1], c='gray')\n",
    "plt.scatter(deem_kpca[0::500, 0], deem_kpca[0::500, 1], c='red')\n",
    "#plt.scatter(Ti[:, 0], Ti[:, 1], c='red')\n",
    "#plt.scatter(Tc[:, 0], Tc[:, 1], c='blue')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
