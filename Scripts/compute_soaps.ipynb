{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "from selection import FPS, random_selection\n",
    "from project_utils import load_structures_from_hdf5\n",
    "\n",
    "# SOAP\n",
    "from soap import quippy_soap, librascal_soap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create root directories\n",
    "if not os.path.exists('../Processed_Data'):\n",
    "    os.mkdir('../Processed_Data')\n",
    "\n",
    "if not os.path.exists('../Results'):\n",
    "    os.mkdir('../Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOAP cutoffs (angstrom)\n",
    "cutoffs = (3.5, 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup SOAP parameters\n",
    "# TODO: define and save the whole set of parameters\n",
    "soap_hyperparameters = dict(max_radial=12,\n",
    "                            max_angular=9,\n",
    "                            cutoff_smooth_width=0.3,\n",
    "                            gaussian_sigma_constant=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SOAP hyperparameters for quick reference\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'w') as f:\n",
    "    soap_hyperparameters_copy = soap_hyperparameters.copy()\n",
    "    soap_hyperparameters_copy['interaction_cutoff'] = cutoffs\n",
    "    json.dump(soap_hyperparameters_copy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of FPS SOAP components to retain \n",
    "n_components = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of randomly selected structure to use to select the SOAP components\n",
    "n_random = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of representative environments\n",
    "n_representatives = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of training structures\n",
    "f_train = 0.7750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structure_properties(structures, \n",
    "                                 properties=['Energy_per_Si'], \n",
    "                                 property_names=['structure_energies'],\n",
    "                                 work_dir='.'):\n",
    "    \"\"\"\n",
    "        Extracts and saves number of Si, volumes per Si, and other structure properties\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    # Dictonary of standard properties\n",
    "    property_dict = dict(n_Si=[], structure_volumes=[])\n",
    "    \n",
    "    # Append extra properties to dictionary\n",
    "    for pn in property_names:\n",
    "        property_dict[pn] = []\n",
    "    \n",
    "    # Iterate over structures and fill the property dictionary\n",
    "    for structure in structures:\n",
    "        Z = structure.get_atomic_numbers()\n",
    "        n_Si = np.count_nonzero(Z == 14)\n",
    "        property_dict['n_Si'].append(n_Si)\n",
    "        property_dict['structure_volumes'].append(structure.cell.volume / n_Si)\n",
    "        \n",
    "        for p, pn in zip(properties, property_names):\n",
    "            property_dict[pn].append(structure.info[p])\n",
    "\n",
    "    # Save properties\n",
    "    for key, value in property_dict.items():\n",
    "        if key == 'n_Si':\n",
    "            fmt = '%d'\n",
    "        else:\n",
    "            fmt = '%.18e'\n",
    "        np.savetxt(f'{work_dir}/{key}.dat', np.asarray(value), fmt=fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM 10k\n",
    "deem_10k = read('../Raw_Data/DEEM_10k/DEEM_10000.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structure properties\n",
    "extract_structure_properties(deem_10k, \n",
    "                             properties=['Energy_per_Si'], \n",
    "                             property_names=['structure_energies'],\n",
    "                             work_dir='../Processed_Data/DEEM_10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random structures from which to select SOAP components\n",
    "random_idxs = random_selection(len(deem_10k), n=n_random)\n",
    "random_structures = [deem_10k[i] for i in random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 150.23it/s]\n",
      "100%|██████████| 499/499 [01:45<00:00,  4.73it/s]\n",
      "100%|██████████| 2000/2000 [00:36<00:00, 54.49it/s]\n",
      "100%|██████████| 499/499 [01:45<00:00,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# Select random structures, compute SOAPs, and select components with FPS\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    # Compute SOAPs on the sample structures\n",
    "    soaps = librascal_soap(random_structures, [14],\n",
    "                           interaction_cutoff=cutoff,\n",
    "                           **soap_hyperparameters)\n",
    "\n",
    "    # Concatenate SOAPs\n",
    "    soaps = np.vstack(soaps)\n",
    "\n",
    "    # Compute FPS components and save\n",
    "    component_idxs, distances = FPS(soaps.T, n=n_components)\n",
    "    np.savetxt(f'{work_dir}/FPS_components.idxs', \n",
    "               np.stack((component_idxs, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete the SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [01:07, 149.19it/s]\n",
      "10000it [03:12, 52.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute SOAPs for all structures retaining only the FPS components\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    component_idxs = np.loadtxt(f'{work_dir}/FPS_components.idxs', usecols=0, dtype=int)\n",
    "    \n",
    "    output_file = librascal_soap(deem_10k, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=component_idxs,\n",
    "                                 output=f'{work_dir}/soaps.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:04<00:00, 156.02it/s]\n",
      "100%|██████████| 10000/10000 [03:07<00:00, 53.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute SOAPs for all structures retaining ALL components, but average over structures\n",
    "# This computes normalized SOAPs, which seem to work, but using unnormalized SOAPs is probably more robust\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    output_file = librascal_soap(deem_10k, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=None,\n",
    "                                 average=True,\n",
    "                                 output=f'{work_dir}/soaps_full_avg.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "n_train = int(f_train*len(deem_10k))\n",
    "idxs = np.arange(0, len(deem_10k))\n",
    "np.random.shuffle(idxs)\n",
    "train_idxs = idxs[0:n_train]\n",
    "test_idxs = idxs[n_train:]\n",
    "\n",
    "np.savetxt('../Processed_Data/DEEM_10k/train.idxs', train_idxs, fmt='%d')\n",
    "np.savetxt('../Processed_Data/DEEM_10k/test.idxs', test_idxs, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1999/1999 [02:49<00:00, 11.82it/s]\n",
      "100%|██████████| 1999/1999 [02:48<00:00, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build representatives from train set\n",
    "train_idxs = np.loadtxt('../Processed_Data/DEEM_10k/train.idxs', dtype=int)\n",
    "n_digits = len(str(len(deem_10k) - 1))\n",
    "datasets = [str(i).zfill(n_digits) for i in train_idxs]\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    soaps = load_structures_from_hdf5(f'{work_dir}/soaps.hdf5', datasets=datasets, concatenate=True)\n",
    "    \n",
    "    # Select representatives from just the train set (indices are relative to the train set)\n",
    "    representatives, distances = FPS(soaps, n=n_representatives)   \n",
    "    \n",
    "    # Save representatives\n",
    "    np.savetxt(f'{work_dir}/FPS_representatives.idxs', \n",
    "               np.stack((representatives, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "\n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 944/74999 [01:43<2:14:41,  9.16it/s]"
     ]
    }
   ],
   "source": [
    "# Build unique environments\n",
    "max_unique = 75000 # \"Safety\" measure\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    soaps = load_structures_from_hdf5(f'{work_dir}/soaps.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS (indices are relative to the whole dataset)\n",
    "    unique, distances = FPS(soaps, n=max_unique)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete the SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA structures\n",
    "iza = read('../Raw_Data/GULP/IZA_226/IZA.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structure volumes\n",
    "extract_structure_properties(iza, \n",
    "                             properties=[], \n",
    "                             property_names=[],\n",
    "                             work_dir='../Processed_Data/IZA_226')\n",
    "\n",
    "# Extract energies separately\n",
    "iza_energies = np.loadtxt('../Raw_Data/GULP/IZA_226/Energies_IZA.dat', usecols=8)\n",
    "np.savetxt('../Processed_Data/IZA_226/structure_energies.dat', iza_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "226it [00:02, 109.86it/s]\n",
      "226it [00:05, 43.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute IZA SOAPs using DEEM components\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    idxs_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    # Load DEEM_10k component indices\n",
    "    component_idxs = np.loadtxt(f'{idxs_dir}/FPS_components.idxs', usecols=0, dtype=int)\n",
    "    \n",
    "    output_file = librascal_soap(iza, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=component_idxs,\n",
    "                                 output=f'{work_dir}/soaps.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 10345/15278 [00:36<00:17, 287.23it/s]\n",
      " 84%|████████▍ | 12841/15278 [00:40<00:07, 319.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build unique environments\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/IZA_226onDEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    soaps = load_structures_from_hdf5(f'{work_dir}/soaps.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS\n",
    "    unique, distances = FPS(soaps, n=-1)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [00:01<00:00, 124.56it/s]\n",
      "100%|██████████| 226/226 [00:04<00:00, 46.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute SOAPs for all structures retaining ALL components, but average over structures\n",
    "# This computes normalized SOAPs, which seem to work, but using unnormalized SOAPs is probably more robust\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/IZA_226/Data/{cutoff}'\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    output_file = librascal_soap(iza, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=None,\n",
    "                                 average=True,\n",
    "                                 output=f'{work_dir}/soaps_full_avg.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD on DEEM_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA structures\n",
    "cod = read('../Raw_Data/GULP/COD_196/COD.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structure volumes\n",
    "extract_structure_properties(cod, \n",
    "                             properties=[], \n",
    "                             property_names=[],\n",
    "                             work_dir='../Processed_Data/COD_196')\n",
    "\n",
    "# Extract structure energies separately\n",
    "cod_energies = np.loadtxt('../Raw_Data/GULP/COD_196/Energies_COD.dat', usecols=8)\n",
    "np.savetxt('../Processed_Data/COD_196/structure_energies.dat', cod_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "196it [00:00, 341.95it/s]\n",
      "196it [00:01, 130.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}'\n",
    "    idxs_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    # Load DEEM_10k component indices\n",
    "    component_idxs = np.loadtxt(f'{idxs_dir}/FPS_components.idxs', usecols=0, dtype=int)\n",
    "    \n",
    "    output_file = librascal_soap(cod, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=component_idxs,\n",
    "                                 output=f'{work_dir}/soaps.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2611/3553 [00:01<00:00, 2307.60it/s]\n",
      " 88%|████████▊ | 3127/3553 [00:01<00:00, 2290.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build unique environments\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/COD_196onDEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Load SOAPs\n",
    "    soaps = load_structures_from_hdf5(f'{work_dir}/soaps.hdf5', datasets=None, concatenate=True)    \n",
    "    \n",
    "    # Get unique structures from FPS\n",
    "    unique, distances = FPS(soaps, n=-1)\n",
    "    np.savetxt(f'{work_dir}/FPS_unique.idxs',\n",
    "               np.stack((unique, distances), axis=1), fmt='%6d\\t%.18e')\n",
    "    \n",
    "    # Delete SOAPs so we aren't carrying them around\n",
    "    del soaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEM_330k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM 10k\n",
    "deem_330k = read('../Raw_Data/DEEM_330k/XYZ/DEEM_331172.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_structure_properties(deem_330k, \n",
    "                             properties=['Energy_per_Si'], \n",
    "                             property_names=['structure_energies'],\n",
    "                             work_dir='../Processed_Data/DEEM_330k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331172it [38:11, 144.51it/s]\n",
      "331172it [1:49:48, 50.26it/s] \n"
     ]
    }
   ],
   "source": [
    "for cutoff in cutoffs:\n",
    "    work_dir = f'../Processed_Data/DEEM_330konDEEM_10k/Data/{cutoff}'\n",
    "    idxs_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    \n",
    "    # Make required directories\n",
    "    if not os.path.exists(work_dir):\n",
    "        os.makedirs(work_dir)\n",
    "    \n",
    "    # Load DEEM_10k component indices\n",
    "    component_idxs = np.loadtxt(f'{idxs_dir}/FPS_components.idxs', usecols=0, dtype=int)\n",
    "    \n",
    "    output_file = librascal_soap(deem_330k, [14],\n",
    "                                 interaction_cutoff=cutoff,\n",
    "                                 **soap_hyperparameters,\n",
    "                                 component_idxs=component_idxs,\n",
    "                                 average=True,\n",
    "                                 output=f'{work_dir}/soaps_avg.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract DEEM 10k SOAPs from DEEM 330k SOAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM 10k\n",
    "deem_10k = read('../Raw_Data/DEEM_10k/DEEM_10000.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM 10k\n",
    "deem_330k = read('../Raw_Data/DEEM_330k/XYZ/DEEM_331172.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stride construction from the 330k set to get the 10k set\n",
    "deem_10k_idxs = np.arange(0, len(deem_330k), 32)[0:10000]\n",
    "n_digits_deem = len(str(len(deem_330k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure we have the correct structures\n",
    "print(deem_10k == [deem_330k[i] for i in deem_10k_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to zero-padded string for dataset access in HDF5 file\n",
    "deem_10k_idxs = np.array([str(i).zfill(n_digits_deem) for i in deem_10k_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check that we can pull the correct SOAPs with the indices\n",
    "soaps_10k = load_structures_from_hdf5('../Processed_Data/DEEM_10k/Data/6.0/soaps.hdf5',\n",
    "                                      datasets=None, concatenate=False)\n",
    "soaps_10k = np.vstack([np.mean(soaps, axis=0) for soaps in soaps_10k])\n",
    "\n",
    "soaps_330k = load_structures_from_hdf5('../Processed_Data/DEEM_330konDEEM_10k/Data/6.0/soaps.hdf5',\n",
    "                                      datasets=deem_10k_idxs, concatenate=True)\n",
    "\n",
    "print(np.allclose(soaps_10k, soaps_330k, rtol=1.0E-12, atol=1.0E-12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indices\n",
    "np.savetxt('../Processed_Data/DEEM_330konDEEM_10k/deem_10k.idxs', deem_10k_idxs, fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
