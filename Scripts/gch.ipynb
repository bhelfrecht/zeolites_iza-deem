{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read, write\n",
    "\n",
    "# ML\n",
    "from skcosmo.decomposition import PCovR\n",
    "from sklearn.linear_model import Ridge\n",
    "from gch_init import gch_init\n",
    "from gch_run import gch_run\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import subprocess\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "import project_utils as utils\n",
    "from tools import save_json, load_json, recursive_array_convert\n",
    "\n",
    "# SOAP\n",
    "from soap import librascal_soap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 6.0\n",
    "spectrum = 'power'\n",
    "spectrum_name = spectrum.capitalize()\n",
    "n_cantons = 4\n",
    "group_name = 'OO+OSi+SiSi'\n",
    "mixing_suffixes = ['', '_0.0', '_1.0']\n",
    "#mixing_suffixes = ['']\n",
    "df_types = ['OvR', 'OvO']\n",
    "df_type = 'OvR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_230'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'\n",
    "model_data_dir = f'LPCovR/{df_type}/{n_cantons}-Class/{spectrum_name}/{group_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gch_dir = f'{model_dir}/{cutoff}/GCH/{n_cantons}-Class/{spectrum_name}/{group_name}'\n",
    "# gch_dir = f'{model_dir}/{cutoff}/GCHTMP/{n_cantons}-Class/{spectrum_name}/{group_name}'\n",
    "os.makedirs(gch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP hyperparameters and spline arguments\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')\n",
    "soap_hyperparameters.update(interaction_cutoff=cutoff)\n",
    "\n",
    "soap_spline = load_json('../Processed_Data/soap_spline.json')\n",
    "spline_args = soap_spline[f'{cutoff}'][f'{spectrum_name}Spectrum']\n",
    "soap_args = load_json('../Processed_Data/soap_args.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train sets for IZA and Deem\n",
    "iza_train_idxs = np.loadtxt(f'../Processed_Data/IZA_230/svm_train.idxs', dtype=int)\n",
    "iza_sort_train_idxs = np.argsort(iza_train_idxs)\n",
    "iza_unsort_train_idxs = np.argsort(iza_sort_train_idxs)\n",
    "\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_train.idxs', dtype=int)\n",
    "\n",
    "# Load test sets for IZA and Deem\n",
    "iza_test_idxs = np.loadtxt('../Processed_Data/IZA_230/svm_test.idxs', dtype=int)\n",
    "iza_sort_test_idxs = np.argsort(iza_test_idxs)\n",
    "iza_unsort_test_idxs = np.argsort(iza_sort_test_idxs)\n",
    "\n",
    "deem_test_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_test.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iza_frames = read('../Raw_Data/GULP/IZA_230/IZA_230.xyz', index=':')\n",
    "deem_frames = read('../Raw_Data/DEEM_330k/XYZ/DEEM_331172.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save structures for the test set\n",
    "write(\n",
    "    f'{gch_dir}/iza+deem.xyz', \n",
    "    (\n",
    "        [iza_frames[i] for i in iza_test_idxs]\n",
    "        + [deem_frames[i] for i in deem_test_idxs]\n",
    "    ),\n",
    "    format='extxyz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_energies = np.loadtxt('../Raw_Data/GULP/DEEM_330k/optimization_summary.dat', usecols=(1, 2)) # 1=Ref, 2=GULP\n",
    "iza_energies = np.loadtxt('../Raw_Data/GULP/IZA_230/optimization_summary_fix.dat', usecols=1) # 1=GULP\n",
    "\n",
    "# Center energies relative to the train set\n",
    "energy_scaler = utils.StandardNormScaler(with_mean=True, with_scale=False)\n",
    "energy_scaler.fit(\n",
    "    np.concatenate((iza_energies[iza_train_idxs], deem_energies[deem_train_idxs, 1]))\n",
    ")\n",
    "\n",
    "# Save energies for the test set\n",
    "np.savetxt(\n",
    "    f'{gch_dir}/energies_per_si.dat',\n",
    "    energy_scaler.transform(\n",
    "        np.concatenate((iza_energies[iza_test_idxs], deem_energies[deem_test_idxs, 1]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projections for the test set\n",
    "for mixing in mixing_suffixes:\n",
    "    np.savetxt(\n",
    "        f'{gch_dir}/T{mixing}.dat',\n",
    "        np.concatenate((\n",
    "            utils.load_hdf5(\n",
    "                f'{iza_dir}/{cutoff}/{model_data_dir}/pcovr_structure_projections{mixing}.hdf5',\n",
    "                indices=iza_test_idxs[iza_sort_test_idxs]\n",
    "            )[iza_unsort_test_idxs],\n",
    "            utils.load_hdf5(\n",
    "                f'{deem_dir}/{cutoff}/{model_data_dir}/pcovr_structure_projections{mixing}.hdf5',\n",
    "                indices=deem_test_idxs\n",
    "            )\n",
    "        ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute energy errors from GULP calculations on Deem frameworks\n",
    "energy_rmse = np.sqrt(np.mean(\n",
    "    (deem_energies[deem_train_idxs, 0] - deem_energies[deem_train_idxs, 1]) ** 2\n",
    "))\n",
    "print(energy_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cell uncertainty on IZA frameworks in the train set\n",
    "ref_iza_xyz = [read(f) for f in sorted(glob.glob('../Raw_Data/IZA_230/XYZ/*.xyz'))] # Sort by ID\n",
    "ref_iza_xyz = [ref_iza_xyz[i] for i in iza_train_idxs]\n",
    "\n",
    "opt_iza_xyz = [iza_frames[i] for i in iza_train_idxs]\n",
    "\n",
    "# Compute RMSE in cell vectors\n",
    "cell_errors = np.full(len(ref_iza_xyz), np.nan)\n",
    "for idx, (ref, opt) in enumerate(zip(ref_iza_xyz, opt_iza_xyz)):\n",
    "    cell_errors[idx] = (\n",
    "#         np.linalg.norm(ref.cell - opt.cell) ** 2 / np.linalg.norm(ref.cell) ** 2\n",
    "        (ref.get_volume() - opt.get_volume()) ** 2 / ref.get_volume() ** 2\n",
    "    )\n",
    "    \n",
    "cell_rmse = np.sqrt(np.mean(cell_errors))\n",
    "\n",
    "print(cell_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant volume optimizations in GULP apparently don't optimize the cell at all, so they will have cell error of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a GCH based on PCovR projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global GCH parameters\n",
    "s_c = cell_rmse # Uncertainty in cell between structures\n",
    "s_e = energy_rmse # Uncertainty in energy\n",
    "ndim = 3 # GCH dimensions (includes energy)\n",
    "numref = 100 # Number of reference structures\n",
    "numshaken = 10 # Number of rattled structures per reference\n",
    "conv = 0.50 # Convergence threshold: 100/conv hulls are constructed\n",
    "mode = 'fps' # Selection mode for the reference structures\n",
    "npca = None # Number of KPCA components: None for providing projections, <= 0 for taking all components\n",
    "mp = 0.60 # Cutoff probability for determining the GCH vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mixing in mixing_suffixes:\n",
    "    if mixing == '':\n",
    "        print('===== GCH for PCovR with optimal mixing =====')\n",
    "    else:\n",
    "        print(f'===== GCH for PCovR with mixing = {mixing[1:]} =====')  \n",
    "        \n",
    "    # Unpickle the reference PCovR model\n",
    "    pcovr_model_dict = load_json(\n",
    "        f'{model_dir}/{cutoff}/{model_data_dir}/pcovr_regressor{mixing}.json'\n",
    "    )\n",
    "    pcovr_model_dict = recursive_array_convert(pcovr_model_dict)\n",
    "\n",
    "    # Extract the regressors from PCovR\n",
    "    pcovr_regressor_dict = pcovr_model_dict.pop('regressor')\n",
    "    pcovr_fitted_regressor_dict = pcovr_model_dict.pop('regressor_')\n",
    "\n",
    "    # Initialize the PCovR\n",
    "    pcovr = PCovR()\n",
    "    pcovr.__dict__ = pcovr_model_dict\n",
    "\n",
    "    # Add the regressors to PCovR\n",
    "    ridge = Ridge()\n",
    "    ridge.__dict__ = pcovr_regressor_dict\n",
    "    fitted_ridge = Ridge()\n",
    "    fitted_ridge.__dict__ = pcovr_fitted_regressor_dict\n",
    "    pcovr.regressor = ridge\n",
    "    pcovr.regressor_ = fitted_ridge\n",
    "\n",
    "    # Load centering and scale factors to apply to the rattled structures\n",
    "    norm_scaler_dict = load_json(\n",
    "        f'{model_dir}/{cutoff}/{model_data_dir}/norm_scaler{mixing}.json'\n",
    "    )\n",
    "    norm_scaler_dict = recursive_array_convert(norm_scaler_dict)\n",
    "    norm_scaler = utils.StandardNormScaler()\n",
    "    norm_scaler.__dict__ = norm_scaler_dict\n",
    "    \n",
    "    # Initialize GCH\n",
    "    pk = f'{gch_dir}/T{mixing}.dat' # File containing the kernel (or projections)\n",
    "    pnrg = f'{gch_dir}/energies_per_si.dat' # File containing the energies\n",
    "    setxyz = f'{gch_dir}/iza+deem.xyz' # File containing the structures\n",
    "    wdir_local = f'{gch_dir}/rattled{mixing}' # Directory in which to save the rattled reference structures\n",
    "\n",
    "    gch_init(pk, pnrg, setxyz, wdir_local, s_c, s_e, ndim, numref, numshaken, conv, mode, npca)\n",
    "\n",
    "    # Compute SOAPs for shaken structures\n",
    "    shaken_refs = read(f'{gch_dir}/rattled{mixing}/shaketraj.xyz', index=':')\n",
    "\n",
    "    shaken_ref_soaps = librascal_soap(\n",
    "        shaken_refs,\n",
    "        **soap_hyperparameters,\n",
    "        **soap_args,\n",
    "        **spline_args,\n",
    "        average=True, \n",
    "        concatenate=True\n",
    "    )\n",
    "\n",
    "    shaken_ref_soaps = norm_scaler.transform(shaken_ref_soaps)\n",
    "\n",
    "    # We initialize the GCH on all structures, but project the\n",
    "    # rattled structures using the same train set as was used to build\n",
    "    # the original PCovR model\n",
    "    T_rattled = pcovr.transform(shaken_ref_soaps)\n",
    "    np.savetxt(f'{gch_dir}/rattled{mixing}/T{mixing}.dat', T_rattled)\n",
    "\n",
    "    # Run GCH\n",
    "    shk = f'{gch_dir}/rattled{mixing}/T{mixing}.dat' # File containing the kernel (or projections) for the rattled structures\n",
    "    wdir = f'{gch_dir}/rattled{mixing}' # Directory in which the rattled reference structures reside\n",
    "    gch_run(shk, wdir, mp, compute_distances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a standard convex hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the hull distances\n",
    "def hull_distances(hull, data):\n",
    "        \n",
    "    # Omit the simplices on the 'top' of the GCH\n",
    "    hull_facets = np.delete(\n",
    "        hull.equations,\n",
    "        np.nonzero(hull.equations[:, 0] > 0.0),\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    hull_distance = -1.0 * (\n",
    "        np.matmul(data, hull_facets[:, 0:-1].T) \n",
    "        + hull_facets[:, -1]\n",
    "    )\n",
    "    \n",
    "    hull_distance_energy = -1.0 * hull_distance / hull_facets[:, 0]\n",
    "    \n",
    "    hull_distance = np.amin(hull_distance, axis=1)\n",
    "    hull_distance_energy = np.amin(hull_distance_energy, axis=1)\n",
    "    \n",
    "    return hull_distance, hull_distance_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== CH for PCovR with optimal mixing =====\n",
      "===== CH for PCovR with mixing = 0.0 =====\n",
      "===== CH for PCovR with mixing = 1.0 =====\n"
     ]
    }
   ],
   "source": [
    "# Load energies\n",
    "pnrg = np.loadtxt(f'{gch_dir}/energies_per_si.dat')\n",
    "\n",
    "for mixing in mixing_suffixes:\n",
    "    if mixing == '':\n",
    "        print('===== CH for PCovR with optimal mixing =====')\n",
    "    else:\n",
    "        print(f'===== CH for PCovR with mixing = {mixing[1:]} =====')\n",
    "    \n",
    "    # Load projections\n",
    "    pk = np.loadtxt(f'{gch_dir}/T{mixing}.dat')\n",
    "    data = np.column_stack((pnrg, pk[:, 0:ndim-1]))\n",
    "    ch = ConvexHull(data)\n",
    "    \n",
    "    d, de = hull_distances(ch, data)\n",
    "    np.savetxt(f'{gch_dir}/hull_distances{mixing}.dat', d)\n",
    "    np.savetxt(f'{gch_dir}/hull_distances_energy{mixing}.dat', de)\n",
    "    np.savetxt(f'{gch_dir}/hull_vertices{mixing}.dat', ch.vertices, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
