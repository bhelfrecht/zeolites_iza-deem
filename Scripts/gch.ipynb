{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "sys.path.append('/home/helfrech/Tools/GCH/GCH')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Atoms\n",
    "from ase.io import read, write\n",
    "\n",
    "# ML\n",
    "from skcosmo.decomposition import PCovR\n",
    "from sklearn.linear_model import Ridge\n",
    "from gch_init import gch_init\n",
    "from gch_run import gch_run\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import subprocess\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "import project_utils as utils\n",
    "from tools import load_json, recursive_array_convert\n",
    "\n",
    "# SOAP\n",
    "from soap import librascal_soap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_230'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 6.0\n",
    "spectrum = 'power'\n",
    "spectrum_name = spectrum.capitalize()\n",
    "n_cantons = 4\n",
    "group_name = 'OO+OSi+SiSi'\n",
    "#mixing_suffixes = ['', '_0.0', '_1.0']\n",
    "mixing_suffixes = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gch_dir = f'{model_dir}/{cutoff}/GCH/{n_cantons}-Class/{spectrum_name}/{group_name}'\n",
    "gch_dir = f'{model_dir}/{cutoff}/GCHTMP/{n_cantons}-Class/{spectrum_name}/{group_name}'\n",
    "os.makedirs(gch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP hyperparameters and spline arguments\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')\n",
    "soap_hyperparameters.update(interaction_cutoff=cutoff)\n",
    "\n",
    "soap_spline = load_json('../Processed_Data/soap_spline.json')\n",
    "spline_args = soap_spline[f'{cutoff}'][f'{spectrum_name}Spectrum']\n",
    "soap_args = load_json('../Processed_Data/soap_args.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train sets for IZA and Deem\n",
    "iza_train_idxs = np.loadtxt(f'../Processed_Data/IZA_230/svm_train.idxs', dtype=int)\n",
    "iza_sort_train_idxs = np.argsort(iza_train_idxs)\n",
    "iza_unsort_train_idxs = np.argsort(iza_sort_train_idxs)\n",
    "\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_train.idxs', dtype=int)\n",
    "\n",
    "# Load test sets for IZA and Deem\n",
    "iza_test_idxs = np.loadtxt('../Processed_Data/IZA_230/svm_test.idxs', dtype=int)\n",
    "iza_sort_test_idxs = np.argsort(iza_test_idxs)\n",
    "iza_unsort_test_idxs = np.argsort(iza_sort_test_idxs)\n",
    "\n",
    "deem_test_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_test.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "iza_frames = read('../Raw_Data/GULP/IZA_230/IZA_230.xyz', index=':')\n",
    "deem_frames = read('../Raw_Data/DEEM_330k/XYZ/DEEM_331172.xyz', index=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save structures for the test set\n",
    "write(\n",
    "    f'{gch_dir}/iza+deem.xyz', \n",
    "    (\n",
    "        [iza_frames[i] for i in iza_test_idxs]\n",
    "        + [deem_frames[i] for i in deem_test_idxs]\n",
    "    ),\n",
    "    format='extxyz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "deem_energies = np.loadtxt('../Raw_Data/GULP/DEEM_330k/optimization_summary.dat', usecols=(1, 2)) # 1=Ref, 2=GULP\n",
    "iza_energies = np.loadtxt('../Raw_Data/GULP/IZA_230/optimization_summary_fix.dat', usecols=1) # 1=GULP\n",
    "\n",
    "# Center energies relative to the train set\n",
    "energy_scaler = utils.StandardNormScaler(with_mean=True, with_scale=False)\n",
    "energy_scaler.fit(\n",
    "    np.concatenate((iza_energies[iza_train_idxs], deem_energies[deem_train_idxs, 1]))\n",
    ")\n",
    "\n",
    "# Save energies for the test set\n",
    "np.savetxt(\n",
    "    f'{gch_dir}/energies_per_si.dat',\n",
    "    energy_scaler.transform(\n",
    "        np.concatenate((iza_energies[iza_test_idxs], deem_energies[deem_test_idxs, 1]))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07996961941015034\n"
     ]
    }
   ],
   "source": [
    "# Compute energy errors from GULP calculations on Deem frameworks\n",
    "energy_rmse = np.sqrt(np.mean(\n",
    "    (deem_energies[deem_train_idxs, 0] - deem_energies[deem_train_idxs, 1]) ** 2\n",
    "))\n",
    "print(energy_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.027778524726514407\n"
     ]
    }
   ],
   "source": [
    "# Evaluate cell uncertainty on IZA frameworks in the train set\n",
    "ref_iza_xyz = [read(f) for f in sorted(glob.glob('../Raw_Data/IZA_230/XYZ/*.xyz'))] # Sort by ID\n",
    "ref_iza_xyz = [ref_iza_xyz[i] for i in iza_train_idxs]\n",
    "\n",
    "opt_iza_xyz = [iza_frames[i] for i in iza_train_idxs]\n",
    "\n",
    "# Compute RMSE in cell vectors\n",
    "cell_errors = np.full(len(ref_iza_xyz), np.nan)\n",
    "for idx, (ref, opt) in enumerate(zip(ref_iza_xyz, opt_iza_xyz)):\n",
    "    cell_errors[idx] = (\n",
    "#         np.linalg.norm(ref.cell - opt.cell) ** 2 / np.linalg.norm(ref.cell) ** 2\n",
    "        (ref.get_volume() - opt.get_volume()) ** 2 / ref.get_volume() ** 2\n",
    "    )\n",
    "    \n",
    "cell_rmse = np.sqrt(np.mean(cell_errors))\n",
    "\n",
    "print(cell_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a GCH based on PCovR projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global GCH parameters\n",
    "s_c = cell_rmse # Uncertainty in cell between structures\n",
    "s_e = energy_rmse # Uncertainty in energy\n",
    "ndim = 3 # GCH dimensions (includes energy)\n",
    "numref = 100 # Number of reference structures\n",
    "numshaken = 10 # Number of rattled structures per reference\n",
    "conv = 0.20 # Convergence threshold: 100/conv hulls are constructed\n",
    "# TODO: if we need to re-run the GCH, use conv=0.40 or conv=0.50\n",
    "mode = 'fps' # Selection mode for the reference structures\n",
    "npca = None # Number of KPCA components: None for providing projections, <= 0 for taking all components\n",
    "mp = 0.99 # Cutoff probability for determining the GCH vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GCH for PCovR with optimal mixing =====\n",
      "Loading the kernel matrix, it can take a minute if thousands of elements\n",
      "/scratch/helfrech/Sync/GDrive/Projects/Zeolites_IZA-DEEM2/Scripts/../Processed_Data/Models/6.0/GCHTMP/4-Class/Power/OO+OSi+SiSi/rattled\n",
      "DONE: Loaded data\n",
      "Initializing statistical sampling of the fuzzy GCH\n",
      "Uncertainty in Cartesian positions 0.06767262333946768\n",
      "DONE ! go to /scratch/helfrech/Sync/GDrive/Projects/Zeolites_IZA-DEEM2/Scripts/../Processed_Data/Models/6.0/GCHTMP/4-Class/Power/OO+OSi+SiSi/rattled/ to see what's in there\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d478dd4be4486980d96d1c7cae6fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We will load both the dataset kernel and the shaken kernel, it could take some minutes in case of thousands of structures..\n",
      "You have selected 200 convex hulls samples per pruning iterations\n",
      "Statistical sampling of the fuzzy GCH\n",
      "GCH construction :  96.78909182548523  sec\n",
      "Single Hull construction during before pruning :  98.25645279884338  sec\n"
     ]
    }
   ],
   "source": [
    "# Unpickle the reference PCovR model\n",
    "model_data_dir = f'LPCovR/{n_cantons}-Class/{spectrum_name}/{group_name}'\n",
    "\n",
    "for mixing in mixing_suffixes:\n",
    "    if mixing == '':\n",
    "        print('===== GCH for PCovR with optimal mixing =====')\n",
    "    else:\n",
    "        print(f'===== GCH for PCovR with mixing = {mixing[1:]} =====')\n",
    "    \n",
    "    pcovr_model_dict = load_json(\n",
    "        f'{model_dir}/{cutoff}/{model_data_dir}/pcovr_regressor{mixing}.json'\n",
    "    )\n",
    "    pcovr_model_dict = recursive_array_convert(pcovr_model_dict)\n",
    "\n",
    "    # Extract the regressors from PCovR\n",
    "    pcovr_regressor_dict = pcovr_model_dict.pop('regressor')\n",
    "    pcovr_fitted_regressor_dict = pcovr_model_dict.pop('regressor_')\n",
    "\n",
    "    # Initialize the PCovR\n",
    "    pcovr = PCovR()\n",
    "    pcovr.__dict__ = pcovr_model_dict\n",
    "\n",
    "    # Add the regressors to PCovR\n",
    "    ridge = Ridge()\n",
    "    ridge.__dict__ = pcovr_regressor_dict\n",
    "    fitted_ridge = Ridge()\n",
    "    fitted_ridge.__dict__ = pcovr_fitted_regressor_dict\n",
    "    pcovr.regressor = ridge\n",
    "    pcovr.regressor_ = fitted_ridge\n",
    "\n",
    "    # Load centering and scale factors to apply to the rattled structures\n",
    "    norm_scaler_dict = load_json(\n",
    "        f'{model_dir}/{cutoff}/{model_data_dir}/norm_scaler{mixing}.json'\n",
    "    )\n",
    "    norm_scaler_dict = recursive_array_convert(norm_scaler_dict)\n",
    "    norm_scaler = utils.StandardNormScaler()\n",
    "    norm_scaler.__dict__ = norm_scaler_dict\n",
    "\n",
    "    np.savetxt(\n",
    "        f'{gch_dir}/T{mixing}.dat',\n",
    "        np.concatenate((\n",
    "            utils.load_hdf5(\n",
    "                f'{iza_dir}/{cutoff}/{model_data_dir}/pcovr_structure_projections{mixing}.hdf5',\n",
    "                indices=iza_test_idxs[iza_sort_test_idxs]\n",
    "            )[iza_unsort_test_idxs],\n",
    "            utils.load_hdf5(\n",
    "                f'{deem_dir}/{cutoff}/{model_data_dir}/pcovr_structure_projections{mixing}.hdf5',\n",
    "                indices=deem_test_idxs\n",
    "            )\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    # Initialize GCH\n",
    "    pk = f'{gch_dir}/T{mixing}.dat' # File containing the kernel (or projections)\n",
    "    pnrg = f'{gch_dir}/energies_per_si.dat' # File containing the energies\n",
    "    setxyz = f'{gch_dir}/iza+deem.xyz' # File containing the structures\n",
    "    wdir_local = f'{gch_dir}/rattled{mixing}' # Directory in which to save the rattled reference structures\n",
    "\n",
    "    gch_init(pk, pnrg, setxyz, wdir_local, s_c, s_e, ndim, numref, numshaken, conv, mode, npca)\n",
    "\n",
    "    # Compute SOAPs for shaken structures\n",
    "    shaken_refs = read(f'{gch_dir}/rattled{mixing}/shaketraj.xyz', index=':')\n",
    "\n",
    "    shaken_ref_soaps = librascal_soap(\n",
    "        shaken_refs,\n",
    "        **soap_hyperparameters,\n",
    "        **soap_args,\n",
    "        **spline_args,\n",
    "        average=True, \n",
    "        concatenate=True\n",
    "    )\n",
    "\n",
    "    shaken_ref_soaps = norm_scaler.transform(shaken_ref_soaps)\n",
    "\n",
    "    # We initialize the GCH on all structures, but project the\n",
    "    # rattled structures using the same train set as was used to build\n",
    "    # the original PCovR model\n",
    "    T_rattled = pcovr.transform(shaken_ref_soaps)\n",
    "    np.savetxt(f'{gch_dir}/rattled{mixing}/T{mixing}.dat', T_rattled)\n",
    "\n",
    "    # Run GCH\n",
    "    shk = f'{gch_dir}/rattled{mixing}/T{mixing}.dat' # File containing the kernel (or projections) for the rattled structures\n",
    "    wdir = f'{gch_dir}/rattled{mixing}' # Directory in which the rattled reference structures reside\n",
    "    gch_run(shk, wdir, mp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
