{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from soap import extract_species_pair_groups\n",
    "from skcosmo.decomposition import PCovR\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, PredefinedSplit\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import functools\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import project_utils as utils\n",
    "from tools import load_json, save_json\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = np.random.rand(40000, 1920)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "np.linalg.svd(tmp)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "ctmp = tmp.T @ tmp\n",
    "np.linalg.eigh(ctmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')   \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train sets for IZA and Deem\n",
    "iza_train_idxs = np.loadtxt('../Processed_Data/IZA_230/svm_train.idxs', dtype=int)\n",
    "iza_sort_idxs = np.argsort(iza_train_idxs)\n",
    "iza_unsort_idxs = np.argsort(iza_sort_idxs)\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_train.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cantons for IZA and Deem\n",
    "iza_cantons = np.loadtxt('../Raw_Data/IZA_230/cantons.dat', usecols=1, dtype=int)\n",
    "deem_cantons_2 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_2-class.dat', dtype=int)\n",
    "deem_cantons_4 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_4-class.dat', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons = {}\n",
    "\n",
    "cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs], \n",
    "    deem_cantons_4[deem_train_idxs]\n",
    "))\n",
    "\n",
    "cantons[2] = np.concatenate((\n",
    "    np.ones(len(iza_train_idxs), dtype=int),\n",
    "    deem_cantons_2[deem_train_idxs]\n",
    "))\n",
    "\n",
    "# Build set of class weights (by sample) for centering and scaling\n",
    "class_weights = {n_cantons: utils.balanced_class_weights(cantons[n_cantons]) for n_cantons in (2, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy Deem cantons to test the \"null\" case\n",
    "dummy_cantons = {}\n",
    "dummy_cantons[2] = np.loadtxt('../Processed_Data/DEEM_330k/Data/dummy_cantons_2-class.dat', dtype=int)\n",
    "dummy_cantons[2] = dummy_cantons[2][deem_train_idxs]\n",
    "dummy_cantons[4] = np.loadtxt('../Processed_Data/DEEM_330k/Data/dummy_cantons_4-class.dat', dtype=int)\n",
    "dummy_cantons[4] = dummy_cantons[4][deem_train_idxs]\n",
    "\n",
    "# Build set of dummy class weights (by sample) for centering and scaling\n",
    "dummy_class_weights = {n_cantons: utils.balanced_class_weights(dummy_cantons[n_cantons]) for n_cantons in (2, 4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_230'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV splits\n",
    "n_splits = 2\n",
    "\n",
    "y_scaler_parameters = dict(featurewise=False)\n",
    "\n",
    "pcovr_parameters = dict(n_components=4)\n",
    "ridge_parameters = dict(fit_intercept=False, tol=1.0E-12)\n",
    "\n",
    "pcovr_parameter_grid = dict(\n",
    "    pcovr__regressor__mixing=np.linspace(0.0, 1.0, 3),\n",
    "    pcovr__regressor__regressor__alpha=np.logspace(-10, 0, 3)\n",
    ")\n",
    "\n",
    "ridge_parameter_grid = dict(ridge__regressor__alpha=np.logspace(-10, 0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAPs\n",
    "iza_file = f'{iza_dir}/6.0/soaps_power_full_avg_nonorm.hdf5'\n",
    "idxs_sort = np.argsort(iza_train_idxs)\n",
    "idxs_rev = np.argsort(idxs_sort)\n",
    "iza_soaps = utils.load_hdf5(iza_file, indices=iza_train_idxs[idxs_sort])\n",
    "iza_soaps = iza_soaps[idxs_rev]\n",
    "n_iza_soaps = iza_soaps.shape[0]\n",
    "# n_train_iza = 50\n",
    "n_train_iza = n_iza_soaps // 2\n",
    "# n_train_iza = n_iza_soaps - 2\n",
    "n_test_iza = n_iza_soaps - n_train_iza\n",
    "\n",
    "deem_file = f'{deem_dir}/6.0/soaps_power_full_avg_nonorm.hdf5'\n",
    "deem_soaps = utils.load_hdf5(deem_file, indices=deem_train_idxs)\n",
    "n_deem_soaps = deem_soaps.shape[0]\n",
    "# n_train_deem = 5000\n",
    "n_train_deem = n_deem_soaps // 2\n",
    "# n_train_deem = n_deem_soaps - 2\n",
    "n_test_deem = n_deem_soaps - n_train_deem\n",
    "\n",
    "deem_idxs = np.arange(0, n_deem_soaps)\n",
    "np.random.shuffle(deem_idxs)\n",
    "deem_train_idxs2, deem_test_idxs2 = np.split(deem_idxs, [n_train_deem])\n",
    "\n",
    "iza_idxs = np.arange(0, n_iza_soaps)\n",
    "np.random.shuffle(iza_idxs)\n",
    "iza_train_idxs2, iza_test_idxs2 = np.split(iza_idxs, [n_train_iza])\n",
    "\n",
    "train_soaps = np.vstack((iza_soaps[iza_train_idxs2], deem_soaps[deem_train_idxs2]))\n",
    "test_soaps = np.vstack((iza_soaps[iza_test_idxs2], deem_soaps[deem_test_idxs2]))\n",
    "\n",
    "train_cantons = {}\n",
    "test_cantons = {}\n",
    "\n",
    "train_cantons[2] = np.concatenate((\n",
    "    np.ones(n_train_iza, dtype=int),\n",
    "    np.ones(n_train_deem, dtype=int) * 2\n",
    "))\n",
    "\n",
    "test_cantons[2] = np.concatenate((\n",
    "    np.ones(n_test_iza, dtype=int),\n",
    "    np.ones(n_test_deem, dtype=int) * 2\n",
    "))\n",
    "\n",
    "train_cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs][iza_train_idxs2],\n",
    "    np.ones(n_train_deem, dtype=int) * 4\n",
    "))\n",
    "\n",
    "test_cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs][iza_test_idxs2],\n",
    "    np.ones(n_test_deem, dtype=int) * 4\n",
    "))\n",
    "\n",
    "class_weights = {}\n",
    "class_weights[2] = utils.balanced_class_weights(train_cantons[2])\n",
    "class_weights[4] = utils.balanced_class_weights(train_cantons[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cantons = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decision functions\n",
    "iza_dfs = np.loadtxt(f'{iza_dir}/6.0/LSVC/{n_cantons}-Class/Power/OO+OSi+SiSi/svc_structure_dfs.dat')\n",
    "deem_dfs = np.loadtxt(f'{deem_dir}/6.0/LSVC/{n_cantons}-Class/Power/OO+OSi+SiSi/svc_structure_dfs.dat')\n",
    "\n",
    "train_dfs = np.concatenate((iza_dfs[iza_train_idxs][iza_train_idxs2], deem_dfs[deem_train_idxs][deem_train_idxs2]))\n",
    "test_dfs = np.concatenate((iza_dfs[iza_train_idxs][iza_test_idxs2], deem_dfs[deem_train_idxs][deem_test_idxs2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87091954, -2.25153113, -1.87853216, ...,  2.34189772,\n",
       "        2.41276949,  2.06307019])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "ridge_pipeline.fit(train_soaps, train_dfs)\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.87091957, -2.25153115, -1.87853219, ...,  2.34189773,\n",
       "        2.41276949,  2.06307019])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "            regressor=PCovR(\n",
    "                **pcovr_parameters, \n",
    "                mixing=0.0,\n",
    "                regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            ),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(train_soaps, train_dfs)\n",
    "pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.4s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.3s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.4s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.3s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.4s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=0, shuffle=True),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('ridge',\n",
       "                                        TransformedTargetRegressor(regressor=Ridge(fit_intercept=False,\n",
       "                                                                                   tol=1e-12),\n",
       "                                                                   transformer=StandardNormScaler()))]),\n",
       "             param_grid={'ridge__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit=False, return_train_score=True,\n",
       "             scoring='neg_root_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(**ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, ridge_parameter_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=False, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.33721697, 0.31307101, 0.31380606]),\n",
       " 'std_fit_time': array([0.02304709, 0.00983119, 0.01015139]),\n",
       " 'mean_score_time': array([0.02914107, 0.03050268, 0.03041935]),\n",
       " 'std_score_time': array([0.0023073 , 0.00031483, 0.00027728]),\n",
       " 'param_ridge__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'ridge__regressor__alpha': 1e-10},\n",
       "  {'ridge__regressor__alpha': 1e-05},\n",
       "  {'ridge__regressor__alpha': 1.0}],\n",
       " 'split0_test_score': array([-1.14701053e-10, -8.77499805e-06, -5.47710108e-02]),\n",
       " 'split1_test_score': array([-4.04637623e-10, -9.30601010e-06, -5.56151714e-02]),\n",
       " 'mean_test_score': array([-2.59669338e-10, -9.04050407e-06, -5.51930911e-02]),\n",
       " 'std_test_score': array([1.44968285e-10, 2.65506027e-07, 4.22080282e-04]),\n",
       " 'rank_test_score': array([1, 2, 3], dtype=int32),\n",
       " 'split0_train_score': array([-4.00044401e-11, -3.52366354e-06, -5.26510378e-02]),\n",
       " 'split1_train_score': array([-9.75902490e-11, -3.61603949e-06, -5.26551632e-02]),\n",
       " 'mean_train_score': array([-6.87973445e-11, -3.56985152e-06, -5.26531005e-02]),\n",
       " 'std_train_score': array([2.87929045e-11, 4.61879748e-08, 2.06270389e-06])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=0, shuffle=True),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('pcovr',\n",
       "                                        TransformedTargetRegressor(regressor=PCovR(n_components=4),\n",
       "                                                                   transformer=StandardNormScaler()))]),\n",
       "             param_grid={'pcovr__regressor__mixing': array([0. , 0.5, 1. ]),\n",
       "                         'pcovr__regressor__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit=False, return_train_score=True,\n",
       "             scoring=<function pcovr_score at 0x7fd7463299d8>, verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "            regressor=PCovR(**pcovr_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, pcovr_parameter_grid,\n",
    "    scoring=utils.pcovr_score,\n",
    "    cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=False, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([6.10389733, 6.02509153, 5.99428022, 6.13617504, 6.11217666,\n",
       "        6.11929381, 6.13292754, 6.11214542, 6.08698165]),\n",
       " 'std_fit_time': array([0.04745603, 0.02182877, 0.00472414, 0.0087117 , 0.00712395,\n",
       "        0.00991237, 0.00554502, 0.01084042, 0.00543916]),\n",
       " 'mean_score_time': array([0.14243543, 0.13907838, 0.11555803, 0.10922492, 0.10851419,\n",
       "        0.14373469, 0.13766146, 0.12782061, 0.11375105]),\n",
       " 'std_score_time': array([4.05156612e-03, 4.31466103e-03, 7.97998905e-03, 6.29782677e-04,\n",
       "        4.88758087e-06, 4.83274460e-04, 4.08768654e-03, 6.09719753e-03,\n",
       "        5.34784794e-03]),\n",
       " 'param_pcovr__regressor__mixing': masked_array(data=[0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pcovr__regressor__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0, 1e-10, 1e-05, 1.0, 1e-10, 1e-05,\n",
       "                    1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.0, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.5, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1.0}],\n",
       " 'split0_test_score': array([-0.71118284, -0.7111827 , -0.70835727, -0.19921132, -0.19921132,\n",
       "        -0.2000889 , -0.30348092, -0.30348092, -0.30348092]),\n",
       " 'split1_test_score': array([-0.71882201, -0.71882197, -0.71546973, -0.20030788, -0.20030788,\n",
       "        -0.20127944, -0.29984822, -0.29984822, -0.29984822]),\n",
       " 'mean_test_score': array([-0.71500242, -0.71500234, -0.7119135 , -0.1997596 , -0.1997596 ,\n",
       "        -0.20068417, -0.30166457, -0.30166457, -0.30166457]),\n",
       " 'std_test_score': array([0.00381958, 0.00381964, 0.00355623, 0.00054828, 0.00054828,\n",
       "        0.00059527, 0.00181635, 0.00181635, 0.00181635]),\n",
       " 'rank_test_score': array([9, 8, 7, 1, 2, 3, 5, 4, 6], dtype=int32),\n",
       " 'split0_train_score': array([-0.71849729, -0.71849724, -0.71550119, -0.1997931 , -0.1997931 ,\n",
       "        -0.20065316, -0.301402  , -0.301402  , -0.301402  ]),\n",
       " 'split1_train_score': array([-0.71083989, -0.71083983, -0.70783222, -0.19868568, -0.19868568,\n",
       "        -0.19950325, -0.29997061, -0.29997061, -0.29997061]),\n",
       " 'mean_train_score': array([-0.71466859, -0.71466854, -0.71166671, -0.19923939, -0.19923939,\n",
       "        -0.2000782 , -0.30068631, -0.30068631, -0.30068631]),\n",
       " 'std_train_score': array([0.0038287 , 0.0038287 , 0.00383448, 0.00055371, 0.00055371,\n",
       "        0.00057495, 0.00071569, 0.00071569, 0.00071569])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrinter():\n",
    "    def fit(self, X, y=None):\n",
    "        print(X)\n",
    "        if y is not None:\n",
    "            print(y)\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.5s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.5s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ReplicatedStratifiedKFold(n_splits=2, random_state=0, shuffle=True,\n",
       "             stratify_col=-1),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('ridge',\n",
       "                                        TransformedTargetRegressor(check_inverse=False,\n",
       "                                                                   regressor=Ridge(fit_intercept=False,\n",
       "                                                                                   tol=1e-12),\n",
       "                                                                   transformer=Pipeline(steps=[('drop_features',\n",
       "                                                                                                ColumnTransformerInve...ransformers=[('drop_features',\n",
       "                                                                                                                                        'passthrough',\n",
       "                                                                                                                                        [0])])),\n",
       "                                                                                               ('norm_scaler',\n",
       "                                                                                                StandardNormScaler())])))]),\n",
       "             param_grid={'ridge__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             return_train_score=True,\n",
       "             scoring=make_scorer(class_balanced_metric_score, greater_is_better=False, scorer=<function mean_squared_error at 0x7fd747fc4378>, class_col=-1, squared=False),\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pipeline = Pipeline(\n",
    "    [\n",
    "        ('drop_features', utils.ColumnTransformerInverse([('drop_features', 'passthrough', [0])])),\n",
    "        ('norm_scaler', utils.StandardNormScaler(**y_scaler_parameters))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "#         ('print', DataPrinter()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(**ridge_parameters),\n",
    "            transformer=y_pipeline,\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, ridge_parameter_grid,\n",
    "    scoring=make_scorer(\n",
    "        utils.class_balanced_metric_score,\n",
    "        scorer=mean_squared_error,\n",
    "        class_col=-1,\n",
    "        squared=False,\n",
    "        greater_is_better=False\n",
    "    ),\n",
    "    cv=utils.ReplicatedStratifiedKFold(n_splits=n_splits, stratify_col=-1, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, np.column_stack((train_dfs, train_cantons[n_cantons])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8709197 ],\n",
       "       [-2.25153163],\n",
       "       [-1.87853221],\n",
       "       ...,\n",
       "       [ 2.34189766],\n",
       "       [ 2.41276934],\n",
       "       [ 2.06307012]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.6s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ReplicatedStratifiedKFold(n_splits=2, random_state=0, shuffle=True,\n",
       "             stratify_col=-1),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('pcovr',\n",
       "                                        TransformedTargetRegressor(check_inverse=False,\n",
       "                                                                   regressor=PCovR(n_components=4),\n",
       "                                                                   transformer=Pipeline(steps=[('drop_features',\n",
       "                                                                                                ColumnTransformerInverse(transformer...\n",
       "                         'pcovr__regressor__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit='pcovr', return_train_score=True,\n",
       "             scoring={'pcovr': functools.partial(<function class_balanced_pcovr_score at 0x7fd746329a60>, class_col=-1),\n",
       "                      'rmse': make_scorer(class_balanced_metric_score, greater_is_better=False, scorer=<function mean_squared_error at 0x7fd747fc4378>, class_col=-1, squared=False)},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pipeline = Pipeline(\n",
    "    [\n",
    "        ('drop_features', utils.ColumnTransformerInverse([('drop_features', 'passthrough', [0])])),\n",
    "        ('norm_scaler', utils.StandardNormScaler(**y_scaler_parameters))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "#             regressor=PCovR(mixing=0.0, **pcovr_parameters),\n",
    "            regressor=PCovR(**pcovr_parameters),\n",
    "            transformer=y_pipeline,\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, pcovr_parameter_grid,\n",
    "    scoring={\n",
    "        'pcovr': functools.partial(utils.class_balanced_pcovr_score, class_col=-1),\n",
    "        'rmse': make_scorer(\n",
    "            utils.class_balanced_metric_score,\n",
    "            scorer=mean_squared_error,\n",
    "            class_col=-1,\n",
    "            squared=False,\n",
    "            greater_is_better=False\n",
    "        )\n",
    "    },\n",
    "    cv=utils.ReplicatedStratifiedKFold(n_splits=n_splits, stratify_col=-1, shuffle=True, random_state=0),\n",
    "    refit='pcovr', return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, np.column_stack((train_dfs, train_cantons[n_cantons])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.83344095],\n",
       "       [-2.19005616],\n",
       "       [-1.83995736],\n",
       "       ...,\n",
       "       [ 2.30944517],\n",
       "       [ 2.38278481],\n",
       "       [ 2.0797624 ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([6.3054899 , 6.29282749, 6.17224383, 6.31191695, 6.28120601,\n",
       "        6.29823864, 6.3026067 , 6.29776394, 6.28737533]),\n",
       " 'std_fit_time': array([0.07514679, 0.04826152, 0.01819658, 0.00365031, 0.01173365,\n",
       "        0.00025427, 0.00025403, 0.00641119, 0.00362766]),\n",
       " 'mean_score_time': array([0.1761775 , 0.18165267, 0.17947423, 0.1976105 , 0.1871109 ,\n",
       "        0.17372298, 0.17897904, 0.18496096, 0.18070209]),\n",
       " 'std_score_time': array([0.01060009, 0.01162302, 0.0087558 , 0.00382245, 0.0078907 ,\n",
       "        0.0043757 , 0.00212562, 0.00732172, 0.01073492]),\n",
       " 'param_pcovr__regressor__mixing': masked_array(data=[0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pcovr__regressor__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0, 1e-10, 1e-05, 1.0, 1e-10, 1e-05,\n",
       "                    1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.0, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.5, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1.0}],\n",
       " 'split0_test_pcovr': array([-0.75843334, -1.02599768, -0.70075685, -0.23196954, -0.23196949,\n",
       "        -0.23370341, -0.33618673, -0.33618673, -0.33618673]),\n",
       " 'split1_test_pcovr': array([-0.78644192, -0.80686582, -0.88327349, -0.25045188, -0.25045219,\n",
       "        -0.25442026, -0.39947929, -0.39947929, -0.39947929]),\n",
       " 'mean_test_pcovr': array([-0.77243763, -0.91643175, -0.79201517, -0.24121071, -0.24121084,\n",
       "        -0.24406184, -0.36783301, -0.36783301, -0.36783301]),\n",
       " 'std_test_pcovr': array([0.01400429, 0.10956593, 0.09125832, 0.00924117, 0.00924135,\n",
       "        0.01035843, 0.03164628, 0.03164628, 0.03164628]),\n",
       " 'rank_test_pcovr': array([7, 9, 8, 1, 2, 3, 5, 6, 4], dtype=int32),\n",
       " 'split0_train_pcovr': array([-0.74499212, -0.71948406, -0.74024325, -0.21106154, -0.21106154,\n",
       "        -0.211233  , -0.29546228, -0.29546228, -0.29546228]),\n",
       " 'split1_train_pcovr': array([-0.67838687, -0.67586632, -0.65827497, -0.18672416, -0.18672416,\n",
       "        -0.18691344, -0.24517479, -0.24517479, -0.24517479]),\n",
       " 'mean_train_pcovr': array([-0.7116895 , -0.69767519, -0.69925911, -0.19889285, -0.19889285,\n",
       "        -0.19907322, -0.27031853, -0.27031853, -0.27031853]),\n",
       " 'std_train_pcovr': array([0.03330262, 0.02180887, 0.04098414, 0.01216869, 0.01216869,\n",
       "        0.01215978, 0.02514375, 0.02514375, 0.02514375]),\n",
       " 'split0_test_rmse': array([-4.00414546e-09, -2.76735215e-05, -8.71939871e-02, -1.06311778e-01,\n",
       "        -1.06307204e-01, -1.38561876e-01, -7.93834429e-01, -7.93834429e-01,\n",
       "        -7.93834429e-01]),\n",
       " 'split1_test_rmse': array([-8.00216516e-10, -2.75423368e-05, -1.00907583e-01, -9.33174106e-02,\n",
       "        -9.33321281e-02, -1.67375542e-01, -7.98515798e-01, -7.98515798e-01,\n",
       "        -7.98515798e-01]),\n",
       " 'mean_test_rmse': array([-2.40218099e-09, -2.76079291e-05, -9.40507853e-02, -9.98145945e-02,\n",
       "        -9.98196662e-02, -1.52968709e-01, -7.96175114e-01, -7.96175114e-01,\n",
       "        -7.96175114e-01]),\n",
       " 'std_test_rmse': array([1.60196447e-09, 6.55923842e-08, 6.85679817e-03, 6.49718392e-03,\n",
       "        6.48753802e-03, 1.44068334e-02, 2.34068479e-03, 2.34068479e-03,\n",
       "        2.34068479e-03]),\n",
       " 'rank_test_rmse': array([1, 2, 3, 4, 5, 6, 9, 8, 7], dtype=int32),\n",
       " 'split0_train_rmse': array([-4.08339540e-10, -2.08414032e-06, -3.07199851e-02, -9.54217039e-02,\n",
       "        -9.54218732e-02, -1.11520960e-01, -7.02268236e-01, -7.02268236e-01,\n",
       "        -7.02268236e-01]),\n",
       " 'split1_train_rmse': array([-1.00618199e-10, -2.23169068e-06, -3.28884916e-02, -7.04054003e-02,\n",
       "        -7.04056130e-02, -9.00484345e-02, -5.87052551e-01, -5.87052551e-01,\n",
       "        -5.87052551e-01]),\n",
       " 'mean_train_rmse': array([-2.54478870e-10, -2.15791550e-06, -3.18042383e-02, -8.29135521e-02,\n",
       "        -8.29137431e-02, -1.00784697e-01, -6.44660394e-01, -6.44660394e-01,\n",
       "        -6.44660394e-01]),\n",
       " 'std_train_rmse': array([1.53860670e-10, 7.37751782e-08, 1.08425324e-03, 1.25081518e-02,\n",
       "        1.25081301e-02, 1.07362627e-02, 5.76078426e-02, 5.76078426e-02,\n",
       "        5.76078426e-02])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample weight pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "wts = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "fit_params = {'norm_scaler__sample_weight': wts}\n",
    "pipeline.fit(train_soaps, train_cantons[n_cantons], **fit_params)\n",
    "pipeline.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = utils.ClassBalancedPipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(train_soaps, train_cantons[n_cantons], keys=['norm_scaler__sample_weight'])\n",
    "pipeline.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameter_grid = {'svc__C': np.logspace(-1, 1, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No weights\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons])\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit_params weights\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "wts = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "fit_params = {'norm_scaler__sample_weight': wts}\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons], **fit_params)\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassBalancedPipeline weights\n",
    "pipeline = utils.ClassBalancedPipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons], keys=['norm_scaler__sample_weight'])\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might work but probably better for the time being to work with replicated samples, despite the computational expense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "W = np.diagflat(w)\n",
    "Wsqrt = np.sqrt(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights as they should be\n",
    "np.trace(Wsqrt @ train_soaps @ train_soaps.T @ Wsqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted covariance\n",
    "train_soaps.T @ W @ train_soaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt_idxs = repeat_idxs(train_cantons[n_cantons])\n",
    "replicated_train_soaps = np.repeat(train_soaps, rpt_idxs, axis=0)\n",
    "replicated_train_cantons = np.repeat(train_cantons[n_cantons], rpt_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Weights\" from replication\n",
    "np.trace(replicated_train_soaps @ replicated_train_soaps.T) / len(replicated_train_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicated covariance\n",
    "replicated_train_soaps.T @ replicated_train_soaps / len(replicated_train_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, wu = np.linalg.eigh(Wsqrt @ train_soaps @ train_soaps.T @ Wsqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv, ru = np.linalg.eigh(replicated_train_soaps @ replicated_train_soaps.T / len(replicated_train_soaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(wv > 1.0E-12), np.count_nonzero(rv  > 1.0E-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, label_counts = np.unique(train_cantons[n_cantons], return_counts=True)\n",
    "split_rpt_idxs = np.split(rpt_idxs, np.cumsum(label_counts))[0:-1]\n",
    "class_counts = np.array([len(counts) for counts in split_rpt_idxs])\n",
    "replicated_class_counts = np.array([np.sum(counts) for counts in split_rpt_idxs])\n",
    "extra_samples = (replicated_class_counts - replicated_class_counts[-1]) / class_counts\n",
    "\n",
    "labels = train_cantons[n_cantons]\n",
    "n_samples = len(train_cantons[n_cantons])\n",
    "n_classes = len(unique_labels)\n",
    "wr = np.zeros(n_samples)\n",
    "for ul, lc in zip(unique_labels, label_counts+extra_samples):\n",
    "    label_weight = (n_samples + np.sum(extra_samples * class_counts)) / (n_classes * lc)\n",
    "    wr[labels == ul] = label_weight\n",
    "    \n",
    "wr /= np.sum(wr)\n",
    "Wr = np.diagflat(wr)\n",
    "Wrsqrt = np.sqrt(Wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(Wrsqrt @ train_soaps @ train_soaps.T @ Wrsqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing RR with replicated samples vs. sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeats\n",
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=RidgeCV(alphas=np.logspace(-8, 0, 5), fit_intercept=False, cv=2),\n",
    "            #regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(featurewise=True)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "ridge_pipeline.fit(np.repeat(train_soaps, rpt_idxs, axis=0), np.repeat(train_dfs, rpt_idxs, axis=0))\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.zeros(len(unique_labels))\n",
    "for udx, ul in enumerate(unique_labels):\n",
    "    rmse[udx] = mean_squared_error(\n",
    "        test_dfs[test_cantons[n_cantons] == ul], \n",
    "        ridge_pipeline.predict(test_soaps[test_cantons[n_cantons] == ul]), \n",
    "        squared=False\n",
    "    )\n",
    "\n",
    "print(mean_squared_error(test_dfs, ridge_pipeline.predict(test_soaps), squared=False))\n",
    "print(rmse)\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "ww = np.ones(len(train_dfs)) / len(train_dfs)\n",
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=RidgeCV(alphas=np.logspace(-8, 0, 5), fit_intercept=False, cv=2),\n",
    "            #regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.DataSplitter(\n",
    "                model=utils.StandardNormScaler(featurewise=True),\n",
    "                X_cols=slice(0, -1),\n",
    "                weight_col=-1\n",
    "            ),\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "fit_params = dict(\n",
    "    norm_scaler__sample_weight=ww,\n",
    "    ridge__sample_weight=ww\n",
    ")\n",
    "ridge_pipeline.fit(train_soaps, np.column_stack((train_dfs, ww)), **fit_params)\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.zeros(len(unique_labels))\n",
    "for udx, ul in enumerate(unique_labels):\n",
    "    rmse[udx] = mean_squared_error(\n",
    "        test_dfs[test_cantons[n_cantons] == ul], \n",
    "        ridge_pipeline.predict(test_soaps[test_cantons[n_cantons] == ul]), \n",
    "        squared=False\n",
    "    )\n",
    "    \n",
    "print(mean_squared_error(test_dfs, ridge_pipeline.predict(test_soaps).squeeze(), squared=False))\n",
    "print(rmse)\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing PCA with replicated samples vs. sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
