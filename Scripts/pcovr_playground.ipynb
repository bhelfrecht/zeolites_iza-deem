{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from soap import extract_species_pair_groups\n",
    "from skcosmo.decomposition import PCovR\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, PredefinedSplit\n",
    "from sklearn.compose import TransformedTargetRegressor, ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import functools\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from copy import deepcopy\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import project_utils as utils\n",
    "from tools import load_json, save_json\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmp = np.random.rand(40000, 1920)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "np.linalg.svd(tmp)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "ctmp = tmp.T @ tmp\n",
    "np.linalg.eigh(ctmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')   \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train sets for IZA and Deem\n",
    "iza_train_idxs = np.loadtxt('../Processed_Data/IZA_230/svm_train.idxs', dtype=int)\n",
    "iza_sort_idxs = np.argsort(iza_train_idxs)\n",
    "iza_unsort_idxs = np.argsort(iza_sort_idxs)\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/svm_train.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cantons for IZA and Deem\n",
    "iza_cantons = np.loadtxt('../Raw_Data/IZA_230/cantons.dat', usecols=1, dtype=int)\n",
    "deem_cantons_2 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_2-class.dat', dtype=int)\n",
    "deem_cantons_4 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_4-class.dat', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons = {}\n",
    "\n",
    "cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs], \n",
    "    deem_cantons_4[deem_train_idxs]\n",
    "))\n",
    "\n",
    "cantons[2] = np.concatenate((\n",
    "    np.ones(len(iza_train_idxs), dtype=int),\n",
    "    deem_cantons_2[deem_train_idxs]\n",
    "))\n",
    "\n",
    "# Build set of class weights (by sample) for centering and scaling\n",
    "class_weights = {n_cantons: utils.balanced_class_weights(cantons[n_cantons]) for n_cantons in (2, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy Deem cantons to test the \"null\" case\n",
    "dummy_cantons = {}\n",
    "dummy_cantons[2] = np.loadtxt('../Processed_Data/DEEM_330k/Data/dummy_cantons_2-class.dat', dtype=int)\n",
    "dummy_cantons[2] = dummy_cantons[2][deem_train_idxs]\n",
    "dummy_cantons[4] = np.loadtxt('../Processed_Data/DEEM_330k/Data/dummy_cantons_4-class.dat', dtype=int)\n",
    "dummy_cantons[4] = dummy_cantons[4][deem_train_idxs]\n",
    "\n",
    "# Build set of dummy class weights (by sample) for centering and scaling\n",
    "dummy_class_weights = {n_cantons: utils.balanced_class_weights(dummy_cantons[n_cantons]) for n_cantons in (2, 4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_230'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV splits\n",
    "n_splits = 2\n",
    "\n",
    "y_scaler_parameters = dict(featurewise=False)\n",
    "\n",
    "pcovr_parameters = dict(n_components=4)\n",
    "ridge_parameters = dict(fit_intercept=False, tol=1.0E-12)\n",
    "\n",
    "pcovr_parameter_grid = dict(\n",
    "    pcovr__regressor__mixing=np.linspace(0.0, 1.0, 3),\n",
    "    pcovr__regressor__regressor__alpha=np.logspace(-10, 0, 3)\n",
    ")\n",
    "\n",
    "ridge_parameter_grid = dict(ridge__regressor__alpha=np.logspace(-10, 0, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAPs\n",
    "iza_file = f'{iza_dir}/6.0/soaps_power_full_avg_nonorm.hdf5'\n",
    "idxs_sort = np.argsort(iza_train_idxs)\n",
    "idxs_rev = np.argsort(idxs_sort)\n",
    "iza_soaps = utils.load_hdf5(iza_file, indices=iza_train_idxs[idxs_sort])\n",
    "iza_soaps = iza_soaps[idxs_rev]\n",
    "n_iza_soaps = iza_soaps.shape[0]\n",
    "# n_train_iza = 50\n",
    "n_train_iza = n_iza_soaps // 2\n",
    "# n_train_iza = n_iza_soaps - 2\n",
    "n_test_iza = n_iza_soaps - n_train_iza\n",
    "\n",
    "deem_file = f'{deem_dir}/6.0/soaps_power_full_avg_nonorm.hdf5'\n",
    "deem_soaps = utils.load_hdf5(deem_file, indices=deem_train_idxs)\n",
    "n_deem_soaps = deem_soaps.shape[0]\n",
    "# n_train_deem = 5000\n",
    "n_train_deem = n_deem_soaps // 2\n",
    "# n_train_deem = n_deem_soaps - 2\n",
    "n_test_deem = n_deem_soaps - n_train_deem\n",
    "\n",
    "deem_idxs = np.arange(0, n_deem_soaps)\n",
    "np.random.shuffle(deem_idxs)\n",
    "deem_train_idxs2, deem_test_idxs2 = np.split(deem_idxs, [n_train_deem])\n",
    "\n",
    "iza_idxs = np.arange(0, n_iza_soaps)\n",
    "np.random.shuffle(iza_idxs)\n",
    "iza_train_idxs2, iza_test_idxs2 = np.split(iza_idxs, [n_train_iza])\n",
    "\n",
    "train_soaps = np.vstack((iza_soaps[iza_train_idxs2], deem_soaps[deem_train_idxs2]))\n",
    "test_soaps = np.vstack((iza_soaps[iza_test_idxs2], deem_soaps[deem_test_idxs2]))\n",
    "\n",
    "train_cantons = {}\n",
    "test_cantons = {}\n",
    "\n",
    "train_cantons[2] = np.concatenate((\n",
    "    np.ones(n_train_iza, dtype=int),\n",
    "    np.ones(n_train_deem, dtype=int) * 2\n",
    "))\n",
    "\n",
    "test_cantons[2] = np.concatenate((\n",
    "    np.ones(n_test_iza, dtype=int),\n",
    "    np.ones(n_test_deem, dtype=int) * 2\n",
    "))\n",
    "\n",
    "train_cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs][iza_train_idxs2],\n",
    "    np.ones(n_train_deem, dtype=int) * 4\n",
    "))\n",
    "\n",
    "test_cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs][iza_test_idxs2],\n",
    "    np.ones(n_test_deem, dtype=int) * 4\n",
    "))\n",
    "\n",
    "class_weights = {}\n",
    "class_weights[2] = utils.balanced_class_weights(train_cantons[2])\n",
    "class_weights[4] = utils.balanced_class_weights(train_cantons[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cantons = 2\n",
    "if n_cantons == 2:\n",
    "    col_slice = slice(0, 1)\n",
    "else:\n",
    "    col_slice = slice(0, n_cantons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decision functions\n",
    "iza_dfs = np.loadtxt(f'{iza_dir}/6.0/LSVC/{n_cantons}-Class/Power/OO+OSi+SiSi/svc_structure_dfs.dat')\n",
    "deem_dfs = np.loadtxt(f'{deem_dir}/6.0/LSVC/{n_cantons}-Class/Power/OO+OSi+SiSi/svc_structure_dfs.dat')\n",
    "\n",
    "train_dfs = np.concatenate((iza_dfs[iza_train_idxs][iza_train_idxs2], deem_dfs[deem_train_idxs][deem_train_idxs2]))\n",
    "test_dfs = np.concatenate((iza_dfs[iza_train_idxs][iza_test_idxs2], deem_dfs[deem_train_idxs][deem_test_idxs2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.59229871, -1.33505447, -2.87091946, ...,  2.49580407,\n",
       "        2.01959786,  2.66440218])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "ridge_pipeline.fit(train_soaps, train_dfs)\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.59229874, -1.33505452, -2.87091946, ...,  2.49580409,\n",
       "        2.01959786,  2.66440219])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "            regressor=PCovR(\n",
    "                **pcovr_parameters, \n",
    "                mixing=0.0,\n",
    "                regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            ),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(train_soaps, train_dfs)\n",
    "pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.4s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.3s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.4s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.3s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.4s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=0, shuffle=True),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('ridge',\n",
       "                                        TransformedTargetRegressor(regressor=Ridge(fit_intercept=False,\n",
       "                                                                                   tol=1e-12),\n",
       "                                                                   transformer=StandardNormScaler()))]),\n",
       "             param_grid={'ridge__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit=False, return_train_score=True,\n",
       "             scoring='neg_root_mean_squared_error', verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(**ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, ridge_parameter_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=False, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.31750762, 0.31362426, 0.31510985]),\n",
       " 'std_fit_time': array([0.0026058 , 0.0097183 , 0.01028287]),\n",
       " 'mean_score_time': array([0.03173792, 0.03064442, 0.03054595]),\n",
       " 'std_score_time': array([0.0053283 , 0.00053525, 0.0005765 ]),\n",
       " 'param_ridge__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'ridge__regressor__alpha': 1e-10},\n",
       "  {'ridge__regressor__alpha': 1e-05},\n",
       "  {'ridge__regressor__alpha': 1.0}],\n",
       " 'split0_test_score': array([-1.48292738e-10, -8.75719000e-06, -5.36717411e-02]),\n",
       " 'split1_test_score': array([-7.09144718e-10, -9.60682552e-06, -5.77138167e-02]),\n",
       " 'mean_test_score': array([-4.28718728e-10, -9.18200776e-06, -5.56927789e-02]),\n",
       " 'std_test_score': array([2.80425990e-10, 4.24817760e-07, 2.02103782e-03]),\n",
       " 'rank_test_score': array([1, 2, 3], dtype=int32),\n",
       " 'split0_train_score': array([-4.78323105e-11, -3.57345093e-06, -5.22047931e-02]),\n",
       " 'split1_train_score': array([-1.20425780e-10, -3.69391408e-06, -5.34597435e-02]),\n",
       " 'mean_train_score': array([-8.41290454e-11, -3.63368250e-06, -5.28322683e-02]),\n",
       " 'std_train_score': array([3.62967349e-11, 6.02315751e-08, 6.27475187e-04])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.0s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.0s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.0s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.0s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.1s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.2s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=2, random_state=0, shuffle=True),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('pcovr',\n",
       "                                        TransformedTargetRegressor(regressor=PCovR(n_components=4,\n",
       "                                                                                   regressor=Ridge(alpha=1e-06,\n",
       "                                                                                                   fit_intercept=False,\n",
       "                                                                                                   tol=1e-12)),\n",
       "                                                                   transformer=StandardNormScaler()))]),\n",
       "             param_grid={'pcovr__regressor__mixing': array([0. , 0.5, 1. ]),\n",
       "                         'pcovr__regressor__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit=False, return_train_score=True,\n",
       "             scoring=<function pcovr_score at 0x7f143eb972f0>, verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "            regressor=PCovR(**pcovr_parameters),\n",
    "            transformer=utils.StandardNormScaler(**y_scaler_parameters)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, pcovr_parameter_grid,\n",
    "    scoring=utils.pcovr_score,\n",
    "    cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=False, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([5.90916049, 5.91694891, 5.89721668, 6.07170415, 5.96476841,\n",
       "        5.98468328, 5.98111844, 5.97968423, 5.99518287]),\n",
       " 'std_fit_time': array([0.02554142, 0.01008832, 0.00077999, 0.08218575, 0.00325465,\n",
       "        0.01145029, 0.00961399, 0.01055801, 0.04881632]),\n",
       " 'mean_score_time': array([0.14291453, 0.12813497, 0.12724221, 0.12556481, 0.13017416,\n",
       "        0.13475168, 0.11060739, 0.12984157, 0.13106203]),\n",
       " 'std_score_time': array([0.0011766 , 0.01666141, 0.01502335, 0.01584291, 0.01586008,\n",
       "        0.00801361, 0.00029945, 0.01322436, 0.01225948]),\n",
       " 'param_pcovr__regressor__mixing': masked_array(data=[0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pcovr__regressor__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0, 1e-10, 1e-05, 1.0, 1e-10, 1e-05,\n",
       "                    1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.0, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.5, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1.0}],\n",
       " 'split0_test_score': array([-0.70896875, -0.70896877, -0.70588537, -0.19714539, -0.19714539,\n",
       "        -0.19798032, -0.29435263, -0.29435263, -0.29435263]),\n",
       " 'split1_test_score': array([-0.72285415, -0.72285404, -0.71999606, -0.20363532, -0.20363532,\n",
       "        -0.20467614, -0.30895257, -0.30895257, -0.30895257]),\n",
       " 'mean_test_score': array([-0.71591145, -0.7159114 , -0.71294072, -0.20039035, -0.20039035,\n",
       "        -0.20132823, -0.3016526 , -0.3016526 , -0.3016526 ]),\n",
       " 'std_test_score': array([0.0069427 , 0.00694263, 0.00705535, 0.00324497, 0.00324497,\n",
       "        0.00334791, 0.00729997, 0.00729997, 0.00729997]),\n",
       " 'rank_test_score': array([9, 8, 7, 2, 1, 3, 4, 5, 6], dtype=int32),\n",
       " 'split0_train_score': array([-0.72185053, -0.72185047, -0.71896032, -0.20261097, -0.20261097,\n",
       "        -0.20345027, -0.30105284, -0.30105284, -0.30105284]),\n",
       " 'split1_train_score': array([-0.70795523, -0.70795518, -0.70493691, -0.19608968, -0.19608968,\n",
       "        -0.1969189 , -0.29858258, -0.29858258, -0.29858258]),\n",
       " 'mean_train_score': array([-0.71490288, -0.71490282, -0.71194862, -0.19935032, -0.19935032,\n",
       "        -0.20018458, -0.29981771, -0.29981771, -0.29981771]),\n",
       " 'std_train_score': array([0.00694765, 0.00694765, 0.00701171, 0.00326065, 0.00326065,\n",
       "        0.00326568, 0.00123513, 0.00123513, 0.00123513])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrinter():\n",
    "    def fit(self, X, y=None):\n",
    "        print(X)\n",
    "        if y is not None:\n",
    "            print(y)\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        print(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-10; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.5s\n",
      "[CV] END ......................ridge__regressor__alpha=1e-05; total time=   0.5s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.5s\n",
      "[CV] END ........................ridge__regressor__alpha=1.0; total time=   0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ReplicatedStratifiedKFold(n_splits=2, random_state=0, shuffle=True,\n",
       "             stratify_col=-1),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('ridge',\n",
       "                                        TransformedTargetRegressor(check_inverse=False,\n",
       "                                                                   regressor=Ridge(fit_intercept=False,\n",
       "                                                                                   tol=1e-12),\n",
       "                                                                   transformer=Pipeline(steps=[('drop_features',\n",
       "                                                                                                ColumnTransformerInve...[('drop_features',\n",
       "                                                                                                                                        'passthrough',\n",
       "                                                                                                                                        slice(0, 1, None))])),\n",
       "                                                                                               ('norm_scaler',\n",
       "                                                                                                StandardNormScaler())])))]),\n",
       "             param_grid={'ridge__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             return_train_score=True,\n",
       "             scoring=make_scorer(class_balanced_metric_score, greater_is_better=False, scorer=<function mean_squared_error at 0x7f144082c400>, class_col=-1, squared=False),\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pipeline = Pipeline(\n",
    "    [\n",
    "#         ('drop_features', utils.ColumnTransformerInverse([('drop_features', 'passthrough', col_slice)])),\n",
    "        ('drop_features', utils.ColumnTransformerInverse(\n",
    "            [('drop_features', 'drop', -1)],\n",
    "            remainder='passthrough'\n",
    "        )),\n",
    "        ('norm_scaler', utils.StandardNormScaler(**y_scaler_parameters))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "#         ('print', DataPrinter()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=Ridge(**ridge_parameters),\n",
    "            transformer=y_pipeline,\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, ridge_parameter_grid,\n",
    "    scoring=make_scorer(\n",
    "        utils.class_balanced_metric_score,\n",
    "        scorer=mean_squared_error,\n",
    "        class_col=-1,\n",
    "        squared=False,\n",
    "        greater_is_better=False\n",
    "    ),\n",
    "    cv=utils.ReplicatedStratifiedKFold(n_splits=n_splits, stratify_col=-1, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, np.column_stack((train_dfs, train_cantons[n_cantons])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38715866],\n",
       "       [-1.09143563],\n",
       "       [ 0.36412421],\n",
       "       ...,\n",
       "       [ 0.85145064],\n",
       "       [ 6.75896238],\n",
       "       [ 2.24166333]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-10; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1e-05; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=0.5, pcovr__regressor__regressor__alpha=1.0; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.5s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-10; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.4s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1e-05; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.3s\n",
      "[CV] END pcovr__regressor__mixing=1.0, pcovr__regressor__regressor__alpha=1.0; total time=   6.4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ReplicatedStratifiedKFold(n_splits=2, random_state=0, shuffle=True,\n",
       "             stratify_col=-1),\n",
       "             error_score='raise',\n",
       "             estimator=Pipeline(steps=[('norm_scaler', StandardNormScaler()),\n",
       "                                       ('pcovr',\n",
       "                                        TransformedTargetRegressor(check_inverse=False,\n",
       "                                                                   regressor=PCovR(n_components=4,\n",
       "                                                                                   regressor=Ridge(alpha=1e-06,\n",
       "                                                                                                   fit_intercept=False,\n",
       "                                                                                                   tol=1e-12)),\n",
       "                                                                   transformer=Pipeline(s...\n",
       "                         'pcovr__regressor__regressor__alpha': array([1.e-10, 1.e-05, 1.e+00])},\n",
       "             refit='pcovr', return_train_score=True,\n",
       "             scoring={'pcovr': functools.partial(<function class_balanced_pcovr_score at 0x7f143eb97378>, class_col=-1),\n",
       "                      'rmse': make_scorer(class_balanced_metric_score, greater_is_better=False, scorer=<function mean_squared_error at 0x7f144082c400>, class_col=-1, squared=False)},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pipeline = Pipeline(\n",
    "    [\n",
    "#         ('drop_features', utils.ColumnTransformerInverse([('drop_features', 'passthrough', col_slice)])),\n",
    "        ('drop_features', utils.ColumnTransformerInverse(\n",
    "            [('drop_features', 'drop', -1)], remainder='passthrough'\n",
    "        )),\n",
    "        ('norm_scaler', utils.StandardNormScaler(**y_scaler_parameters))\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "#         ('print', DataPrinter()),\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('pcovr', TransformedTargetRegressor(\n",
    "#             regressor=PCovR(mixing=0.0, **pcovr_parameters),\n",
    "            regressor=PCovR(**pcovr_parameters),\n",
    "            transformer=y_pipeline,\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "# IZA + DEEM classification\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, pcovr_parameter_grid,\n",
    "    scoring={\n",
    "        'pcovr': functools.partial(utils.class_balanced_pcovr_score, class_col=-1),\n",
    "        'rmse': make_scorer(\n",
    "            utils.class_balanced_metric_score,\n",
    "            scorer=mean_squared_error,\n",
    "            class_col=-1,\n",
    "            squared=False,\n",
    "            greater_is_better=False\n",
    "        )\n",
    "    },\n",
    "    cv=utils.ReplicatedStratifiedKFold(n_splits=n_splits, stratify_col=-1, shuffle=True, random_state=0),\n",
    "    refit='pcovr', return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, np.column_stack((train_dfs, train_cantons[n_cantons])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35488922],\n",
       "       [-0.9396629 ],\n",
       "       [ 0.35352605],\n",
       "       ...,\n",
       "       [ 0.88023965],\n",
       "       [ 6.79922626],\n",
       "       [ 2.25907879]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([6.10000634, 6.0878762 , 6.1032238 , 6.3533957 , 6.29492998,\n",
       "        6.29024899, 6.29276741, 6.29065943, 6.3384794 ]),\n",
       " 'std_fit_time': array([0.01149154, 0.01272595, 0.00660896, 0.07314229, 0.00877452,\n",
       "        0.0013479 , 0.00630844, 0.00952983, 0.03946459]),\n",
       " 'mean_score_time': array([0.16890574, 0.16970551, 0.18906629, 0.18010783, 0.18232131,\n",
       "        0.17057621, 0.18263209, 0.18077219, 0.17820716]),\n",
       " 'std_score_time': array([0.00018692, 0.00112069, 0.00234258, 0.01030493, 0.01221991,\n",
       "        0.00011671, 0.01323068, 0.01078165, 0.00802684]),\n",
       " 'param_pcovr__regressor__mixing': masked_array(data=[0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_pcovr__regressor__regressor__alpha': masked_array(data=[1e-10, 1e-05, 1.0, 1e-10, 1e-05, 1.0, 1e-10, 1e-05,\n",
       "                    1.0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.0, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 0.5,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 0.5, 'pcovr__regressor__regressor__alpha': 1.0},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-10},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1e-05},\n",
       "  {'pcovr__regressor__mixing': 1.0,\n",
       "   'pcovr__regressor__regressor__alpha': 1.0}],\n",
       " 'split0_test_pcovr': array([-0.87612901, -0.7287882 , -1.05434727, -0.23674887, -0.23674889,\n",
       "        -0.23799177, -0.35686055, -0.35686055, -0.35686055]),\n",
       " 'split1_test_pcovr': array([-0.84494241, -0.94439986, -1.02771443, -0.24552691, -0.24552693,\n",
       "        -0.24673183, -0.36763004, -0.36763004, -0.36763004]),\n",
       " 'mean_test_pcovr': array([-0.86053571, -0.83659403, -1.04103085, -0.24113789, -0.24113791,\n",
       "        -0.2423618 , -0.36224529, -0.36224529, -0.36224529]),\n",
       " 'std_test_pcovr': array([0.0155933 , 0.10780583, 0.01331642, 0.00438902, 0.00438902,\n",
       "        0.00437003, 0.00538475, 0.00538475, 0.00538475]),\n",
       " 'rank_test_pcovr': array([8, 7, 9, 1, 2, 3, 5, 6, 4], dtype=int32),\n",
       " 'split0_train_pcovr': array([-0.73525923, -0.74714067, -0.71909896, -0.21020125, -0.21020125,\n",
       "        -0.21039033, -0.33642009, -0.33642009, -0.33642009]),\n",
       " 'split1_train_pcovr': array([-0.6875319 , -0.67945898, -0.6631674 , -0.20305164, -0.20305164,\n",
       "        -0.20323996, -0.27985607, -0.27985607, -0.27985607]),\n",
       " 'mean_train_pcovr': array([-0.71139557, -0.71329982, -0.69113318, -0.20662645, -0.20662645,\n",
       "        -0.20681514, -0.30813808, -0.30813808, -0.30813808]),\n",
       " 'std_train_pcovr': array([0.02386366, 0.03384084, 0.02796578, 0.00357481, 0.00357481,\n",
       "        0.00357519, 0.02828201, 0.02828201, 0.02828201]),\n",
       " 'split0_test_rmse': array([-1.00467246e-09, -2.24926079e-05, -7.59630089e-02, -5.58625759e-02,\n",
       "        -5.58667219e-02, -1.03604574e-01, -8.10395907e-01, -8.10395907e-01,\n",
       "        -8.10395907e-01]),\n",
       " 'split1_test_rmse': array([-1.04719271e-09, -2.06487747e-05, -6.58371538e-02, -8.48462216e-02,\n",
       "        -8.48488882e-02, -1.23336624e-01, -7.99221812e-01, -7.99221812e-01,\n",
       "        -7.99221812e-01]),\n",
       " 'mean_test_rmse': array([-1.02593259e-09, -2.15706913e-05, -7.09000813e-02, -7.03543988e-02,\n",
       "        -7.03578051e-02, -1.13470599e-01, -8.04808859e-01, -8.04808859e-01,\n",
       "        -8.04808859e-01]),\n",
       " 'std_test_rmse': array([2.12601257e-11, 9.21916560e-07, 5.06292755e-03, 1.44918228e-02,\n",
       "        1.44910832e-02, 9.86602544e-03, 5.58704753e-03, 5.58704753e-03,\n",
       "        5.58704754e-03]),\n",
       " 'rank_test_rmse': array([1, 2, 5, 3, 4, 6, 8, 9, 7], dtype=int32),\n",
       " 'split0_train_rmse': array([-1.43445074e-10, -2.21329845e-06, -3.22552148e-02, -4.94620525e-02,\n",
       "        -4.94622724e-02, -6.97993625e-02, -8.10831805e-01, -8.10831805e-01,\n",
       "        -8.10831805e-01]),\n",
       " 'split1_train_rmse': array([-1.93563345e-10, -2.19758597e-06, -3.22587545e-02, -7.15704484e-02,\n",
       "        -7.15706423e-02, -9.01115148e-02, -6.78051817e-01, -6.78051817e-01,\n",
       "        -6.78051817e-01]),\n",
       " 'mean_train_rmse': array([-1.68504209e-10, -2.20544221e-06, -3.22569846e-02, -6.05162505e-02,\n",
       "        -6.05164573e-02, -7.99554387e-02, -7.44441811e-01, -7.44441811e-01,\n",
       "        -7.44441811e-01]),\n",
       " 'std_train_rmse': array([2.50591353e-11, 7.85623830e-09, 1.76984374e-06, 1.10541979e-02,\n",
       "        1.10541850e-02, 1.01560762e-02, 6.63899942e-02, 6.63899942e-02,\n",
       "        6.63899942e-02])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample weight pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "wts = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "fit_params = {'norm_scaler__sample_weight': wts}\n",
    "pipeline.fit(train_soaps, train_cantons[n_cantons], **fit_params)\n",
    "pipeline.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = utils.ClassBalancedPipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline.fit(train_soaps, train_cantons[n_cantons], keys=['norm_scaler__sample_weight'])\n",
    "pipeline.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_parameter_grid = {'svc__C': np.logspace(-1, 1, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No weights\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons])\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit_params weights\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "wts = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "fit_params = {'norm_scaler__sample_weight': wts}\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons], **fit_params)\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassBalancedPipeline weights\n",
    "pipeline = utils.ClassBalancedPipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('svc', LinearSVC(class_weight='balanced', max_iter=10000, dual=False))\n",
    "    ],\n",
    ")\n",
    "\n",
    "gscv = GridSearchCV(\n",
    "    pipeline, svc_parameter_grid,\n",
    "    scoring='balanced_accuracy',\n",
    "    cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "    refit=True, return_train_score=True, error_score='raise',\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "gscv.fit(train_soaps, train_cantons[n_cantons], keys=['norm_scaler__sample_weight'])\n",
    "gscv.decision_function(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might work but probably better for the time being to work with replicated samples, despite the computational expense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = utils.balanced_class_weights(train_cantons[n_cantons])\n",
    "W = np.diagflat(w)\n",
    "Wsqrt = np.sqrt(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights as they should be\n",
    "np.trace(Wsqrt @ train_soaps @ train_soaps.T @ Wsqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted covariance\n",
    "train_soaps.T @ W @ train_soaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt_idxs = repeat_idxs(train_cantons[n_cantons])\n",
    "replicated_train_soaps = np.repeat(train_soaps, rpt_idxs, axis=0)\n",
    "replicated_train_cantons = np.repeat(train_cantons[n_cantons], rpt_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Weights\" from replication\n",
    "np.trace(replicated_train_soaps @ replicated_train_soaps.T) / len(replicated_train_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicated covariance\n",
    "replicated_train_soaps.T @ replicated_train_soaps / len(replicated_train_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv, wu = np.linalg.eigh(Wsqrt @ train_soaps @ train_soaps.T @ Wsqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv, ru = np.linalg.eigh(replicated_train_soaps @ replicated_train_soaps.T / len(replicated_train_soaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(wv > 1.0E-12), np.count_nonzero(rv  > 1.0E-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels, label_counts = np.unique(train_cantons[n_cantons], return_counts=True)\n",
    "split_rpt_idxs = np.split(rpt_idxs, np.cumsum(label_counts))[0:-1]\n",
    "class_counts = np.array([len(counts) for counts in split_rpt_idxs])\n",
    "replicated_class_counts = np.array([np.sum(counts) for counts in split_rpt_idxs])\n",
    "extra_samples = (replicated_class_counts - replicated_class_counts[-1]) / class_counts\n",
    "\n",
    "labels = train_cantons[n_cantons]\n",
    "n_samples = len(train_cantons[n_cantons])\n",
    "n_classes = len(unique_labels)\n",
    "wr = np.zeros(n_samples)\n",
    "for ul, lc in zip(unique_labels, label_counts+extra_samples):\n",
    "    label_weight = (n_samples + np.sum(extra_samples * class_counts)) / (n_classes * lc)\n",
    "    wr[labels == ul] = label_weight\n",
    "    \n",
    "wr /= np.sum(wr)\n",
    "Wr = np.diagflat(wr)\n",
    "Wrsqrt = np.sqrt(Wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(Wrsqrt @ train_soaps @ train_soaps.T @ Wrsqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing RR with replicated samples vs. sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeats\n",
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=RidgeCV(alphas=np.logspace(-8, 0, 5), fit_intercept=False, cv=2),\n",
    "            #regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.StandardNormScaler(featurewise=True)\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "ridge_pipeline.fit(np.repeat(train_soaps, rpt_idxs, axis=0), np.repeat(train_dfs, rpt_idxs, axis=0))\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.zeros(len(unique_labels))\n",
    "for udx, ul in enumerate(unique_labels):\n",
    "    rmse[udx] = mean_squared_error(\n",
    "        test_dfs[test_cantons[n_cantons] == ul], \n",
    "        ridge_pipeline.predict(test_soaps[test_cantons[n_cantons] == ul]), \n",
    "        squared=False\n",
    "    )\n",
    "\n",
    "print(mean_squared_error(test_dfs, ridge_pipeline.predict(test_soaps), squared=False))\n",
    "print(rmse)\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "ww = np.ones(len(train_dfs)) / len(train_dfs)\n",
    "ridge_pipeline = Pipeline(\n",
    "    [\n",
    "        ('norm_scaler', utils.StandardNormScaler()),\n",
    "        ('ridge', TransformedTargetRegressor(\n",
    "            regressor=RidgeCV(alphas=np.logspace(-8, 0, 5), fit_intercept=False, cv=2),\n",
    "            #regressor=Ridge(alpha=1.0E-6, **ridge_parameters),\n",
    "            transformer=utils.DataSplitter(\n",
    "                model=utils.StandardNormScaler(featurewise=True),\n",
    "                X_cols=slice(0, -1),\n",
    "                weight_col=-1\n",
    "            ),\n",
    "            check_inverse=False\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "fit_params = dict(\n",
    "    norm_scaler__sample_weight=ww,\n",
    "    ridge__sample_weight=ww\n",
    ")\n",
    "ridge_pipeline.fit(train_soaps, np.column_stack((train_dfs, ww)), **fit_params)\n",
    "ridge_pipeline.predict(test_soaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.zeros(len(unique_labels))\n",
    "for udx, ul in enumerate(unique_labels):\n",
    "    rmse[udx] = mean_squared_error(\n",
    "        test_dfs[test_cantons[n_cantons] == ul], \n",
    "        ridge_pipeline.predict(test_soaps[test_cantons[n_cantons] == ul]), \n",
    "        squared=False\n",
    "    )\n",
    "    \n",
    "print(mean_squared_error(test_dfs, ridge_pipeline.predict(test_soaps).squeeze(), squared=False))\n",
    "print(rmse)\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing PCA with replicated samples vs. sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
