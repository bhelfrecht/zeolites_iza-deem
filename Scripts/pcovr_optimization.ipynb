{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ML\n",
    "from regression import PCovR\n",
    "from soap import extract_species_pair_groups\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.compose import GridSearchCV\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "import json\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import project_utils as utils\n",
    "from tools import load_json, save_json\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "# Import COSMO style toolkit\n",
    "import cosmoplot.colorbars as cosmocbars\n",
    "import cosmoplot.utils as cosmoutils\n",
    "import cosmoplot.style as cosmostyle\n",
    "\n",
    "cosmostyle.set_style('article')\n",
    "colorList = cosmostyle.color_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/kernel-tutorials')\n",
    "sys.path.append('/scratch/helfrech/Sync/GDrive/Projects/KPCovR/KernelPCovR/analysis/scripts')\n",
    "# from utilities.sklearn_covr.kpcovr import KernelPCovR as KPCovR2\n",
    "# from utilities.sklearn_covr.pcovr import PCovR as PCovR2\n",
    "from helpers import l_regr, l_kpcovr, l_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: clean up the PCovR/KPCovR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: custom scorer for PCovR with loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regression_losses(y_train, y_test, yp_train, yp_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lr_test = np.sum(l_regr(y_test, yp_test))\n",
    "    lr_train = np.sum(l_regr(y_train, yp_train))\n",
    "\n",
    "    return lr_train, lr_test\n",
    "\n",
    "def compute_kernel_projection_losses(k_train, k_test, k_test_test, t_train, t_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lp_test = np.sum(l_kpcovr(k_train=k_train,\n",
    "                              k_test=k_test,\n",
    "                              k_test_test=k_test_test,\n",
    "                              t_train=t_train, t_test=t_test))\n",
    "\n",
    "    lp_train = np.sum(l_kpcovr(k_train=k_train,\n",
    "                               t_train=t_train, t_test=t_test))\n",
    "\n",
    "    return lp_train, lp_test\n",
    "\n",
    "def compute_linear_projection_losses(x_train, x_test, xr_train, xr_test):\n",
    "    \n",
    "    # Sum over cantonwise losses\n",
    "    lp_train = np.sum(l_proj(x_train, xr=xr_train))\n",
    "    lp_test = np.sum(l_proj(x_test, xr=xr_test))\n",
    "\n",
    "    return lp_train, lp_test\n",
    "\n",
    "def load_pcovr_losses(filename, alphas, regularizations, dtype):\n",
    "    \n",
    "    # Load losses\n",
    "    loss_matrix_shape = (len(alphas), len(regularizations))\n",
    "    pcovr_errors = np.loadtxt(filename, dtype=dtype)\n",
    "    alpha_matrix = np.reshape(pcovr_errors['alpha'], loss_matrix_shape)\n",
    "    reg_matrix = np.reshape(pcovr_errors['regularization'], loss_matrix_shape)\n",
    "    lr_train_matrix = np.reshape(np.mean(pcovr_errors['lr_train'], axis=1), loss_matrix_shape)\n",
    "    lr_test_matrix = np.reshape(np.mean(pcovr_errors['lr_test'], axis=1), loss_matrix_shape)\n",
    "    lp_train_matrix = np.reshape(np.mean(pcovr_errors['lp_train'], axis=1), loss_matrix_shape)\n",
    "    lp_test_matrix = np.reshape(np.mean(pcovr_errors['lp_test'], axis=1), loss_matrix_shape)\n",
    "    alphas = alpha_matrix[:, 0]\n",
    "    opt_reg_idx = np.unravel_index(np.argmin(lr_test_matrix + lp_test_matrix), \n",
    "                                   lr_test_matrix.shape)[1]\n",
    "    \n",
    "    return alphas, opt_reg_idx, \\\n",
    "        lr_train_matrix, lr_test_matrix, lp_train_matrix, lp_test_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "with open('../Processed_Data/soap_hyperparameters.json', 'r') as f:\n",
    "    soap_hyperparameters = json.load(f)\n",
    "    \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train set and CV indices for Deem\n",
    "deem_train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/train.idxs', dtype=int)\n",
    "deem_cv_idxs = np.loadtxt('../Processed_Data/DEEM_330k/cv_2.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train set and CV indices for IZA\n",
    "iza_train_idxs = np.loadtxt('../Processed_Data/IZA_226/train.idxs', dtype=int)\n",
    "iza_cv_idxs = np.loadtxt('../Processed_Data/IZA_226/cv_2.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IZA cantons\n",
    "iza_cantons = np.loadtxt('../Raw_Data/GULP/IZA_226/cantons.txt', usecols=1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DEEM cantons\n",
    "deem_cantons_2 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_2-class.dat', dtype=int)\n",
    "deem_cantons_4 = np.loadtxt('../Processed_Data/DEEM_330k/Data/cantons_4-class.dat', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build set of \"master\" canton labels\n",
    "cantons = {}\n",
    "\n",
    "cantons[4] = np.concatenate((\n",
    "    iza_cantons[iza_train_idxs], \n",
    "    deem_cantons_4[deem_train_idxs]\n",
    "))\n",
    "\n",
    "cantons[2] = np.concatenate((\n",
    "    np.ones(len(iza_train_idxs), dtype=int),\n",
    "    deem_cantons_2[deem_train_idxs]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate IZA and Deem CV idxs\n",
    "cv_idxs = np.vstack((iza_cv_idxs, deem_cv_idxs + len(iza_train_idxs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../Processed_Data/Models'\n",
    "\n",
    "deem_name = 'DEEM_330k'\n",
    "iza_name = 'IZA_226'\n",
    "deem_dir = f'../Processed_Data/{deem_name}/Data'\n",
    "iza_dir = f'../Processed_Data/{iza_name}/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: handle n_components for binary and multiclass cases somehow\n",
    "pcovr_parameters = dict(n_components=2)\n",
    "\n",
    "alphas = np.linspace(0.0, 1.0, 11)\n",
    "regularizations = np.logspace(-12, -1, 2)\n",
    "parameter_grid=dict(\n",
    "    pcovr__regressor__regularization=regularizations, \n",
    "    pcovr__regressor__alpha=alphas # TODO: make this the mixing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear PCovR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model setup\n",
    "n_species = 2\n",
    "group_names = {'power': ['OO', 'OSi', 'SiSi', \n",
    "                         'OO+OSi', 'OO+SiSi', 'OSi+SiSi',\n",
    "                         'OO+OSi+SiSi'], \n",
    "               'radial': ['O', 'Si', 'O+Si']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/PCovR'\n",
    "    \n",
    "    for spectrum_type in tqdm(('power', 'radial'), desc='Spectrum', leave=False):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        # Load SOAPs\n",
    "        iza_file = f'{iza_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        iza_soaps = utils.load_hdf5(iza_file, indices=iza_train_idxs)\n",
    "\n",
    "        deem_file = f'{deem_dir}/{cutoff}/soaps_{spectrum_type}_full_avg_nonorm.hdf5'\n",
    "        deem_soaps = utils.load_hdf5(deem_file, indices=deem_train_idxs)\n",
    "        \n",
    "        soaps = np.vstack((iza_soaps, deem_soaps))\n",
    "        \n",
    "        n_features = soaps_train.shape[1]\n",
    "        feature_groups = extract_species_pair_groups(n_features, n_species, \n",
    "                                                     spectrum_type=spectrum_type,\n",
    "                                                     combinations=True)\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(tqdm(group_names[spectrum_type], \n",
    "                                                      desc='Species', leave=False),\n",
    "                                                 feature_groups):\n",
    "            \n",
    "            for n_cantons in tqdm((2, 4), desc='Classes', leave=False):\n",
    "                \n",
    "                # Prepare inputs and outputs\n",
    "                df_dir = f'Linear_Models/SVC/{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                \n",
    "                # Load decision functions\n",
    "                iza_dfs = np.loadtxt(f'{iza_dir}/{cutoff}/{df_dir}/svc_structure_dfs.dat')\n",
    "                iza_dfs = iza_dfs[iza_train_idxs]\n",
    "                \n",
    "                deem_dfs = np.loadtxt(f'{deem_dir}/{cutoff}/{df_dir}/svc_structure_dfs.dat')\n",
    "                deem_dfs = deem_dfs[deem_train_idxs]\n",
    "                \n",
    "                dfs = np.concatenate((iza_dfs, deem_dfs))\n",
    "                                \n",
    "                cache_dir = mkdtemp()\n",
    "                \n",
    "                pipeline = Pipeline(\n",
    "                    [\n",
    "                        ('norm_scaler', utils.NormScaler()),\n",
    "                        ('pcovr', TransformedTargetRegressor(\n",
    "                            regressor=PCovR(**pcovr_parameters),\n",
    "                            transformer=utils.NormScaler(featurewise=True)\n",
    "                        ))\n",
    "                    ],\n",
    "                    memory=cache_dir\n",
    "                )\n",
    "                \n",
    "                # IZA + DEEM classification\n",
    "                gscv = GridSearchCV(\n",
    "                    pipeline, parameter_grid,\n",
    "                    scoring={\n",
    "                        regression_loss, # TODO: scoring wrapper or `score` method in the PCovR class\n",
    "                        projection_loss # TODO\n",
    "                    },\n",
    "                    cv=utils.cv_generator(cv_idxs),\n",
    "                    refit=False, return_train_score=True, error_score='raise'\n",
    "                )\n",
    "                gscv.fit(soaps[:, feature_idxs], dfs)\n",
    "                \n",
    "                # Prepare outputs\n",
    "                output_dir = f'{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                os.makedirs(f'{work_dir}/{output_dir}', exist_ok=True)\n",
    "                \n",
    "                save_json(gscv.cv_results_, f'{work_dir}/{output_dir}/cv_results.json')\n",
    "                rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the cross-validated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IZA + DEEM classification\n",
    "for cutoff in cutoffs:\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/PCovR'\n",
    "    for spectrum_type in ('power', 'radial'):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        for group_name in group_names[spectrum_type]:\n",
    "            for n_cantons in (2, 4):\n",
    "                result_dir = f'{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "                cv_results = load_json(f'){work_dir}/{output_dir}/cv_results.json')\n",
    "                print(f'-----Optimal Parameters for {cutoff} {spectrum_type} {group_name} {n_cantons} -----')\n",
    "                for score in (): # TODO: names of custom scorers\n",
    "                    idx = np.argmin(cv_results[f'rank_test_{score}'])\n",
    "                    opt_parameters = utils.get_optimal_parameters(cv_results, score, **svm_parameters)\n",
    "                    print(f'{score} =', cv_results[f'mean_test_{score}'][idx])\n",
    "                    print(opt_parameters)\n",
    "                    print('')\n",
    "                    \n",
    "                    save_json(opt_parameters, f'{work_dir}/{output_dir}/pcovr_parameters_{score}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for optimization problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    work_dir = f'{model_dir}/{cutoff}/Linear_Models/PCovR'\n",
    "    \n",
    "    for spectrum_type in ('power', 'radial'):\n",
    "        spectrum_name = spectrum_type.capitalize()\n",
    "        \n",
    "        for species_pairing, feature_idxs in zip(group_names[spectrum_type], feature_groups):\n",
    "                        \n",
    "            for n_cantons in (2, 4):\n",
    "                \n",
    "                print(f'Cutoff: {cutoff}, Spectrum: {spectrum_name}, Species: {species_pairing}, {n_cantons}-Class')\n",
    "                \n",
    "                output_dir = f'{n_cantons}-Class/{spectrum_name}/{species_pairing}'\n",
    "            \n",
    "                loss_file = f'{work_dir}/{output_dir}/pcovr_optimization.dat'\n",
    "\n",
    "                # TODO: Load losses\n",
    "                \n",
    "\n",
    "                fig = plt.figure(figsize=(7.0, 3.5))\n",
    "                axs_loss_sum_train = fig.add_subplot(1, 2, 1)\n",
    "                axs_loss_sum_test = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "                # Sum of projection and regression loss over all cantons for the train set\n",
    "                axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "                axs_loss_sum_train.semilogy(alphas, lp_train_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "                axs_loss_sum_train.semilogy(alphas, lr_train_matrix[:, opt_reg_idx]\n",
    "                                            + lp_train_matrix[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "                axs_loss_sum_train.legend()\n",
    "                axs_loss_sum_train.set_title('Train')\n",
    "                axs_loss_sum_train.set_xlabel('alpha')\n",
    "                axs_loss_sum_train.set_ylabel('loss')\n",
    "\n",
    "                # Sum of projection and regression loss over all cantons for the test set\n",
    "                axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx], 'o-', label='l_regr')\n",
    "                axs_loss_sum_test.semilogy(alphas, lp_test_matrix[:, opt_reg_idx], 'o-', label='l_proj')\n",
    "                axs_loss_sum_test.semilogy(alphas, lr_test_matrix[:, opt_reg_idx]\n",
    "                                           + lp_test_matrix[:, opt_reg_idx], 'o-', label='l_regr+l_proj')\n",
    "\n",
    "                axs_loss_sum_test.legend()\n",
    "                axs_loss_sum_test.set_title('Test')\n",
    "                axs_loss_sum_test.set_xlabel('alpha')\n",
    "                axs_loss_sum_test.set_ylabel('loss')\n",
    "\n",
    "                plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
