{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/helfrech/Tools/Toolbox/utils')\n",
    "\n",
    "# Maths\n",
    "import numpy as np\n",
    "\n",
    "# ML\n",
    "from kernels import gaussian_kernel\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import KernelCenterer\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Utilities\n",
    "import h5py\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from tqdm.auto import tqdm\n",
    "from tools import load_json, save_json\n",
    "import project_utils as utils\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test sets\n",
    "train_idxs = np.loadtxt('../Processed_Data/DEEM_330k/ridge_train.idxs', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOAP cutoffs\n",
    "soap_hyperparameters = load_json('../Processed_Data/soap_hyperparameters.json')   \n",
    "cutoffs = soap_hyperparameters['interaction_cutoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory in which to store models\n",
    "os.makedirs('../Processed_Data/Models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set property names for loading\n",
    "property_names = ['volumes', 'energies']\n",
    "\n",
    "# Load structure properties\n",
    "structure_properties = {}\n",
    "for pn in property_names:\n",
    "    structure_properties[pn] = np.loadtxt(f'../Processed_Data/DEEM_330k/Data/structure_{pn}.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV splits\n",
    "n_splits = 5\n",
    "\n",
    "# Use all defaults for the template parameters\n",
    "ridge_parameters = dict()\n",
    "\n",
    "# Regularization parameters for cross-validation\n",
    "regularizations = np.logspace(-10, 0, 11)\n",
    "parameter_grid = dict(ridge__regressor__alpha=regularizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97ac93139a64065a05c424226f582f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cutoff', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Property', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Property', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over cutoffs\n",
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_330k/Data/{cutoff}'\n",
    "    \n",
    "    # Read SOAPs in training set\n",
    "    soaps = utils.load_hdf5(f'{data_dir}/soaps_power_full_avg_nonorm.hdf5', indices=train_idxs)\n",
    "    \n",
    "    # Loop over properties\n",
    "    for pn in tqdm(property_names, desc='Property', leave=False):\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        # Set working directory\n",
    "        work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "        os.makedirs(work_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the property values (just from the train set)\n",
    "        y = structure_properties[pn][train_idxs]\n",
    "         \n",
    "        # Cross validation pipeline\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                ('norm_scaler', utils.StandardNormScaler()), \n",
    "                ('ridge', TransformedTargetRegressor(\n",
    "                    regressor=Ridge(**ridge_parameters), \n",
    "                    transformer=utils.StandardNormScaler()\n",
    "                ))\n",
    "            ],\n",
    "        )\n",
    "        gscv = GridSearchCV(\n",
    "            pipeline, parameter_grid, \n",
    "            scoring=[\n",
    "                'neg_root_mean_squared_error', \n",
    "                'neg_mean_absolute_error'\n",
    "            ],\n",
    "            cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            refit=False, return_train_score=True, error_score='raise', n_jobs=4\n",
    "        )\n",
    "        gscv.fit(soaps, y)\n",
    "        \n",
    "        save_json(gscv.cv_results_, f'{work_dir}/cv_results.json', array_convert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Optimal Parameters for 3.5 volumes mean_absolute_error-----\n",
      "neg_mean_absolute_error = -2.552061285701326\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 volumes root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -3.7051810263578226\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 energies mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.6715277769635979\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 energies root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.9367011619628013\n",
      "{'alpha': 0.001}\n",
      "\n",
      "-----Optimal Parameters for 6.0 volumes mean_absolute_error-----\n",
      "neg_mean_absolute_error = -1.0821631613521194\n",
      "{'alpha': 0.1}\n",
      "\n",
      "-----Optimal Parameters for 6.0 volumes root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -1.7742008005202385\n",
      "{'alpha': 0.01}\n",
      "\n",
      "-----Optimal Parameters for 6.0 energies mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.2011229827081546\n",
      "{'alpha': 1e-06}\n",
      "\n",
      "-----Optimal Parameters for 6.0 energies root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.3824125108103671\n",
      "{'alpha': 1e-05}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract optimal parameters as a check\n",
    "\n",
    "# Loop over cutoffs\n",
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Loop over properties\n",
    "    for pn in property_names:\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "        cv_results = load_json(f'{work_dir}/cv_results.json')\n",
    "        \n",
    "        # Loop over error types\n",
    "        for error, error_name in zip(\n",
    "            ['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
    "            ['mae', 'rmse']\n",
    "        ):\n",
    "        \n",
    "            idx = np.argmin(cv_results[f'rank_test_{error}'])\n",
    "            opt_parameters = utils.get_optimal_parameters(cv_results, error, **ridge_parameters)\n",
    "\n",
    "            # Print error and parameters\n",
    "            print(f'-----Optimal Parameters for {cutoff} {pn} {error[4:]}-----')\n",
    "            print(f'{error} =', cv_results[f'mean_test_{error}'][idx])\n",
    "            print(opt_parameters)\n",
    "            print('')\n",
    "            \n",
    "            # Save optimal parameters for easy access\n",
    "            save_json(opt_parameters, f'{work_dir}/ridge_parameters_{error_name}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Ridge Regression (random train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets (random DEEM)\n",
    "train_idxs_random = np.loadtxt('../Processed_Data/DEEM_330k/ridge_train_random.idxs', dtype=int)\n",
    "sort_idxs = np.argsort(train_idxs_random)\n",
    "rev_idxs = np.argsort(sort_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98545a4f48b148aeb22e255d86919caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cutoff', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Property', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Property', max=2.0, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over cutoffs\n",
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_330k/Data/{cutoff}'\n",
    "    \n",
    "    # Read SOAPs in training set\n",
    "    soaps = utils.load_hdf5(f'{data_dir}/soaps_power_full_avg_nonorm.hdf5', indices=train_idxs_random[sort_idxs])\n",
    "    soaps = soaps[rev_idxs]\n",
    "    \n",
    "    # Loop over properties\n",
    "    for pn in tqdm(property_names, desc='Property', leave=False):\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        # Set working directory\n",
    "        work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "        os.makedirs(work_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the property values (just from the train set)\n",
    "        y = structure_properties[pn][train_idxs_random]\n",
    "         \n",
    "        # Cross validation pipeline\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                ('norm_scaler', utils.StandardNormScaler()), \n",
    "                ('ridge', TransformedTargetRegressor(\n",
    "                    regressor=Ridge(**ridge_parameters), \n",
    "                    transformer=utils.StandardNormScaler()\n",
    "                ))\n",
    "            ],\n",
    "        )\n",
    "        gscv = GridSearchCV(\n",
    "            pipeline, parameter_grid, \n",
    "            scoring=[\n",
    "                'neg_root_mean_squared_error', \n",
    "                'neg_mean_absolute_error'\n",
    "            ],\n",
    "            cv=KFold(n_splits=n_splits, shuffle=True, random_state=0),\n",
    "            refit=False, return_train_score=True, error_score='raise', n_jobs=4\n",
    "        )\n",
    "        gscv.fit(soaps, y)\n",
    "        \n",
    "        save_json(gscv.cv_results_, f'{work_dir}/cv_results_random.json', array_convert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Optimal Parameters for 3.5 volumes mean_absolute_error-----\n",
      "neg_mean_absolute_error = -2.6405177225997285\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 volumes root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -3.7671040988835145\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 energies mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.6800422758359846\n",
      "{'alpha': 0.0001}\n",
      "\n",
      "-----Optimal Parameters for 3.5 energies root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.9654442919931505\n",
      "{'alpha': 0.001}\n",
      "\n",
      "-----Optimal Parameters for 6.0 volumes mean_absolute_error-----\n",
      "neg_mean_absolute_error = -1.118881417444896\n",
      "{'alpha': 0.1}\n",
      "\n",
      "-----Optimal Parameters for 6.0 volumes root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -1.7833032039956365\n",
      "{'alpha': 0.01}\n",
      "\n",
      "-----Optimal Parameters for 6.0 energies mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.199422690774243\n",
      "{'alpha': 1e-07}\n",
      "\n",
      "-----Optimal Parameters for 6.0 energies root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.31327487949455873\n",
      "{'alpha': 1e-06}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract optimal parameters as a check\n",
    "\n",
    "# Loop over cutoffs\n",
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Loop over properties\n",
    "    for pn in property_names:\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "        cv_results = load_json(f'{work_dir}/cv_results_random.json')\n",
    "        \n",
    "        # Loop over error types\n",
    "        for error, error_name in zip(\n",
    "            ['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
    "            ['mae', 'rmse']\n",
    "        ):\n",
    "        \n",
    "            idx = np.argmin(cv_results[f'rank_test_{error}'])\n",
    "            opt_parameters = utils.get_optimal_parameters(cv_results, error, **ridge_parameters)\n",
    "\n",
    "            # Print error and parameters\n",
    "            print(f'-----Optimal Parameters for {cutoff} {pn} {error[4:]}-----')\n",
    "            print(f'{error} =', cv_results[f'mean_test_{error}'][idx])\n",
    "            print(opt_parameters)\n",
    "            print('')\n",
    "            \n",
    "            # Save optimal parameters for easy access\n",
    "            save_json(opt_parameters, f'{work_dir}/ridge_parameters_{error_name}_random.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Ridge Regression IZA compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test sets (random DEEM)\n",
    "train_idxs_iza = np.loadtxt('../Processed_Data/IZA_230/svm_train.idxs', dtype=int)\n",
    "sort_idxs_iza = np.argsort(train_idxs_iza)\n",
    "rev_idxs_iza = np.argsort(sort_idxs_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compositions\n",
    "iza_compositions = np.loadtxt('../Raw_Data/IZA_230/cantons_compositions.dat', usecols=2)[train_idxs_iza]\n",
    "property_name = 'composition'\n",
    "property_label = property_name.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV splits\n",
    "n_splits_iza = 2\n",
    "\n",
    "# Use all defaults for the template parameters\n",
    "ridge_parameters_iza = dict()\n",
    "\n",
    "# Regularization parameters for cross-validation\n",
    "regularizations_iza = np.logspace(-10, 5, 16)\n",
    "parameter_grid_iza = dict(ridge__regressor__alpha=regularizations_iza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb38ba3c884061a23904adde749981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cutoff', max=2.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over cutoffs\n",
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/IZA_230/Data/{cutoff}'\n",
    "    \n",
    "    # Read SOAPs in training set\n",
    "    soaps = utils.load_hdf5(f'{data_dir}/soaps_power_full_avg_nonorm.hdf5', indices=train_idxs_iza[sort_idxs_iza])\n",
    "    soaps = soaps[rev_idxs_iza]\n",
    "        \n",
    "    # Set working directory\n",
    "    work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "\n",
    "    # Cross validation pipeline\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            ('norm_scaler', utils.StandardNormScaler()), \n",
    "            ('ridge', TransformedTargetRegressor(\n",
    "                regressor=Ridge(**ridge_parameters_iza), \n",
    "                transformer=utils.StandardNormScaler()\n",
    "            ))\n",
    "        ],\n",
    "    )\n",
    "    gscv = GridSearchCV(\n",
    "        pipeline, parameter_grid_iza, \n",
    "        scoring=[\n",
    "            'neg_root_mean_squared_error', \n",
    "            'neg_mean_absolute_error'\n",
    "        ],\n",
    "        cv=KFold(n_splits=n_splits_iza, shuffle=True, random_state=0),\n",
    "        refit=False, return_train_score=True, error_score='raise', n_jobs=4\n",
    "    )\n",
    "    gscv.fit(soaps, iza_compositions)\n",
    "\n",
    "    save_json(gscv.cv_results_, f'{work_dir}/cv_results.json', array_convert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Optimal Parameters for 3.5 composition mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.09230897966963178\n",
      "{'alpha': 0.1}\n",
      "\n",
      "-----Optimal Parameters for 3.5 composition root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.1127996556075398\n",
      "{'alpha': 0.1}\n",
      "\n",
      "-----Optimal Parameters for 6.0 composition mean_absolute_error-----\n",
      "neg_mean_absolute_error = -0.0876635821591121\n",
      "{'alpha': 1.0}\n",
      "\n",
      "-----Optimal Parameters for 6.0 composition root_mean_squared_error-----\n",
      "neg_root_mean_squared_error = -0.10591209191457866\n",
      "{'alpha': 1.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract optimal parameters as a check\n",
    "\n",
    "# Loop over cutoffs\n",
    "for cutoff in cutoffs:        \n",
    "    work_dir = f'../Processed_Data/Models/{cutoff}/LRR/{property_label}'\n",
    "    cv_results = load_json(f'{work_dir}/cv_results.json')\n",
    "\n",
    "    # Loop over error types\n",
    "    for error, error_name in zip(\n",
    "        ['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
    "        ['mae', 'rmse']\n",
    "    ):\n",
    "\n",
    "        idx = np.argmin(cv_results[f'rank_test_{error}'])\n",
    "        opt_parameters = utils.get_optimal_parameters(cv_results, error, **ridge_parameters_iza)\n",
    "\n",
    "        # Print error and parameters\n",
    "        print(f'-----Optimal Parameters for {cutoff} {property_name} {error[4:]}-----')\n",
    "        print(f'{error} =', cv_results[f'mean_test_{error}'][idx])\n",
    "        print(opt_parameters)\n",
    "        print('')\n",
    "\n",
    "        # Save optimal parameters for easy access\n",
    "        save_json(opt_parameters, f'{work_dir}/ridge_parameters_{error_name}.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all defaults for the template parameters\n",
    "kernel_ridge_parameters = dict(kernel='precomputed')\n",
    "\n",
    "# Set ranges of kernel gamma (for Gaussian kernel) and regularization\n",
    "log_gammas = np.linspace(-3, 3, 7)\n",
    "##log_gammas = np.array([-2.0])\n",
    "\n",
    "regularizations = np.logspace(-10, 0, 11)\n",
    "##regularizations = np.array([1.0E-3, 1.0E-2])\n",
    "\n",
    "# Use the filenames of the kernels that we will be loading\n",
    "# in the \"hacked\" pipeline.\n",
    "# The gamma names for the kernel parsing will be set in the loop\n",
    "parameter_grid_base = dict(\n",
    "    ridge__regressor__alpha=regularizations, \n",
    ")\n",
    "\n",
    "cv_idxs = np.arange(0, len(train_idxs), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over cutoffs\n",
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    # Set data directory\n",
    "    data_dir = f'../Processed_Data/DEEM_10k/Data/{cutoff}'\n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}/KRR'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Read SOAPs in training set\n",
    "    soaps = utils.load_hdf5(f'{data_dir}/soaps_power_full_nonorm.hdf5')\n",
    "    soaps = np.array([np.mean(soaps[i], axis=0) for i in train_idxs])\n",
    "    \n",
    "    # Build a \"superkernel\": a concatenation of the train and test kernels\n",
    "    # (which we can store easily in memory and on disk) so that\n",
    "    # the pipeline doesn't compute whole new kernels for each CV set.\n",
    "    # We still have to compute a kernel for each gamma, though.\n",
    "    # We store the kernels named by the logarithm of the gamma parameter,\n",
    "    # so the filenames don't get ridiculous\n",
    "    for log_gamma in tqdm(log_gammas, desc='Gamma', leave=False):\n",
    "        gamma = 10 ** log_gamma\n",
    "        K = gaussian_kernel(soaps, soaps, gamma=gamma) # Use this\n",
    "        utils.save_hdf5(\n",
    "            f'{model_dir}/gaussian_kernel_{log_gamma}.hdf5', K, \n",
    "            attrs=dict(gamma=gamma, log_gamma=log_gamma), chunks=(100, 100)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparameters\n",
    "\n",
    "# Loop over cutoffs\n",
    "for cutoff in tqdm(cutoffs, desc='Cutoff', leave=True):\n",
    "    \n",
    "    # Set data directory\n",
    "    model_dir = f'../Processed_Data/Models/{cutoff}/KRR'\n",
    "    \n",
    "    parameter_grid = deepcopy(parameter_grid_base)\n",
    "    parameter_grid['kernel_loader__filename'] = \\\n",
    "        [f'{model_dir}/gaussian_kernel_{log_gamma}.hdf5' for log_gamma in log_gammas]\n",
    "          \n",
    "    # Loop over properties\n",
    "    for pn in tqdm(property_names, desc='Property', leave=False):\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        # Set working directory\n",
    "        work_dir = f'{model_dir}/{property_label}'\n",
    "        os.makedirs(work_dir, exist_ok=True)\n",
    "                \n",
    "        # Load the property values (just from the train set)\n",
    "        y = structure_properties[pn][train_idxs]\n",
    "        \n",
    "        # \"Hacked\" pipeline: instead of recomputing the expensive kernels\n",
    "        # at each CV iteration, we will fit instead with a set of indices\n",
    "        # and use the filename containing the kernel at a particular\n",
    "        # gamma as a hyperparameter for KernelLoader, which will\n",
    "        # load the kernel at initialization.\n",
    "                \n",
    "        # NOTE: can't just use the custom kernel as a callable\n",
    "        # to KernelRidge, as the custom kernel necessarily operates on 2D arrays\n",
    "        # of the features for all environments in a given structure,\n",
    "        # whereas the callable must operate on pairs of samples.\n",
    "        # The KernelLoader/KernelConstructor is used instead\n",
    "        cache_dir = mkdtemp()\n",
    "        pipeline = Pipeline(\n",
    "            [\n",
    "                ('kernel_loader', utils.KernelLoader()),\n",
    "                ('kernel_norm_scaler', utils.KernelNormScaler()), # TODO: make sure the kernel scaling is the same as linear in the case of linear kernel\n",
    "                ('ridge', TransformedTargetRegressor(\n",
    "                    regressor=KernelRidge(**kernel_ridge_parameters),\n",
    "                    transformer=utils.SampleSelector(X=y, model=utils.StandardNormScaler()),\n",
    "                    check_inverse=False\n",
    "                ))\n",
    "            ],\n",
    "            memory=cache_dir\n",
    "        )\n",
    "        \n",
    "        # \"Hacked\" CV: since we want to access the kernels via indices,\n",
    "        # we fit the CV with cv_indices, and the cv_generator \n",
    "        # is used to pick out the correct folds\n",
    "        gscv = GridSearchCV(\n",
    "            pipeline, parameter_grid, \n",
    "            scoring=dict(\n",
    "                neg_mean_absolute_error=make_scorer(\n",
    "                    utils.score_by_index,\n",
    "                    greater_is_better=False,\n",
    "                    y=y, scorer=mean_absolute_error\n",
    "                ),\n",
    "                neg_root_mean_squared_error=make_scorer(\n",
    "                    utils.score_by_index,\n",
    "                    greater_is_better=False,\n",
    "                    y=y, scorer=mean_squared_error,\n",
    "                    squared=False\n",
    "                )\n",
    "            ),\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "            refit=False, return_train_score=True, error_score='raise'\n",
    "        )\n",
    "        gscv.fit(cv_idxs, cv_idxs)\n",
    "        save_json(gscv.cv_results_, f'{work_dir}/cv_results.json', array_convert=True)\n",
    "        rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dummy_train_idxs = [1, 2, 3, 4]\n",
    "dummy_test_idxs = [0]\n",
    "\n",
    "y = structure_properties['energies'][train_idxs]\n",
    "y_train_init = y[cv_idxs.T[dummy_train_idxs].flatten()]\n",
    "y_test_init = y[cv_idxs.T[dummy_test_idxs].flatten()]\n",
    "\n",
    "kernel_loader = utils.KernelLoader(\n",
    "    filename='../Processed_Data/Models/6.0/Kernel_Models/Gaussian/gaussian_kernel_-2.0.hdf5'\n",
    ")\n",
    "kernel_scaler = utils.KernelNormScaler()\n",
    "ttr = TransformedTargetRegressor(\n",
    "    regressor=KernelRidge(alpha=1.0E-2, kernel='precomputed'),\n",
    "    transformer=utils.SampleSelector(X=y, model=utils.NormScaler()),\n",
    "    check_inverse=False\n",
    ")\n",
    "\n",
    "kernel_loader.fit(cv_idxs.T[dummy_train_idxs])\n",
    "K_train = kernel_loader.transform(cv_idxs.T[dummy_train_idxs])\n",
    "K_test = kernel_loader.transform(cv_idxs.T[dummy_test_idxs])\n",
    "\n",
    "kernel_scaler.fit(K_train)\n",
    "K_train = kernel_scaler.transform(K_train)\n",
    "K_test = kernel_scaler.transform(K_test)\n",
    "\n",
    "ttr.fit(K_train, cv_idxs.T[dummy_train_idxs].flatten())\n",
    "y_train_pred = ttr.predict(K_train)\n",
    "y_test_pred = ttr.predict(K_test)\n",
    "\n",
    "print(mean_absolute_error(y_train_init, y_train_pred))\n",
    "print(mean_absolute_error(y_test_init, y_test_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dummy_train_idxs = [1, 2, 3, 4]\n",
    "dummy_test_idxs = [0]\n",
    "\n",
    "y = structure_properties['energies'][train_idxs]\n",
    "y_train_init = y[cv_idxs.T[dummy_train_idxs].flatten()]\n",
    "y_test_init = y[cv_idxs.T[dummy_test_idxs].flatten()]\n",
    "\n",
    "kernel_loader = utils.KernelLoader(\n",
    "    filename='../Processed_Data/Models/6.0/Kernel_Models/Gaussian/gaussian_kernel_-2.0.hdf5'\n",
    ")\n",
    "kernel_scaler = utils.KernelNormScaler()\n",
    "ttr = TransformedTargetRegressor(\n",
    "    regressor=KernelRidge(alpha=1.0E-2, kernel='precomputed'),\n",
    "    transformer=utils.NormScaler(),\n",
    ")\n",
    "sample_selector = utils.SampleSelector(X=y)\n",
    "\n",
    "kernel_loader.fit(cv_idxs.T[dummy_train_idxs])\n",
    "K_train = kernel_loader.transform(cv_idxs.T[dummy_train_idxs])\n",
    "K_test = kernel_loader.transform(cv_idxs.T[dummy_test_idxs])\n",
    "\n",
    "kernel_scaler.fit(K_train)\n",
    "K_train = kernel_scaler.transform(K_train)\n",
    "K_test = kernel_scaler.transform(K_test)\n",
    "\n",
    "ttr.fit(K_train, sample_selector.transform(cv_idxs.T[dummy_train_idxs]))\n",
    "y_train_pred = ttr.predict(K_train)\n",
    "y_test_pred = ttr.predict(K_test)\n",
    "\n",
    "print(mean_absolute_error(y_train_init, y_train_pred))\n",
    "print(mean_absolute_error(y_test_init, y_test_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dummy_train_idxs = [1, 2, 3, 4]\n",
    "dummy_test_idxs = [0]\n",
    "\n",
    "y = structure_properties['energies'][train_idxs]\n",
    "y_train_init = y[cv_idxs.T[dummy_train_idxs].flatten()]\n",
    "y_test_init = y[cv_idxs.T[dummy_test_idxs].flatten()]\n",
    "\n",
    "kernel_loader = utils.KernelLoader(\n",
    "    filename='../Processed_Data/Models/6.0/Kernel_Models/Gaussian/gaussian_kernel_-2.0.hdf5'\n",
    ")\n",
    "kernel_scaler = utils.KernelNormScaler()\n",
    "krr = KernelRidge(alpha=1.0E-2, kernel='precomputed')\n",
    "sample_selector = utils.SampleSelector(X=y, model=utils.NormScaler())\n",
    "\n",
    "kernel_loader.fit(cv_idxs.T[dummy_train_idxs])\n",
    "K_train = kernel_loader.transform(cv_idxs.T[dummy_train_idxs])\n",
    "K_test = kernel_loader.transform(cv_idxs.T[dummy_test_idxs])\n",
    "\n",
    "kernel_scaler.fit(K_train)\n",
    "K_train = kernel_scaler.transform(K_train)\n",
    "K_test = kernel_scaler.transform(K_test)\n",
    "\n",
    "sample_selector.fit(cv_idxs.T[dummy_train_idxs])\n",
    "y_train = sample_selector.transform(cv_idxs.T[dummy_train_idxs])\n",
    "y_test = sample_selector.transform(cv_idxs.T[dummy_test_idxs])\n",
    "\n",
    "krr.fit(K_train, y_train)\n",
    "y_train_pred = krr.predict(K_train)\n",
    "y_test_pred = krr.predict(K_test)\n",
    "\n",
    "y_train_pred = sample_selector.inverse_transform(y_train_pred)\n",
    "y_test_pred = sample_selector.inverse_transform(y_test_pred)\n",
    "\n",
    "print(mean_absolute_error(y_train_init, y_train_pred))\n",
    "print(mean_absolute_error(y_test_init, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimal parameters as a check\n",
    "\n",
    "# Loop over cutoffs\n",
    "for cutoff in cutoffs:\n",
    "    \n",
    "    # Loop over properties\n",
    "    for pn in property_names:\n",
    "        property_label = pn.capitalize()\n",
    "        \n",
    "        work_dir = f'../Processed_Data/Models/{cutoff}/Kernel_Models/Gaussian/KRR/{property_label}'\n",
    "        cv_results = load_json(f'{work_dir}/cv_results.json')\n",
    "        \n",
    "        # Loop over error types\n",
    "        for error, error_name in zip(\n",
    "            ['neg_mean_absolute_error', 'neg_root_mean_squared_error'],\n",
    "            ['mae', 'rmse']\n",
    "        ):\n",
    "        \n",
    "            idx = np.argmin(cv_results[f'rank_test_{error}'])\n",
    "            opt_parameters = utils.get_optimal_parameters(cv_results, error, **kernel_ridge_parameters)\n",
    "\n",
    "            # Print error and parameters\n",
    "            print(f'-----Optimal Parameters for {cutoff} {pn} {error[4:]}-----')\n",
    "            print(f'{error} =', cv_results[f'mean_test_{error}'][idx])\n",
    "            print(opt_parameters)\n",
    "            print('')\n",
    "            \n",
    "            # Save optimal parameters for easy access\n",
    "            save_json(opt_parameters, f'{work_dir}/ridge_parameters_{error_name}.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
